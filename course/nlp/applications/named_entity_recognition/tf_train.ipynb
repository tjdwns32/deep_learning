{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version :  1.9.0\n",
      "WARNING:tensorflow:From C:\\Users\\juni\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:417: calling reverse_sequence (from tensorflow.python.ops.array_ops) with seq_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "seq_dim is deprecated, use seq_axis instead\n",
      "WARNING:tensorflow:From C:\\Users\\juni\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:432: calling reverse_sequence (from tensorflow.python.ops.array_ops) with batch_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "batch_dim is deprecated, use batch_axis instead\n",
      "model/emb:0\n",
      "model/birnn/fw/gru_cell/gates/kernel:0\n",
      "model/birnn/fw/gru_cell/gates/bias:0\n",
      "model/birnn/fw/gru_cell/candidate/kernel:0\n",
      "model/birnn/fw/gru_cell/candidate/bias:0\n",
      "model/birnn/bw/gru_cell/gates/kernel:0\n",
      "model/birnn/bw/gru_cell/gates/bias:0\n",
      "model/birnn/bw/gru_cell/candidate/kernel:0\n",
      "model/birnn/bw/gru_cell/candidate/bias:0\n",
      "model/Rnn2Target/weights:0\n",
      "model/Rnn2Target/biases:0\n",
      "WARNING:tensorflow:From <ipython-input-1-adf07c185369>:187: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.MonitoredTrainingSession\n",
      "INFO:tensorflow:Restoring parameters from ./trained_models\\model.ckpt-1970\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Starting standard services.\n",
      "INFO:tensorflow:Saving checkpoint to path ./trained_models\\model.ckpt\n",
      "INFO:tensorflow:model/global_step/sec: 0\n",
      "INFO:tensorflow:Starting queue runners.\n",
      "Epoch =   1 Step =    1971 loss = 0.027\n",
      "Epoch =   1 Step =    1972 loss = 0.026\n",
      "Epoch =   1 Step =    1973 loss = 0.026\n",
      "Epoch =   1 Step =    1974 loss = 0.031\n",
      "Epoch =   1 Step =    1975 loss = 0.030\n",
      "Epoch =   1 Step =    1976 loss = 0.029\n",
      "Epoch =   1 Step =    1977 loss = 0.029\n",
      "Epoch =   1 Step =    1978 loss = 0.029\n",
      "Epoch =   1 Step =    1979 loss = 0.029\n",
      "Epoch =   2 Step =    1980 loss = 0.029\n",
      "Epoch =   3 Step =    1990 loss = 0.029\n",
      "Epoch =   4 Step =    2000 loss = 0.028\n",
      "Epoch =   5 Step =    2010 loss = 0.028\n",
      "Epoch =   6 Step =    2020 loss = 0.028\n",
      "Epoch =   7 Step =    2030 loss = 0.028\n",
      "Epoch =   8 Step =    2040 loss = 0.028\n",
      "Epoch =   9 Step =    2050 loss = 0.028\n",
      "Epoch =  10 Step =    2060 loss = 0.028\n",
      "Epoch =  11 Step =    2070 loss = 0.028\n",
      "Epoch =  13 Step =    2080 loss = 0.028\n",
      "Epoch =  14 Step =    2090 loss = 0.029\n",
      "Epoch =  15 Step =    2100 loss = 0.029\n",
      "Epoch =  16 Step =    2110 loss = 0.029\n",
      "Epoch =  17 Step =    2120 loss = 0.029\n",
      "Epoch =  18 Step =    2130 loss = 0.029\n",
      "Epoch =  19 Step =    2140 loss = 0.029\n",
      "Epoch =  20 Step =    2150 loss = 0.029\n",
      "Epoch =  21 Step =    2160 loss = 0.029\n",
      "Epoch =  22 Step =    2170 loss = 0.029\n",
      "Epoch =  24 Step =    2180 loss = 0.029\n",
      "Epoch =  25 Step =    2190 loss = 0.029\n",
      "Epoch =  26 Step =    2200 loss = 0.029\n",
      "Epoch =  27 Step =    2210 loss = 0.029\n",
      "Epoch =  28 Step =    2220 loss = 0.029\n",
      "Epoch =  29 Step =    2230 loss = 0.029\n",
      "Epoch =  30 Step =    2240 loss = 0.029\n",
      "Epoch =  31 Step =    2250 loss = 0.029\n",
      "Epoch =  32 Step =    2260 loss = 0.030\n",
      "Epoch =  33 Step =    2270 loss = 0.030\n",
      "INFO:tensorflow:model/global_step/sec: 2.5331\n",
      "Epoch =  35 Step =    2280 loss = 0.030\n",
      "Epoch =  36 Step =    2290 loss = 0.030\n",
      "Epoch =  37 Step =    2300 loss = 0.030\n",
      "Epoch =  38 Step =    2310 loss = 0.030\n",
      "Epoch =  39 Step =    2320 loss = 0.030\n",
      "Epoch =  40 Step =    2330 loss = 0.030\n",
      "Epoch =  41 Step =    2340 loss = 0.030\n",
      "Epoch =  42 Step =    2350 loss = 0.031\n",
      "Epoch =  43 Step =    2360 loss = 0.031\n",
      "Epoch =  44 Step =    2370 loss = 0.031\n",
      "Epoch =  46 Step =    2380 loss = 0.031\n",
      "Epoch =  47 Step =    2390 loss = 0.031\n",
      "Epoch =  48 Step =    2400 loss = 0.031\n",
      "Epoch =  49 Step =    2410 loss = 0.031\n",
      "Epoch =  50 Step =    2420 loss = 0.032\n",
      "Epoch =  51 Step =    2430 loss = 0.032\n",
      "Epoch =  52 Step =    2440 loss = 0.032\n",
      "Epoch =  53 Step =    2450 loss = 0.032\n",
      "Epoch =  54 Step =    2460 loss = 0.032\n",
      "Epoch =  55 Step =    2470 loss = 0.033\n",
      "Epoch =  57 Step =    2480 loss = 0.033\n",
      "Epoch =  58 Step =    2490 loss = 0.033\n",
      "Epoch =  59 Step =    2500 loss = 0.033\n",
      "Epoch =  60 Step =    2510 loss = 0.033\n",
      "Epoch =  61 Step =    2520 loss = 0.033\n",
      "Epoch =  62 Step =    2530 loss = 0.034\n",
      "Epoch =  63 Step =    2540 loss = 0.034\n",
      "Epoch =  64 Step =    2550 loss = 0.034\n",
      "Epoch =  65 Step =    2560 loss = 0.034\n",
      "Epoch =  66 Step =    2570 loss = 0.034\n",
      "Epoch =  68 Step =    2580 loss = 0.034\n",
      "INFO:tensorflow:model/global_step/sec: 2.60868\n",
      "Epoch =  69 Step =    2590 loss = 0.035\n",
      "Epoch =  70 Step =    2600 loss = 0.035\n",
      "Epoch =  71 Step =    2610 loss = 0.035\n",
      "Epoch =  72 Step =    2620 loss = 0.035\n",
      "Epoch =  73 Step =    2630 loss = 0.036\n",
      "Epoch =  74 Step =    2640 loss = 0.036\n",
      "Epoch =  75 Step =    2650 loss = 0.036\n",
      "Epoch =  76 Step =    2660 loss = 0.037\n",
      "Epoch =  77 Step =    2670 loss = 0.037\n",
      "Epoch =  79 Step =    2680 loss = 0.038\n",
      "Epoch =  80 Step =    2690 loss = 0.038\n",
      "Epoch =  81 Step =    2700 loss = 0.039\n",
      "Epoch =  82 Step =    2710 loss = 0.040\n",
      "Epoch =  83 Step =    2720 loss = 0.040\n",
      "Epoch =  84 Step =    2730 loss = 0.041\n",
      "Epoch =  85 Step =    2740 loss = 0.042\n",
      "Epoch =  86 Step =    2750 loss = 0.042\n",
      "Epoch =  87 Step =    2760 loss = 0.043\n",
      "Epoch =  88 Step =    2770 loss = 0.045\n",
      "Epoch =  90 Step =    2780 loss = 0.047\n",
      "Epoch =  91 Step =    2790 loss = 0.050\n",
      "Epoch =  92 Step =    2800 loss = 0.052\n",
      "Epoch =  93 Step =    2810 loss = 0.055\n",
      "Epoch =  94 Step =    2820 loss = 0.058\n",
      "Epoch =  95 Step =    2830 loss = 0.061\n",
      "Epoch =  96 Step =    2840 loss = 0.064\n",
      "Epoch =  97 Step =    2850 loss = 0.067\n",
      "Epoch =  98 Step =    2860 loss = 0.070\n",
      "Epoch =  99 Step =    2870 loss = 0.073\n",
      "Epoch = 100 Step =    2880 loss = 0.076\n",
      "Epoch = 102 Step =    2890 loss = 0.078\n",
      "Epoch = 103 Step =    2900 loss = 0.081\n",
      "INFO:tensorflow:model/global_step/sec: 2.61666\n",
      "Epoch = 104 Step =    2910 loss = 0.084\n",
      "Epoch = 105 Step =    2920 loss = 0.086\n",
      "Epoch = 106 Step =    2930 loss = 0.089\n",
      "Epoch = 107 Step =    2940 loss = 0.091\n",
      "Epoch = 108 Step =    2950 loss = 0.094\n",
      "Epoch = 109 Step =    2960 loss = 0.096\n",
      "Epoch = 110 Step =    2970 loss = 0.098\n",
      "Epoch = 111 Step =    2980 loss = 0.101\n",
      "Epoch = 113 Step =    2990 loss = 0.103\n",
      "Epoch = 114 Step =    3000 loss = 0.106\n",
      "Epoch = 115 Step =    3010 loss = 0.108\n",
      "Epoch = 116 Step =    3020 loss = 0.111\n",
      "Epoch = 117 Step =    3030 loss = 0.114\n",
      "Epoch = 118 Step =    3040 loss = 0.116\n",
      "Epoch = 119 Step =    3050 loss = 0.119\n",
      "Epoch = 120 Step =    3060 loss = 0.121\n",
      "Epoch = 121 Step =    3070 loss = 0.124\n",
      "Epoch = 122 Step =    3080 loss = 0.126\n",
      "Epoch = 124 Step =    3090 loss = 0.129\n",
      "Epoch = 125 Step =    3100 loss = 0.131\n",
      "Epoch = 126 Step =    3110 loss = 0.133\n",
      "Epoch = 127 Step =    3120 loss = 0.135\n",
      "Epoch = 128 Step =    3130 loss = 0.138\n",
      "Epoch = 129 Step =    3140 loss = 0.140\n",
      "Epoch = 130 Step =    3150 loss = 0.142\n",
      "Epoch = 131 Step =    3160 loss = 0.145\n",
      "Epoch = 132 Step =    3170 loss = 0.147\n",
      "Epoch = 133 Step =    3180 loss = 0.150\n",
      "Epoch = 135 Step =    3190 loss = 0.152\n",
      "Epoch = 136 Step =    3200 loss = 0.154\n",
      "Epoch = 137 Step =    3210 loss = 0.157\n",
      "INFO:tensorflow:model/global_step/sec: 2.608\n",
      "Epoch = 138 Step =    3220 loss = 0.159\n",
      "Epoch = 139 Step =    3230 loss = 0.162\n",
      "Epoch = 140 Step =    3240 loss = 0.164\n",
      "Epoch = 141 Step =    3250 loss = 0.166\n",
      "Epoch = 142 Step =    3260 loss = 0.169\n",
      "Epoch = 143 Step =    3270 loss = 0.171\n",
      "Epoch = 144 Step =    3280 loss = 0.174\n",
      "Epoch = 146 Step =    3290 loss = 0.176\n",
      "Epoch = 147 Step =    3300 loss = 0.178\n",
      "Epoch = 148 Step =    3310 loss = 0.180\n",
      "Epoch = 149 Step =    3320 loss = 0.183\n",
      "Epoch = 150 Step =    3330 loss = 0.185\n",
      "Epoch = 151 Step =    3340 loss = 0.187\n",
      "Epoch = 152 Step =    3350 loss = 0.189\n",
      "Epoch = 153 Step =    3360 loss = 0.191\n",
      "Epoch = 154 Step =    3370 loss = 0.193\n",
      "Epoch = 155 Step =    3380 loss = 0.195\n",
      "Epoch = 157 Step =    3390 loss = 0.197\n",
      "Epoch = 158 Step =    3400 loss = 0.199\n",
      "Epoch = 159 Step =    3410 loss = 0.201\n",
      "Epoch = 160 Step =    3420 loss = 0.203\n",
      "Epoch = 161 Step =    3430 loss = 0.205\n",
      "Epoch = 162 Step =    3440 loss = 0.207\n",
      "Epoch = 163 Step =    3450 loss = 0.209\n",
      "Epoch = 164 Step =    3460 loss = 0.210\n",
      "Epoch = 165 Step =    3470 loss = 0.212\n",
      "Epoch = 166 Step =    3480 loss = 0.214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 168 Step =    3490 loss = 0.216\n",
      "Epoch = 169 Step =    3500 loss = 0.218\n",
      "Epoch = 170 Step =    3510 loss = 0.219\n",
      "Epoch = 171 Step =    3520 loss =   nan\n",
      "INFO:tensorflow:Saving checkpoint to path ./trained_models\\model.ckpt\n",
      "INFO:tensorflow:model/global_step/sec: 2.61685\n",
      "Epoch = 172 Step =    3530 loss =   nan\n",
      "Epoch = 173 Step =    3540 loss =   nan\n",
      "Epoch = 174 Step =    3550 loss =   nan\n",
      "Epoch = 175 Step =    3560 loss =   nan\n",
      "Epoch = 176 Step =    3570 loss =   nan\n",
      "Epoch = 177 Step =    3580 loss =   nan\n",
      "Epoch = 179 Step =    3590 loss =   nan\n",
      "Epoch = 180 Step =    3600 loss =   nan\n",
      "Epoch = 181 Step =    3610 loss =   nan\n",
      "Epoch = 182 Step =    3620 loss =   nan\n",
      "Epoch = 183 Step =    3630 loss =   nan\n",
      "Epoch = 184 Step =    3640 loss =   nan\n",
      "Epoch = 185 Step =    3650 loss =   nan\n",
      "Epoch = 186 Step =    3660 loss =   nan\n",
      "Epoch = 187 Step =    3670 loss =   nan\n",
      "Epoch = 188 Step =    3680 loss =   nan\n",
      "Epoch = 190 Step =    3690 loss =   nan\n",
      "Epoch = 191 Step =    3700 loss =   nan\n",
      "Epoch = 192 Step =    3710 loss =   nan\n",
      "Epoch = 193 Step =    3720 loss =   nan\n",
      "Epoch = 194 Step =    3730 loss =   nan\n",
      "Epoch = 195 Step =    3740 loss =   nan\n",
      "Epoch = 196 Step =    3750 loss =   nan\n",
      "Epoch = 197 Step =    3760 loss =   nan\n",
      "Epoch = 198 Step =    3770 loss =   nan\n",
      "Epoch = 199 Step =    3780 loss =   nan\n",
      "Epoch = 200 Step =    3790 loss =   nan\n",
      "Epoch = 202 Step =    3800 loss =   nan\n",
      "Epoch = 203 Step =    3810 loss =   nan\n",
      "Epoch = 204 Step =    3820 loss =   nan\n",
      "Epoch = 205 Step =    3830 loss =   nan\n",
      "Epoch = 206 Step =    3840 loss =   nan\n",
      "INFO:tensorflow:model/global_step/sec: 2.59984\n",
      "Epoch = 207 Step =    3850 loss =   nan\n",
      "Epoch = 208 Step =    3860 loss =   nan\n",
      "Epoch = 209 Step =    3870 loss =   nan\n",
      "Epoch = 210 Step =    3880 loss =   nan\n",
      "Epoch = 211 Step =    3890 loss =   nan\n",
      "Epoch = 213 Step =    3900 loss =   nan\n",
      "Epoch = 214 Step =    3910 loss =   nan\n",
      "Epoch = 215 Step =    3920 loss =   nan\n",
      "Epoch = 216 Step =    3930 loss =   nan\n",
      "Epoch = 217 Step =    3940 loss =   nan\n",
      "Epoch = 218 Step =    3950 loss =   nan\n",
      "Epoch = 219 Step =    3960 loss =   nan\n",
      "Epoch = 220 Step =    3970 loss =   nan\n",
      "Epoch = 221 Step =    3980 loss =   nan\n",
      "Epoch = 222 Step =    3990 loss =   nan\n",
      "Epoch = 224 Step =    4000 loss =   nan\n",
      "Epoch = 225 Step =    4010 loss =   nan\n",
      "Epoch = 226 Step =    4020 loss =   nan\n",
      "Epoch = 227 Step =    4030 loss =   nan\n",
      "Epoch = 228 Step =    4040 loss =   nan\n",
      "Epoch = 229 Step =    4050 loss =   nan\n",
      "Epoch = 230 Step =    4060 loss =   nan\n",
      "Epoch = 231 Step =    4070 loss =   nan\n",
      "Epoch = 232 Step =    4080 loss =   nan\n",
      "Epoch = 233 Step =    4090 loss =   nan\n",
      "Epoch = 235 Step =    4100 loss =   nan\n",
      "Epoch = 236 Step =    4110 loss =   nan\n",
      "Epoch = 237 Step =    4120 loss =   nan\n",
      "Epoch = 238 Step =    4130 loss =   nan\n",
      "Epoch = 239 Step =    4140 loss =   nan\n",
      "Epoch = 240 Step =    4150 loss =   nan\n",
      "INFO:tensorflow:model/global_step/sec: 2.60027\n",
      "Epoch = 241 Step =    4160 loss =   nan\n",
      "Epoch = 242 Step =    4170 loss =   nan\n",
      "Epoch = 243 Step =    4180 loss =   nan\n",
      "Epoch = 244 Step =    4190 loss =   nan\n",
      "Epoch = 246 Step =    4200 loss =   nan\n",
      "Epoch = 247 Step =    4210 loss =   nan\n",
      "Epoch = 248 Step =    4220 loss =   nan\n",
      "Epoch = 249 Step =    4230 loss =   nan\n",
      "Epoch = 250 Step =    4240 loss =   nan\n",
      "Epoch = 251 Step =    4250 loss =   nan\n",
      "Epoch = 252 Step =    4260 loss =   nan\n",
      "Epoch = 253 Step =    4270 loss =   nan\n",
      "Epoch = 254 Step =    4280 loss =   nan\n",
      "Epoch = 255 Step =    4290 loss =   nan\n",
      "Epoch = 257 Step =    4300 loss =   nan\n",
      "Epoch = 258 Step =    4310 loss =   nan\n",
      "Epoch = 259 Step =    4320 loss =   nan\n",
      "Epoch = 260 Step =    4330 loss =   nan\n",
      "Epoch = 261 Step =    4340 loss =   nan\n",
      "Epoch = 262 Step =    4350 loss =   nan\n",
      "Epoch = 263 Step =    4360 loss =   nan\n",
      "Epoch = 264 Step =    4370 loss =   nan\n",
      "Epoch = 265 Step =    4380 loss =   nan\n",
      "Epoch = 266 Step =    4390 loss =   nan\n",
      "Epoch = 268 Step =    4400 loss =   nan\n",
      "Epoch = 269 Step =    4410 loss =   nan\n",
      "Epoch = 270 Step =    4420 loss =   nan\n",
      "Epoch = 271 Step =    4430 loss =   nan\n",
      "Epoch = 272 Step =    4440 loss =   nan\n",
      "Epoch = 273 Step =    4450 loss =   nan\n",
      "Epoch = 274 Step =    4460 loss =   nan\n",
      "INFO:tensorflow:model/global_step/sec: 2.60003\n",
      "Epoch = 275 Step =    4470 loss =   nan\n",
      "Epoch = 276 Step =    4480 loss =   nan\n",
      "Epoch = 277 Step =    4490 loss =   nan\n",
      "Epoch = 279 Step =    4500 loss =   nan\n",
      "Epoch = 280 Step =    4510 loss =   nan\n",
      "Epoch = 281 Step =    4520 loss =   nan\n",
      "Epoch = 282 Step =    4530 loss =   nan\n",
      "Epoch = 283 Step =    4540 loss =   nan\n",
      "Epoch = 284 Step =    4550 loss =   nan\n",
      "Epoch = 285 Step =    4560 loss =   nan\n",
      "Epoch = 286 Step =    4570 loss =   nan\n",
      "Epoch = 287 Step =    4580 loss =   nan\n",
      "Epoch = 288 Step =    4590 loss =   nan\n",
      "Epoch = 290 Step =    4600 loss =   nan\n",
      "Epoch = 291 Step =    4610 loss =   nan\n",
      "Epoch = 292 Step =    4620 loss =   nan\n",
      "Epoch = 293 Step =    4630 loss =   nan\n",
      "Epoch = 294 Step =    4640 loss =   nan\n",
      "Epoch = 295 Step =    4650 loss =   nan\n",
      "Epoch = 296 Step =    4660 loss =   nan\n",
      "Epoch = 297 Step =    4670 loss =   nan\n",
      "Epoch = 298 Step =    4680 loss =   nan\n",
      "Epoch = 299 Step =    4690 loss =   nan\n",
      "Epoch = 300 Step =    4700 loss =   nan\n",
      "Training is done.\n",
      "INFO:tensorflow:Restoring parameters from ./trained_models\\model.ckpt-3528\n",
      "INFO:tensorflow:Froze 11 variables.\n",
      "INFO:tensorflow:Converted 11 variables to const ops.\n",
      "2371 ops in the final graph.\n",
      "의\tI-DT\t0.09232304990291595\n",
      "정\tB-TI\t0.09217729419469833\n",
      "지\tB-TI\t0.09321316331624985\n",
      "기\tB-TI\t0.09320665895938873\n",
      "단\tB-TI\t0.09328790009021759\n",
      "은\tB-TI\t0.09321732074022293\n",
      " \tB-TI\t0.09267337620258331\n",
      "첫\tB-PS\t0.09214608371257782\n",
      " \tB-PS\t0.09205768257379532\n",
      "사\tI-PS\t0.09272753447294235\n",
      "업\tB-TI\t0.0926508828997612\n",
      "으\tB-TI\t0.09311534464359283\n",
      "로\tI-DT\t0.09363895654678345\n",
      " \tI-DT\t0.09325636178255081\n",
      "4\tB-LC\t0.09305885434150696\n",
      "5\tB-LC\t0.09306935220956802\n",
      " \tI-DT\t0.0929734855890274\n",
      "명\tO\t0.09254533797502518\n",
      " \tO\t0.09284773468971252\n",
      "시\tI-TI\t0.09304240345954895\n",
      "의\tI-DT\t0.09286998212337494\n",
      "원\tI-TI\t0.09246007353067398\n",
      "들\tB-LC\t0.09208482503890991\n",
      "의\tB-LC\t0.09258855134248734\n",
      " \tB-LC\t0.09237406402826309\n",
      "선\tI-TI\t0.09265252202749252\n",
      "거\tI-TI\t0.09284981340169907\n",
      " \tI-TI\t0.09277467429637909\n",
      "공\tI-TI\t0.09223952889442444\n",
      "약\tB-TI\t0.0925937220454216\n",
      "을\tI-TI\t0.09267179667949677\n",
      " \tI-TI\t0.09305071830749512\n",
      "수\tI-TI\t0.09384588897228241\n",
      "집\tI-TI\t0.09395696967840195\n",
      "해\tI-DT\t0.09293601661920547\n",
      " \tI-TI\t0.09353915601968765\n",
      "개\tI-TI\t0.09319532662630081\n",
      "인\tB-OG\t0.0924195945262909\n",
      "별\tI-PS\t0.09247571974992752\n",
      "로\tI-DT\t0.09215964376926422\n",
      " \tI-PS\t0.09217192977666855\n",
      "카\tI-PS\t0.09295932203531265\n",
      "드\tI-TI\t0.09212110936641693\n",
      "를\tB-PS\t0.09222816675901413\n",
      " \tI-TI\t0.09206651896238327\n",
      "만\tB-PS\t0.09183113276958466\n",
      "들\tI-TI\t0.09191407263278961\n",
      "었\tI-TI\t0.09251237660646439\n",
      "다\tI-LC\t0.09217635542154312\n",
      ".\tI-DT\t0.09181556850671768\n",
      "한\tO\t0.09245549142360687\n",
      "국\tO\t0.09281159192323685\n",
      "소\tI-OG\t0.092154860496521\n",
      "비\tI-LC\t0.0921742245554924\n",
      "자\tI-LC\t0.09297880530357361\n",
      "보\tI-LC\t0.09276474267244339\n",
      "호\tB-DT\t0.0920645147562027\n",
      "원\tB-TI\t0.09266269207000732\n",
      "은\tB-TI\t0.09303463995456696\n",
      " \tB-TI\t0.09226527810096741\n",
      "1\tO\t0.0922480896115303\n",
      "9\tI-TI\t0.09294348210096359\n",
      "일\tI-TI\t0.09381074458360672\n",
      " \tI-TI\t0.093291737139225\n",
      "시\tI-TI\t0.09464830160140991\n",
      "판\tI-TI\t0.09407064318656921\n",
      "중\tI-TI\t0.09293578565120697\n",
      "인\tB-LC\t0.09282070398330688\n",
      " \tB-LC\t0.09278366714715958\n",
      "선\tI-TI\t0.09325975179672241\n",
      "물\tI-TI\t0.09326104819774628\n",
      "세\tI-TI\t0.09316780418157578\n",
      "트\tB-LC\t0.09239448606967926\n",
      "의\tI-DT\t0.09260808676481247\n",
      " \tI-PS\t0.09210427850484848\n",
      "상\tI-PS\t0.09192101657390594\n",
      "당\tB-DT\t0.09261535108089447\n",
      "수\tI-DT\t0.09317421168088913\n",
      "가\tB-TI\t0.09279713779687881\n",
      " \tI-DT\t0.09297321736812592\n",
      "과\tI-DT\t0.0933041200041771\n",
      "대\tI-DT\t0.09308719635009766\n",
      " \tI-DT\t0.0928913876414299\n",
      "포\tO\t0.0920364260673523\n",
      "장\tI-DT\t0.09224126487970352\n",
      "된\tI-DT\t0.09197116643190384\n",
      " \tO\t0.09158989042043686\n",
      "것\tB-DT\t0.09245054423809052\n",
      "으\tI-LC\t0.0917506143450737\n",
      "로\tB-LC\t0.0926894024014473\n",
      " \tB-LC\t0.09270956367254257\n",
      "드\tB-DT\t0.09283524751663208\n",
      "러\tB-DT\t0.09218134731054306\n",
      "났\tB-TI\t0.0928838700056076\n",
      "다\tB-TI\t0.09289254248142242\n",
      "고\tB-TI\t0.09283503890037537\n",
      " \tI-DT\t0.09280488640069962\n",
      "밝\tB-TI\t0.0925859734416008\n",
      "혔\tB-TI\t0.09220796823501587\n",
      "다\tI-DT\t0.09306783974170685\n",
      ".\tI-DT\t0.09300673753023148\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Named Entity Recognition \n",
    "\n",
    "        Author : Sangkeun Jung (2017)\n",
    "        - using Tensorflow\n",
    "\"\"\"\n",
    "\n",
    "import sys, os, inspect\n",
    "\n",
    "# add common to path\n",
    "from pathlib import Path\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "common_path = str(Path(currentdir).parent.parent)\n",
    "sys.path.append( common_path )\n",
    "\n",
    "from common.nlp.vocab import Vocab\n",
    "from common.nlp.data_loader import N2NTextData\n",
    "from common.nlp.converter import N2NConverter\n",
    "\n",
    "from dataset import NERDataset\n",
    "from dataset import load_data\n",
    "from common.ml.hparams import HParams\n",
    "\n",
    "import numpy as np\n",
    "import copy \n",
    "import time \n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn\n",
    "from tensorflow.contrib.layers.python.layers import linear\n",
    "from tensorflow.python.ops import variable_scope\n",
    "from tensorflow.contrib.seq2seq import sequence_loss#sequence의 loss의 평균값을 구할 수있음\n",
    "\n",
    "from common.ml.tf.deploy import freeze_graph\n",
    "\n",
    "\n",
    "\n",
    "print( \"Tensorflow Version : \", tf.__version__)\n",
    "\n",
    "class NER():\n",
    "    def __init__(self, hps, mode=\"train\"):\n",
    "        self.hps = hps\n",
    "        self.x = tf.placeholder(tf.int32,   [None, hps.num_steps], name=\"pl_tokens\")\n",
    "        self.y = tf.placeholder(tf.int32,   [None, hps.num_steps], name=\"pl_target\")\n",
    "        self.w = tf.placeholder(tf.float32, [None, hps.num_steps], name=\"pl_weight\")\n",
    "        self.keep_prob = tf.placeholder(tf.float32, [], name=\"pl_keep_prob\")\n",
    "\n",
    "        ### 4 blocks ###\n",
    "        # 1) embedding\n",
    "        # 2) dropout on input embedding\n",
    "        # 3) sentence encoding using rnn\n",
    "        # 4) bidirectional rnn's output to target classes\n",
    "        # 5) loss calcaulation\n",
    "\n",
    "        def _embedding(x):\n",
    "            # character embedding \n",
    "            shape       = [hps.vocab_size, hps.emb_size]\n",
    "            initializer = tf.initializers.variance_scaling(distribution=\"uniform\", dtype=tf.float32)\n",
    "            emb_mat     = tf.get_variable(\"emb\", shape, initializer=initializer, dtype=tf.float32)\n",
    "            input_emb   = tf.nn.embedding_lookup(emb_mat, x)   # [batch_size, sent_len, emb_dim]\n",
    "\n",
    "            # split input_emb -> num_steps\n",
    "            step_inputs = tf.unstack(input_emb, axis=1)\n",
    "            return step_inputs\n",
    "\n",
    "        def _sequence_dropout(step_inputs, keep_prob):\n",
    "            # apply dropout to each input\n",
    "            # input : a list of input tensor which shape is [None, input_dim]\n",
    "            with tf.name_scope('sequence_dropout') as scope:\n",
    "                step_outputs = []\n",
    "                for t, input in enumerate(step_inputs):\n",
    "                    step_outputs.append( tf.nn.dropout(input, keep_prob) )\n",
    "            return step_outputs\n",
    "\n",
    "        def sequence_encoding_n2n(step_inputs, seq_length, cell_size):\n",
    "            # birnn based N2N encoding and output\n",
    "            f_rnn_cell = tf.contrib.rnn.GRUCell(cell_size, reuse=False)\n",
    "            b_rnn_cell = tf.contrib.rnn.GRUCell(cell_size, reuse=False)\n",
    "            _inputs    = tf.stack(step_inputs, axis=1)\n",
    "\n",
    "            # step_inputs = a list of [batch_size, emb_dim]\n",
    "            # input = [batch_size, num_step, emb_dim]\n",
    "            # np.stack( [a,b,c,] )\n",
    "            outputs, states, = tf.nn.bidirectional_dynamic_rnn( f_rnn_cell,\n",
    "                                                                b_rnn_cell,\n",
    "                                                                _inputs,\n",
    "                                                                sequence_length=tf.cast(seq_length, tf.int64),\n",
    "                                                                time_major=False,\n",
    "                                                                dtype=tf.float32,\n",
    "                                                                scope='birnn',\n",
    "                                                            )\n",
    "            output_fw, output_bw = outputs\n",
    "            states_fw, states_bw = states \n",
    "\n",
    "            output       = tf.concat([output_fw, output_bw], 2)\n",
    "            step_outputs = tf.unstack(output, axis=1)\n",
    "\n",
    "            final_state  = tf.concat([states_fw, states_bw], 1)\n",
    "            return step_outputs # a list of [batch_size, enc_dim]\n",
    "\n",
    "        def _to_class_n2n(step_inputs, num_class):\n",
    "            T = len(step_inputs)\n",
    "            step_output_logits = []\n",
    "            for t in range(T):\n",
    "                # encoder to linear(map)\n",
    "                out = step_inputs[t]\n",
    "                if t==0: out = linear(out, num_class, scope=\"Rnn2Target\")\n",
    "                else:    out = linear(out, num_class, scope=\"Rnn2Target\", reuse=True)\n",
    "                step_output_logits.append(out)\n",
    "            return step_output_logits\n",
    "\n",
    "        def _loss(step_outputs, step_refs, weights):\n",
    "            # step_outputs : a list of [batch_size, num_class] float32 - unscaled logits\n",
    "            # step_refs    : [batch_size, num_steps] int32\n",
    "            # weights      : [batch_size, num_steps] float32\n",
    "            # calculate sequence wise loss function using cross-entropy\n",
    "            _batch_output_logits = tf.stack(step_outputs, axis=1)\n",
    "            loss = sequence_loss(\n",
    "                                    logits=_batch_output_logits,        \n",
    "                                    targets=step_refs,\n",
    "                                    weights=weights\n",
    "                                )\n",
    "            return loss\n",
    "        \n",
    "        seq_length    = tf.reduce_sum(self.w, 1) # [batch_size]\n",
    "\n",
    "        step_inputs       = _embedding(self.x)\n",
    "        step_inputs       = _sequence_dropout(step_inputs, self.keep_prob)\n",
    "        step_enc_outputs  = sequence_encoding_n2n(step_inputs, seq_length, hps.enc_dim)\n",
    "        step_outputs      = _to_class_n2n(step_enc_outputs, hps.num_target_class)\n",
    "\n",
    "        self.loss = _loss(step_outputs, self.y, self.w)\n",
    "\n",
    "        # step_preds and step_out_probs\n",
    "        step_out_probs = []\n",
    "        step_out_preds = []\n",
    "        for _output in step_outputs:\n",
    "            _out_probs  = tf.nn.softmax(_output)\n",
    "            _out_pred   = tf.argmax(_out_probs, 1)\n",
    "\n",
    "            step_out_probs.append(_out_probs)\n",
    "            step_out_preds.append(_out_pred)\n",
    "\n",
    "        # stack for interface\n",
    "        self.step_out_probs = tf.stack(step_out_probs, axis=1, name=\"step_out_probs\")\n",
    "        self.step_out_preds = tf.stack(step_out_preds, axis=1, name=\"step_out_preds\")\n",
    "\n",
    "        self.global_step = tf.get_variable(\"global_step\", [], tf.int32, initializer=tf.zeros_initializer, trainable=False)\n",
    "\n",
    "        if mode == \"train\":\n",
    "            optimizer       = tf.train.AdamOptimizer(hps.learning_rate)\n",
    "            self.train_op   = optimizer.minimize(self.loss, global_step=self.global_step)\n",
    "        else:\n",
    "            self.train_op = tf.no_op()\n",
    "\n",
    "        for v in tf.trainable_variables(): print(v.name)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_default_hparams():\n",
    "        return HParams(\n",
    "            learning_rate     = 0.01,\n",
    "            keep_prob         = 0.5,\n",
    "        )\n",
    "\n",
    "\n",
    "def train(train_id_data, num_vocabs, num_taget_class):\n",
    "    #\n",
    "    # train sentiment analysis using given train_id_data\n",
    "    #\n",
    "    max_epoch = 300\n",
    "    model_dir = \"./trained_models\"\n",
    "    hps = NER.get_default_hparams()\n",
    "    hps.update(\n",
    "                    batch_size= 100,\n",
    "                    num_steps = 128,\n",
    "                    emb_size  = 50,\n",
    "                    enc_dim   = 100,\n",
    "                    vocab_size=num_vocabs,\n",
    "                    num_target_class=num_taget_class\n",
    "               )\n",
    "\n",
    "    with tf.variable_scope(\"model\"):\n",
    "        model = NER(hps, \"train\")\n",
    "\n",
    "    sv = tf.train.Supervisor(is_chief=True,\n",
    "                             logdir=model_dir,\n",
    "                             summary_op=None,  \n",
    "                             global_step=model.global_step)\n",
    "\n",
    "    # tf assign compatible operators for gpu and cpu \n",
    "    tf_config = tf.ConfigProto(allow_soft_placement=True)\n",
    "\n",
    "    with sv.managed_session(config=tf_config) as sess:\n",
    "        local_step       = 0\n",
    "        prev_global_step = sess.run(model.global_step)\n",
    "\n",
    "        train_data_set = NERDataset(train_id_data, hps.batch_size, hps.num_steps)\n",
    "        losses = []\n",
    "        while not sv.should_stop():\n",
    "            fetches = [model.global_step, model.loss, model.train_op]\n",
    "            a_batch_data = next( train_data_set.iterator )\n",
    "            y, x, w = a_batch_data\n",
    "            fetched = sess.run(fetches, {\n",
    "                                            model.x: x, \n",
    "                                            model.y: y, \n",
    "                                            model.w: w,\n",
    "\n",
    "                                            model.keep_prob: hps.keep_prob,\n",
    "                                        }\n",
    "                              )\n",
    "\n",
    "            local_step += 1\n",
    "\n",
    "            _global_step = fetched[0]\n",
    "            _loss        = fetched[1]\n",
    "            losses.append( _loss )\n",
    "            if local_step < 10 or local_step % 10 == 0:\n",
    "                epoch = train_data_set.get_epoch_num()\n",
    "                print(\"Epoch = {:3d} Step = {:7d} loss = {:5.3f}\".format(epoch, _global_step, np.mean(losses)) )\n",
    "                _loss = []                \n",
    "                if epoch >= max_epoch : break \n",
    "\n",
    "        print(\"Training is done.\")\n",
    "    sv.stop()\n",
    "\n",
    "    # model.out_pred, model.out_probs\n",
    "    freeze_graph(model_dir, \"model/step_out_preds,model/step_out_probs\", \"frozen_graph.tf.pb\") ## freeze graph with params to probobuf format\n",
    "    \n",
    "from tensorflow.core.framework import graph_pb2\n",
    "def predict(token_vocab, target_vocab, sent):\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # force to use cpu only (prediction)\n",
    "    model_dir = \"./trained_models\"\n",
    "\n",
    "    # prepare sentence converting\n",
    "    # to make raw sentence to id data easily\n",
    "    pred_data     = N2NTextData(sent, mode='sentence')\n",
    "    pred_id_data  = N2NConverter.convert(pred_data, target_vocab, token_vocab)\n",
    "    pred_data_set = NERDataset(pred_id_data, 1, 128)\n",
    "    #\n",
    "    a_batch_data = next(pred_data_set.predict_iterator) # a result\n",
    "    b_nes_id, b_token_ids, b_weight = a_batch_data\n",
    "\n",
    "    # Restore graph\n",
    "    # note that frozen_graph.tf.pb contains graph definition with parameter values in binary format\n",
    "    _graph_fn =  os.path.join(model_dir, 'frozen_graph.tf.pb')\n",
    "    with tf.gfile.GFile(_graph_fn, \"rb\") as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "    \n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.import_graph_def(graph_def)\n",
    "\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        # to check load graph\n",
    "        #for n in tf.get_default_graph().as_graph_def().node: print(n.name)\n",
    "\n",
    "        # make interface for input\n",
    "        pl_token     = graph.get_tensor_by_name('import/model/pl_tokens:0')\n",
    "        pl_weight    = graph.get_tensor_by_name('import/model/pl_weight:0')\n",
    "        pl_keep_prob = graph.get_tensor_by_name('import/model/pl_keep_prob:0')\n",
    "\n",
    "        # make interface for output\n",
    "        step_out_preds = graph.get_tensor_by_name('import/model/step_out_preds:0')\n",
    "        step_out_probs = graph.get_tensor_by_name('import/model/step_out_probs:0')\n",
    "        \n",
    "\n",
    "        # predict sentence \n",
    "        b_best_step_pred_indexs, b_step_pred_probs = sess.run([step_out_preds, step_out_probs], \n",
    "                                                              feed_dict={\n",
    "                                                                            pl_token  : b_token_ids,\n",
    "                                                                            pl_weight : b_weight,\n",
    "                                                                            pl_keep_prob : 1.0,\n",
    "                                                                        }\n",
    "                                                             )\n",
    "        best_step_pred_indexs = b_best_step_pred_indexs[0]\n",
    "        step_pred_probs = b_step_pred_probs[0]\n",
    "\n",
    "        step_best_targets = []\n",
    "        step_best_target_probs = []\n",
    "        for time_step, best_pred_index in enumerate(best_step_pred_indexs):\n",
    "            _target_class = target_vocab.get_symbol(best_pred_index)\n",
    "            step_best_targets.append( _target_class )\n",
    "            _prob = step_pred_probs[time_step][best_pred_index]\n",
    "            step_best_target_probs.append( _prob ) \n",
    "\n",
    "        for idx, char in enumerate(list(sent)):\n",
    "            print('{}\\t{}\\t{}'.format(char, step_best_targets[idx], step_best_target_probs[idx]) ) \n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_id_data, token_vocab, target_vocab = load_data()\n",
    "    num_vocabs       = token_vocab.get_num_tokens()\n",
    "    num_target_class = target_vocab.get_num_targets()\n",
    "\n",
    "    train_data_set = NERDataset(train_id_data, 5, 128)\n",
    "    train(train_id_data, num_vocabs, num_target_class)\n",
    "    \n",
    "    predict(token_vocab, target_vocab, '의정지기단은 첫 사업으로 45 명 시의원들의 선거 공약을 수집해 개인별로 카드를 만들었다.')\n",
    "    predict(token_vocab, target_vocab, '한국소비자보호원은 19일 시판중인 선물세트의 상당수가 과대 포장된 것으로 드러났다고 밝혔다.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "맑\tB-DT\t0.09279853105545044\n",
      "은\tB-DT\t0.09245676547288895\n",
      " \tI-DT\t0.09251606464385986\n",
      "햇\tB-DT\t0.09343211352825165\n",
      "살\tI-DT\t0.09317658096551895\n",
      "과\tI-DT\t0.09348512440919876\n",
      " \tI-DT\t0.09325464814901352\n",
      "푸\tI-DT\t0.09280725568532944\n",
      "른\tO\t0.09268514066934586\n",
      " \tI-DT\t0.09244753420352936\n",
      "구\tO\t0.09336932003498077\n",
      "름\tO\t0.09244924038648605\n",
      "!\tO\t0.09321863204240799\n"
     ]
    }
   ],
   "source": [
    " predict(token_vocab, target_vocab, '맑은 햇살과 푸른 구름!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
