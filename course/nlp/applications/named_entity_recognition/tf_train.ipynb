{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tjdwn\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version :  1.10.0\n",
      "<common.nlp.data_loader.N2NTextData object at 0x00000221A4039F28>\n",
      "WARNING:tensorflow:From C:\\Users\\tjdwn\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:430: calling reverse_sequence (from tensorflow.python.ops.array_ops) with seq_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "seq_dim is deprecated, use seq_axis instead\n",
      "WARNING:tensorflow:From C:\\Users\\tjdwn\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:454: calling reverse_sequence (from tensorflow.python.ops.array_ops) with batch_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "batch_dim is deprecated, use batch_axis instead\n",
      "model/emb:0\n",
      "model/birnn/fw/gru_cell/gates/kernel:0\n",
      "model/birnn/fw/gru_cell/gates/bias:0\n",
      "model/birnn/fw/gru_cell/candidate/kernel:0\n",
      "model/birnn/fw/gru_cell/candidate/bias:0\n",
      "model/birnn/bw/gru_cell/gates/kernel:0\n",
      "model/birnn/bw/gru_cell/gates/bias:0\n",
      "model/birnn/bw/gru_cell/candidate/kernel:0\n",
      "model/birnn/bw/gru_cell/candidate/bias:0\n",
      "model/Rnn2Target/weights:0\n",
      "model/Rnn2Target/biases:0\n",
      "WARNING:tensorflow:From <ipython-input-1-f2e1d4316e54>:187: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.MonitoredTrainingSession\n",
      "INFO:tensorflow:Restoring parameters from ./trained_models\\model.ckpt-0\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Starting standard services.\n",
      "INFO:tensorflow:Saving checkpoint to path ./trained_models\\model.ckpt\n",
      "INFO:tensorflow:Starting queue runners.\n",
      "INFO:tensorflow:model/global_step/sec: 0\n",
      "Epoch =   1 Step =       1 loss = 2.564\n",
      "Epoch =   1 Step =       2 loss = 2.476\n",
      "Epoch =   1 Step =       3 loss = 2.325\n",
      "Epoch =   1 Step =       4 loss = 2.039\n",
      "Epoch =   1 Step =       5 loss = 1.883\n",
      "Epoch =   1 Step =       6 loss = 1.812\n",
      "Epoch =   1 Step =       7 loss = 1.710\n",
      "Epoch =   1 Step =       8 loss = 1.634\n",
      "Epoch =   1 Step =       9 loss = 1.550\n",
      "Epoch =   1 Step =      10 loss = 1.486\n",
      "Epoch =   2 Step =      20 loss = 1.181\n",
      "Epoch =   3 Step =      30 loss = 1.031\n",
      "Epoch =   4 Step =      40 loss = 0.904\n",
      "Epoch =   5 Step =      50 loss = 0.802\n",
      "Epoch =   6 Step =      60 loss = 0.721\n",
      "Epoch =   7 Step =      70 loss = 0.650\n",
      "Epoch =   8 Step =      80 loss = 0.595\n",
      "Epoch =   9 Step =      90 loss = 0.550\n",
      "Epoch =  10 Step =     100 loss = 0.511\n",
      "Epoch =  11 Step =     110 loss = 0.478\n",
      "Epoch =  12 Step =     120 loss = 0.450\n",
      "Epoch =  13 Step =     130 loss = 0.425\n",
      "Epoch =  14 Step =     140 loss = 0.403\n",
      "Epoch =  15 Step =     150 loss = 0.384\n",
      "Epoch =  16 Step =     160 loss = 0.366\n",
      "Epoch =  17 Step =     170 loss = 0.350\n",
      "Epoch =  18 Step =     180 loss = 0.336\n",
      "Epoch =  19 Step =     190 loss = 0.323\n",
      "Epoch =  20 Step =     200 loss = 0.311\n",
      "Epoch =  20 Step =     210 loss = 0.300\n",
      "Epoch =  21 Step =     220 loss = 0.289\n",
      "Epoch =  22 Step =     230 loss = 0.280\n",
      "Epoch =  23 Step =     240 loss = 0.271\n",
      "Epoch =  24 Step =     250 loss = 0.262\n",
      "Epoch =  25 Step =     260 loss = 0.255\n",
      "Epoch =  26 Step =     270 loss = 0.247\n",
      "Epoch =  27 Step =     280 loss = 0.241\n",
      "Epoch =  28 Step =     290 loss = 0.234\n",
      "Epoch =  29 Step =     300 loss = 0.228\n",
      "Epoch =  30 Step =     310 loss = 0.222\n",
      "Epoch =  31 Step =     320 loss = 0.217\n",
      "Epoch =  32 Step =     330 loss = 0.212\n",
      "Epoch =  33 Step =     340 loss = 0.207\n",
      "Epoch =  34 Step =     350 loss = 0.202\n",
      "Epoch =  35 Step =     360 loss = 0.198\n",
      "Epoch =  36 Step =     370 loss = 0.193\n",
      "Epoch =  37 Step =     380 loss = 0.189\n",
      "Epoch =  38 Step =     390 loss = 0.186\n",
      "Epoch =  39 Step =     400 loss = 0.182\n",
      "Epoch =  40 Step =     410 loss = 0.178\n",
      "Epoch =  40 Step =     420 loss = 0.175\n",
      "Epoch =  41 Step =     430 loss = 0.172\n",
      "Epoch =  42 Step =     440 loss = 0.168\n",
      "Epoch =  43 Step =     450 loss = 0.165\n",
      "Epoch =  44 Step =     460 loss = 0.163\n",
      "Epoch =  45 Step =     470 loss = 0.160\n",
      "Epoch =  46 Step =     480 loss = 0.157\n",
      "Epoch =  47 Step =     490 loss = 0.154\n",
      "Epoch =  48 Step =     500 loss = 0.152\n",
      "Epoch =  49 Step =     510 loss = 0.150\n",
      "Epoch =  50 Step =     520 loss = 0.147\n",
      "Epoch =  51 Step =     530 loss = 0.145\n",
      "Epoch =  52 Step =     540 loss = 0.143\n",
      "Epoch =  53 Step =     550 loss = 0.141\n",
      "Epoch =  54 Step =     560 loss = 0.139\n",
      "Epoch =  55 Step =     570 loss = 0.137\n",
      "Epoch =  56 Step =     580 loss = 0.135\n",
      "Epoch =  57 Step =     590 loss = 0.133\n",
      "Epoch =  58 Step =     600 loss = 0.132\n",
      "Epoch =  59 Step =     610 loss = 0.130\n",
      "Epoch =  60 Step =     620 loss = 0.128\n",
      "Epoch =  60 Step =     630 loss = 0.126\n",
      "Epoch =  61 Step =     640 loss = 0.125\n",
      "Epoch =  62 Step =     650 loss = 0.123\n",
      "Epoch =  63 Step =     660 loss = 0.122\n",
      "Epoch =  64 Step =     670 loss = 0.120\n",
      "Epoch =  65 Step =     680 loss = 0.119\n",
      "Epoch =  66 Step =     690 loss = 0.117\n",
      "Epoch =  67 Step =     700 loss = 0.116\n",
      "Epoch =  68 Step =     710 loss = 0.115\n",
      "Epoch =  69 Step =     720 loss = 0.113\n",
      "Epoch =  70 Step =     730 loss = 0.112\n",
      "Epoch =  71 Step =     740 loss = 0.111\n",
      "Epoch =  72 Step =     750 loss = 0.109\n",
      "Epoch =  73 Step =     760 loss = 0.108\n",
      "Epoch =  74 Step =     770 loss = 0.107\n",
      "Epoch =  75 Step =     780 loss = 0.106\n",
      "Epoch =  76 Step =     790 loss = 0.105\n",
      "Epoch =  77 Step =     800 loss = 0.104\n",
      "Epoch =  78 Step =     810 loss = 0.103\n",
      "Epoch =  79 Step =     820 loss = 0.102\n",
      "Epoch =  80 Step =     830 loss = 0.101\n",
      "Epoch =  80 Step =     840 loss = 0.100\n",
      "Epoch =  81 Step =     850 loss = 0.099\n",
      "Epoch =  82 Step =     860 loss = 0.098\n",
      "Epoch =  83 Step =     870 loss = 0.097\n",
      "Epoch =  84 Step =     880 loss = 0.096\n",
      "Epoch =  85 Step =     890 loss = 0.095\n",
      "Epoch =  86 Step =     900 loss = 0.094\n",
      "Epoch =  87 Step =     910 loss = 0.093\n",
      "Epoch =  88 Step =     920 loss = 0.093\n",
      "Epoch =  89 Step =     930 loss = 0.092\n",
      "Epoch =  90 Step =     940 loss = 0.091\n",
      "Epoch =  91 Step =     950 loss = 0.090\n",
      "Epoch =  92 Step =     960 loss = 0.089\n",
      "Epoch =  93 Step =     970 loss = 0.089\n",
      "Epoch =  94 Step =     980 loss = 0.088\n",
      "Epoch =  95 Step =     990 loss = 0.087\n",
      "Epoch =  96 Step =    1000 loss = 0.087\n",
      "Epoch =  97 Step =    1010 loss = 0.086\n",
      "Epoch =  98 Step =    1020 loss = 0.085\n",
      "Epoch =  99 Step =    1030 loss = 0.085\n",
      "Epoch = 100 Step =    1040 loss = 0.084\n",
      "Epoch = 100 Step =    1050 loss = 0.083\n",
      "Epoch = 101 Step =    1060 loss = 0.083\n",
      "Epoch = 102 Step =    1070 loss = 0.082\n",
      "Epoch = 103 Step =    1080 loss = 0.081\n",
      "Epoch = 104 Step =    1090 loss = 0.081\n",
      "Epoch = 105 Step =    1100 loss = 0.080\n",
      "Epoch = 106 Step =    1110 loss = 0.080\n",
      "Epoch = 107 Step =    1120 loss = 0.079\n",
      "Epoch = 108 Step =    1130 loss = 0.078\n",
      "Epoch = 109 Step =    1140 loss = 0.078\n",
      "Epoch = 110 Step =    1150 loss = 0.077\n",
      "Epoch = 111 Step =    1160 loss = 0.077\n",
      "Epoch = 112 Step =    1170 loss = 0.076\n",
      "Epoch = 113 Step =    1180 loss = 0.076\n",
      "Epoch = 114 Step =    1190 loss = 0.075\n",
      "Epoch = 115 Step =    1200 loss = 0.075\n",
      "Epoch = 116 Step =    1210 loss = 0.074\n",
      "Epoch = 117 Step =    1220 loss = 0.074\n",
      "Epoch = 118 Step =    1230 loss = 0.073\n",
      "Epoch = 119 Step =    1240 loss = 0.073\n",
      "Epoch = 120 Step =    1250 loss = 0.073\n",
      "Epoch = 120 Step =    1260 loss = 0.072\n",
      "Epoch = 121 Step =    1270 loss = 0.072\n",
      "Epoch = 122 Step =    1280 loss = 0.071\n",
      "Epoch = 123 Step =    1290 loss = 0.071\n",
      "Epoch = 124 Step =    1300 loss = 0.070\n",
      "Epoch = 125 Step =    1310 loss = 0.070\n",
      "Epoch = 126 Step =    1320 loss = 0.069\n",
      "Epoch = 127 Step =    1330 loss = 0.069\n",
      "Epoch = 128 Step =    1340 loss = 0.069\n",
      "Epoch = 129 Step =    1350 loss = 0.068\n",
      "Epoch = 130 Step =    1360 loss = 0.068\n",
      "Epoch = 131 Step =    1370 loss = 0.067\n",
      "Epoch = 132 Step =    1380 loss = 0.067\n",
      "Epoch = 133 Step =    1390 loss = 0.067\n",
      "Epoch = 134 Step =    1400 loss = 0.066\n",
      "Epoch = 135 Step =    1410 loss = 0.066\n",
      "Epoch = 136 Step =    1420 loss = 0.066\n",
      "Epoch = 137 Step =    1430 loss = 0.065\n",
      "Epoch = 138 Step =    1440 loss = 0.065\n",
      "Epoch = 139 Step =    1450 loss = 0.065\n",
      "Epoch = 140 Step =    1460 loss = 0.064\n",
      "Epoch = 140 Step =    1470 loss = 0.064\n",
      "Epoch = 141 Step =    1480 loss = 0.064\n",
      "Epoch = 142 Step =    1490 loss = 0.063\n",
      "Epoch = 143 Step =    1500 loss = 0.063\n",
      "Epoch = 144 Step =    1510 loss = 0.063\n",
      "Epoch = 145 Step =    1520 loss = 0.062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 146 Step =    1530 loss = 0.062\n",
      "Epoch = 147 Step =    1540 loss = 0.062\n",
      "Epoch = 148 Step =    1550 loss = 0.061\n",
      "Epoch = 149 Step =    1560 loss = 0.061\n",
      "Epoch = 150 Step =    1570 loss = 0.061\n",
      "Epoch = 151 Step =    1580 loss = 0.060\n",
      "Epoch = 152 Step =    1590 loss = 0.060\n",
      "Epoch = 153 Step =    1600 loss = 0.060\n",
      "Epoch = 154 Step =    1610 loss = 0.060\n",
      "Epoch = 155 Step =    1620 loss = 0.059\n",
      "Epoch = 156 Step =    1630 loss = 0.059\n",
      "Epoch = 157 Step =    1640 loss = 0.059\n",
      "Epoch = 158 Step =    1650 loss = 0.058\n",
      "Epoch = 159 Step =    1660 loss = 0.058\n",
      "Epoch = 160 Step =    1670 loss = 0.058\n",
      "Epoch = 160 Step =    1680 loss = 0.058\n",
      "Epoch = 161 Step =    1690 loss = 0.057\n",
      "Epoch = 162 Step =    1700 loss = 0.057\n",
      "Epoch = 163 Step =    1710 loss = 0.057\n",
      "Epoch = 164 Step =    1720 loss = 0.057\n",
      "Epoch = 165 Step =    1730 loss = 0.056\n",
      "Epoch = 166 Step =    1740 loss = 0.056\n",
      "Epoch = 167 Step =    1750 loss = 0.056\n",
      "Epoch = 168 Step =    1760 loss = 0.056\n",
      "Epoch = 169 Step =    1770 loss = 0.055\n",
      "Epoch = 170 Step =    1780 loss = 0.055\n",
      "Epoch = 171 Step =    1790 loss = 0.055\n",
      "Epoch = 172 Step =    1800 loss = 0.055\n",
      "Epoch = 173 Step =    1810 loss = 0.055\n",
      "Epoch = 174 Step =    1820 loss = 0.054\n",
      "Epoch = 175 Step =    1830 loss = 0.054\n",
      "Epoch = 176 Step =    1840 loss = 0.054\n",
      "Epoch = 177 Step =    1850 loss = 0.054\n",
      "INFO:tensorflow:model/global_step/sec: 15.4488\n",
      "Epoch = 178 Step =    1860 loss = 0.053\n",
      "Epoch = 179 Step =    1870 loss = 0.053\n",
      "Epoch = 180 Step =    1880 loss = 0.053\n",
      "Epoch = 180 Step =    1890 loss = 0.053\n",
      "Epoch = 181 Step =    1900 loss = 0.053\n",
      "Epoch = 182 Step =    1910 loss = 0.053\n",
      "Epoch = 183 Step =    1920 loss = 0.052\n",
      "Epoch = 184 Step =    1930 loss = 0.052\n",
      "Epoch = 185 Step =    1940 loss = 0.052\n",
      "Epoch = 186 Step =    1950 loss = 0.052\n",
      "Epoch = 187 Step =    1960 loss = 0.052\n",
      "Epoch = 188 Step =    1970 loss = 0.051\n",
      "Epoch = 189 Step =    1980 loss = 0.051\n",
      "Epoch = 190 Step =    1990 loss = 0.051\n",
      "Epoch = 191 Step =    2000 loss = 0.051\n",
      "Epoch = 192 Step =    2010 loss = 0.051\n",
      "Epoch = 193 Step =    2020 loss = 0.051\n",
      "Epoch = 194 Step =    2030 loss = 0.050\n",
      "Epoch = 195 Step =    2040 loss = 0.050\n",
      "Epoch = 196 Step =    2050 loss = 0.050\n",
      "Epoch = 197 Step =    2060 loss = 0.050\n",
      "Epoch = 198 Step =    2070 loss = 0.050\n",
      "Epoch = 199 Step =    2080 loss = 0.050\n",
      "Epoch = 200 Step =    2090 loss = 0.049\n",
      "Epoch = 200 Step =    2100 loss = 0.049\n",
      "Epoch = 201 Step =    2110 loss = 0.049\n",
      "Epoch = 202 Step =    2120 loss = 0.049\n",
      "Epoch = 203 Step =    2130 loss = 0.049\n",
      "Epoch = 204 Step =    2140 loss = 0.049\n",
      "Epoch = 205 Step =    2150 loss = 0.048\n",
      "Epoch = 206 Step =    2160 loss = 0.048\n",
      "Epoch = 207 Step =    2170 loss = 0.048\n",
      "Epoch = 208 Step =    2180 loss = 0.048\n",
      "Epoch = 209 Step =    2190 loss = 0.048\n",
      "Epoch = 210 Step =    2200 loss = 0.048\n",
      "Epoch = 211 Step =    2210 loss = 0.048\n",
      "Epoch = 212 Step =    2220 loss = 0.047\n",
      "Epoch = 213 Step =    2230 loss = 0.047\n",
      "Epoch = 214 Step =    2240 loss = 0.047\n",
      "Epoch = 215 Step =    2250 loss = 0.047\n",
      "Epoch = 216 Step =    2260 loss = 0.047\n",
      "Epoch = 217 Step =    2270 loss = 0.047\n",
      "Epoch = 218 Step =    2280 loss = 0.047\n",
      "Epoch = 219 Step =    2290 loss = 0.046\n",
      "Epoch = 220 Step =    2300 loss = 0.046\n",
      "Epoch = 220 Step =    2310 loss = 0.046\n",
      "Epoch = 221 Step =    2320 loss = 0.046\n",
      "Epoch = 222 Step =    2330 loss = 0.046\n",
      "Epoch = 223 Step =    2340 loss = 0.046\n",
      "Epoch = 224 Step =    2350 loss = 0.046\n",
      "Epoch = 225 Step =    2360 loss = 0.045\n",
      "Epoch = 226 Step =    2370 loss = 0.045\n",
      "Epoch = 227 Step =    2380 loss = 0.045\n",
      "Epoch = 228 Step =    2390 loss = 0.045\n",
      "Epoch = 229 Step =    2400 loss = 0.045\n",
      "Epoch = 230 Step =    2410 loss = 0.045\n",
      "Epoch = 231 Step =    2420 loss = 0.045\n",
      "Epoch = 232 Step =    2430 loss = 0.045\n",
      "Epoch = 233 Step =    2440 loss = 0.044\n",
      "Epoch = 234 Step =    2450 loss = 0.044\n",
      "Epoch = 235 Step =    2460 loss = 0.044\n",
      "Epoch = 236 Step =    2470 loss = 0.044\n",
      "Epoch = 237 Step =    2480 loss = 0.044\n",
      "Epoch = 238 Step =    2490 loss = 0.044\n",
      "Epoch = 239 Step =    2500 loss = 0.044\n",
      "Epoch = 240 Step =    2510 loss = 0.044\n",
      "Epoch = 240 Step =    2520 loss = 0.044\n",
      "Epoch = 241 Step =    2530 loss = 0.043\n",
      "Epoch = 242 Step =    2540 loss = 0.043\n",
      "Epoch = 243 Step =    2550 loss = 0.043\n",
      "Epoch = 244 Step =    2560 loss = 0.043\n",
      "Epoch = 245 Step =    2570 loss = 0.043\n",
      "Epoch = 246 Step =    2580 loss = 0.043\n",
      "Epoch = 247 Step =    2590 loss = 0.043\n",
      "Epoch = 248 Step =    2600 loss = 0.043\n",
      "Epoch = 249 Step =    2610 loss = 0.043\n",
      "Epoch = 250 Step =    2620 loss = 0.043\n",
      "Epoch = 251 Step =    2630 loss = 0.042\n",
      "Epoch = 252 Step =    2640 loss = 0.042\n",
      "Epoch = 253 Step =    2650 loss = 0.042\n",
      "Epoch = 254 Step =    2660 loss = 0.042\n",
      "Epoch = 255 Step =    2670 loss = 0.042\n",
      "Epoch = 256 Step =    2680 loss = 0.042\n",
      "Epoch = 257 Step =    2690 loss = 0.042\n",
      "Epoch = 258 Step =    2700 loss = 0.042\n",
      "Epoch = 259 Step =    2710 loss = 0.042\n",
      "Epoch = 260 Step =    2720 loss = 0.042\n",
      "Epoch = 260 Step =    2730 loss = 0.042\n",
      "Epoch = 261 Step =    2740 loss = 0.041\n",
      "Epoch = 262 Step =    2750 loss = 0.041\n",
      "Epoch = 263 Step =    2760 loss = 0.041\n",
      "Epoch = 264 Step =    2770 loss = 0.041\n",
      "Epoch = 265 Step =    2780 loss = 0.041\n",
      "Epoch = 266 Step =    2790 loss = 0.041\n",
      "Epoch = 267 Step =    2800 loss = 0.041\n",
      "Epoch = 268 Step =    2810 loss = 0.041\n",
      "Epoch = 269 Step =    2820 loss = 0.041\n",
      "Epoch = 270 Step =    2830 loss = 0.041\n",
      "Epoch = 271 Step =    2840 loss = 0.041\n",
      "Epoch = 272 Step =    2850 loss = 0.041\n",
      "Epoch = 273 Step =    2860 loss = 0.040\n",
      "Epoch = 274 Step =    2870 loss = 0.040\n",
      "Epoch = 275 Step =    2880 loss = 0.040\n",
      "Epoch = 276 Step =    2890 loss = 0.040\n",
      "Epoch = 277 Step =    2900 loss = 0.040\n",
      "Epoch = 278 Step =    2910 loss = 0.040\n",
      "Epoch = 279 Step =    2920 loss = 0.040\n",
      "Epoch = 280 Step =    2930 loss = 0.040\n",
      "Epoch = 280 Step =    2940 loss = 0.040\n",
      "Epoch = 281 Step =    2950 loss = 0.040\n",
      "Epoch = 282 Step =    2960 loss = 0.040\n",
      "Epoch = 283 Step =    2970 loss = 0.040\n",
      "Epoch = 284 Step =    2980 loss = 0.040\n",
      "Epoch = 285 Step =    2990 loss = 0.039\n",
      "Epoch = 286 Step =    3000 loss = 0.039\n",
      "Epoch = 287 Step =    3010 loss = 0.039\n",
      "Epoch = 288 Step =    3020 loss = 0.039\n",
      "Epoch = 289 Step =    3030 loss = 0.039\n",
      "Epoch = 290 Step =    3040 loss = 0.039\n",
      "Epoch = 291 Step =    3050 loss = 0.039\n",
      "Epoch = 292 Step =    3060 loss = 0.039\n",
      "Epoch = 293 Step =    3070 loss = 0.039\n",
      "Epoch = 294 Step =    3080 loss = 0.039\n",
      "Epoch = 295 Step =    3090 loss = 0.039\n",
      "Epoch = 296 Step =    3100 loss = 0.039\n",
      "Epoch = 297 Step =    3110 loss = 0.039\n",
      "Epoch = 298 Step =    3120 loss = 0.039\n",
      "Epoch = 299 Step =    3130 loss = 0.038\n",
      "Epoch = 300 Step =    3140 loss = 0.038\n",
      "Epoch = 300 Step =    3150 loss = 0.038\n",
      "Epoch = 301 Step =    3160 loss = 0.038\n",
      "Epoch = 302 Step =    3170 loss = 0.038\n",
      "Epoch = 303 Step =    3180 loss = 0.038\n",
      "Epoch = 304 Step =    3190 loss = 0.038\n",
      "Epoch = 305 Step =    3200 loss = 0.038\n",
      "Epoch = 306 Step =    3210 loss = 0.038\n",
      "Epoch = 307 Step =    3220 loss = 0.038\n",
      "Epoch = 308 Step =    3230 loss = 0.038\n",
      "Epoch = 309 Step =    3240 loss = 0.038\n",
      "Epoch = 310 Step =    3250 loss = 0.038\n",
      "Epoch = 311 Step =    3260 loss = 0.038\n",
      "Epoch = 312 Step =    3270 loss = 0.038\n",
      "Epoch = 313 Step =    3280 loss = 0.038\n",
      "Epoch = 314 Step =    3290 loss = 0.038\n",
      "Epoch = 315 Step =    3300 loss = 0.037\n",
      "Epoch = 316 Step =    3310 loss = 0.037\n",
      "Epoch = 317 Step =    3320 loss = 0.037\n",
      "Epoch = 318 Step =    3330 loss = 0.037\n",
      "Epoch = 319 Step =    3340 loss = 0.037\n",
      "Epoch = 320 Step =    3350 loss = 0.037\n",
      "Epoch = 320 Step =    3360 loss = 0.037\n",
      "Epoch = 321 Step =    3370 loss = 0.037\n",
      "Epoch = 322 Step =    3380 loss = 0.037\n",
      "Epoch = 323 Step =    3390 loss = 0.037\n",
      "Epoch = 324 Step =    3400 loss = 0.037\n",
      "Epoch = 325 Step =    3410 loss = 0.037\n",
      "Epoch = 326 Step =    3420 loss = 0.037\n",
      "Epoch = 327 Step =    3430 loss = 0.037\n",
      "Epoch = 328 Step =    3440 loss = 0.037\n",
      "Epoch = 329 Step =    3450 loss = 0.037\n",
      "Epoch = 330 Step =    3460 loss = 0.037\n",
      "Epoch = 331 Step =    3470 loss = 0.036\n",
      "Epoch = 332 Step =    3480 loss = 0.036\n",
      "Epoch = 333 Step =    3490 loss = 0.036\n",
      "Epoch = 334 Step =    3500 loss = 0.036\n",
      "Epoch = 335 Step =    3510 loss = 0.036\n",
      "Epoch = 336 Step =    3520 loss = 0.036\n",
      "Epoch = 337 Step =    3530 loss = 0.036\n",
      "Epoch = 338 Step =    3540 loss = 0.036\n",
      "Epoch = 339 Step =    3550 loss = 0.036\n",
      "Epoch = 340 Step =    3560 loss = 0.036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 340 Step =    3570 loss = 0.036\n",
      "Epoch = 341 Step =    3580 loss = 0.036\n",
      "Epoch = 342 Step =    3590 loss = 0.036\n",
      "Epoch = 343 Step =    3600 loss = 0.036\n",
      "Epoch = 344 Step =    3610 loss = 0.036\n",
      "Epoch = 345 Step =    3620 loss = 0.036\n",
      "Epoch = 346 Step =    3630 loss = 0.036\n",
      "Epoch = 347 Step =    3640 loss = 0.036\n",
      "Epoch = 348 Step =    3650 loss = 0.036\n",
      "Epoch = 349 Step =    3660 loss = 0.035\n",
      "Epoch = 350 Step =    3670 loss = 0.035\n",
      "Epoch = 351 Step =    3680 loss = 0.035\n",
      "Epoch = 352 Step =    3690 loss = 0.035\n",
      "Epoch = 353 Step =    3700 loss = 0.035\n",
      "Epoch = 354 Step =    3710 loss = 0.035\n",
      "Epoch = 355 Step =    3720 loss = 0.035\n",
      "Epoch = 356 Step =    3730 loss = 0.035\n",
      "Epoch = 357 Step =    3740 loss = 0.035\n",
      "Epoch = 358 Step =    3750 loss = 0.035\n",
      "Epoch = 359 Step =    3760 loss = 0.035\n",
      "Epoch = 360 Step =    3770 loss = 0.035\n",
      "Epoch = 360 Step =    3780 loss = 0.035\n",
      "Epoch = 361 Step =    3790 loss = 0.035\n",
      "Epoch = 362 Step =    3800 loss = 0.035\n",
      "Epoch = 363 Step =    3810 loss = 0.035\n",
      "Epoch = 364 Step =    3820 loss = 0.035\n",
      "Epoch = 365 Step =    3830 loss = 0.035\n",
      "INFO:tensorflow:model/global_step/sec: 16.4844\n",
      "Epoch = 366 Step =    3840 loss = 0.035\n",
      "Epoch = 367 Step =    3850 loss = 0.035\n",
      "Epoch = 368 Step =    3860 loss = 0.035\n",
      "Epoch = 369 Step =    3870 loss = 0.035\n",
      "Epoch = 370 Step =    3880 loss = 0.035\n",
      "Epoch = 371 Step =    3890 loss = 0.034\n",
      "Epoch = 372 Step =    3900 loss = 0.034\n",
      "Epoch = 373 Step =    3910 loss = 0.034\n",
      "Epoch = 374 Step =    3920 loss = 0.034\n",
      "Epoch = 375 Step =    3930 loss = 0.034\n",
      "Epoch = 376 Step =    3940 loss = 0.034\n",
      "Epoch = 377 Step =    3950 loss = 0.034\n",
      "Epoch = 378 Step =    3960 loss = 0.034\n",
      "Epoch = 379 Step =    3970 loss = 0.034\n",
      "Epoch = 380 Step =    3980 loss = 0.034\n",
      "Epoch = 380 Step =    3990 loss = 0.034\n",
      "Epoch = 381 Step =    4000 loss = 0.034\n",
      "Epoch = 382 Step =    4010 loss = 0.034\n",
      "Epoch = 383 Step =    4020 loss = 0.034\n",
      "Epoch = 384 Step =    4030 loss = 0.034\n",
      "Epoch = 385 Step =    4040 loss = 0.034\n",
      "Epoch = 386 Step =    4050 loss = 0.034\n",
      "Epoch = 387 Step =    4060 loss = 0.034\n",
      "Epoch = 388 Step =    4070 loss = 0.034\n",
      "Epoch = 389 Step =    4080 loss = 0.034\n",
      "Epoch = 390 Step =    4090 loss = 0.034\n",
      "Epoch = 391 Step =    4100 loss = 0.034\n",
      "Epoch = 392 Step =    4110 loss = 0.034\n",
      "Epoch = 393 Step =    4120 loss = 0.034\n",
      "Epoch = 394 Step =    4130 loss = 0.034\n",
      "Epoch = 395 Step =    4140 loss = 0.034\n",
      "Epoch = 396 Step =    4150 loss = 0.034\n",
      "Epoch = 397 Step =    4160 loss = 0.034\n",
      "Epoch = 398 Step =    4170 loss = 0.033\n",
      "Epoch = 399 Step =    4180 loss = 0.033\n",
      "Epoch = 400 Step =    4190 loss = 0.033\n",
      "Epoch = 400 Step =    4200 loss = 0.033\n",
      "Epoch = 401 Step =    4210 loss = 0.033\n",
      "Epoch = 402 Step =    4220 loss = 0.033\n",
      "Epoch = 403 Step =    4230 loss = 0.033\n",
      "Epoch = 404 Step =    4240 loss = 0.033\n",
      "Epoch = 405 Step =    4250 loss = 0.033\n",
      "Epoch = 406 Step =    4260 loss = 0.033\n",
      "Epoch = 407 Step =    4270 loss = 0.033\n",
      "Epoch = 408 Step =    4280 loss = 0.033\n",
      "Epoch = 409 Step =    4290 loss = 0.033\n",
      "Epoch = 410 Step =    4300 loss = 0.033\n",
      "Epoch = 411 Step =    4310 loss = 0.033\n",
      "Epoch = 412 Step =    4320 loss = 0.033\n",
      "Epoch = 413 Step =    4330 loss = 0.033\n",
      "Epoch = 414 Step =    4340 loss = 0.033\n",
      "Epoch = 415 Step =    4350 loss = 0.033\n",
      "Epoch = 416 Step =    4360 loss = 0.033\n",
      "Epoch = 417 Step =    4370 loss = 0.033\n",
      "Epoch = 418 Step =    4380 loss = 0.033\n",
      "Epoch = 419 Step =    4390 loss = 0.033\n",
      "Epoch = 420 Step =    4400 loss = 0.033\n",
      "Epoch = 420 Step =    4410 loss = 0.033\n",
      "Epoch = 421 Step =    4420 loss = 0.033\n",
      "Epoch = 422 Step =    4430 loss = 0.033\n",
      "Epoch = 423 Step =    4440 loss = 0.033\n",
      "Epoch = 424 Step =    4450 loss = 0.033\n",
      "Epoch = 425 Step =    4460 loss = 0.033\n",
      "Epoch = 426 Step =    4470 loss = 0.033\n",
      "Epoch = 427 Step =    4480 loss = 0.033\n",
      "Epoch = 428 Step =    4490 loss = 0.033\n",
      "Epoch = 429 Step =    4500 loss = 0.033\n",
      "Epoch = 430 Step =    4510 loss = 0.032\n",
      "Epoch = 431 Step =    4520 loss = 0.032\n",
      "Epoch = 432 Step =    4530 loss = 0.032\n",
      "Epoch = 433 Step =    4540 loss = 0.032\n",
      "Epoch = 434 Step =    4550 loss = 0.032\n",
      "Epoch = 435 Step =    4560 loss = 0.032\n",
      "Epoch = 436 Step =    4570 loss = 0.032\n",
      "Epoch = 437 Step =    4580 loss = 0.032\n",
      "Epoch = 438 Step =    4590 loss = 0.032\n",
      "Epoch = 439 Step =    4600 loss = 0.032\n",
      "Epoch = 440 Step =    4610 loss = 0.032\n",
      "Epoch = 440 Step =    4620 loss = 0.032\n",
      "Epoch = 441 Step =    4630 loss = 0.032\n",
      "Epoch = 442 Step =    4640 loss = 0.032\n",
      "Epoch = 443 Step =    4650 loss = 0.032\n",
      "Epoch = 444 Step =    4660 loss = 0.032\n",
      "Epoch = 445 Step =    4670 loss = 0.032\n",
      "Epoch = 446 Step =    4680 loss = 0.032\n",
      "Epoch = 447 Step =    4690 loss = 0.032\n",
      "Epoch = 448 Step =    4700 loss = 0.032\n",
      "Epoch = 449 Step =    4710 loss = 0.032\n",
      "Epoch = 450 Step =    4720 loss = 0.032\n",
      "Epoch = 451 Step =    4730 loss = 0.032\n",
      "Epoch = 452 Step =    4740 loss = 0.032\n",
      "Epoch = 453 Step =    4750 loss = 0.032\n",
      "Epoch = 454 Step =    4760 loss = 0.032\n",
      "Epoch = 455 Step =    4770 loss = 0.032\n",
      "Epoch = 456 Step =    4780 loss = 0.032\n",
      "Epoch = 457 Step =    4790 loss = 0.032\n",
      "Epoch = 458 Step =    4800 loss = 0.032\n",
      "Epoch = 459 Step =    4810 loss = 0.032\n",
      "Epoch = 460 Step =    4820 loss = 0.032\n",
      "Epoch = 460 Step =    4830 loss = 0.032\n",
      "Epoch = 461 Step =    4840 loss = 0.032\n",
      "Epoch = 462 Step =    4850 loss = 0.032\n",
      "Epoch = 463 Step =    4860 loss = 0.032\n",
      "Epoch = 464 Step =    4870 loss = 0.032\n",
      "Epoch = 465 Step =    4880 loss = 0.032\n",
      "Epoch = 466 Step =    4890 loss = 0.032\n",
      "Epoch = 467 Step =    4900 loss = 0.032\n",
      "Epoch = 468 Step =    4910 loss = 0.032\n",
      "Epoch = 469 Step =    4920 loss = 0.032\n",
      "Epoch = 470 Step =    4930 loss = 0.032\n",
      "Epoch = 471 Step =    4940 loss = 0.032\n",
      "Epoch = 472 Step =    4950 loss = 0.032\n",
      "Epoch = 473 Step =    4960 loss = 0.032\n",
      "Epoch = 474 Step =    4970 loss = 0.032\n",
      "Epoch = 475 Step =    4980 loss = 0.032\n",
      "Epoch = 476 Step =    4990 loss = 0.031\n",
      "Epoch = 477 Step =    5000 loss = 0.031\n",
      "Epoch = 478 Step =    5010 loss = 0.031\n",
      "Epoch = 479 Step =    5020 loss = 0.031\n",
      "Epoch = 480 Step =    5030 loss = 0.031\n",
      "Epoch = 480 Step =    5040 loss = 0.031\n",
      "Epoch = 481 Step =    5050 loss = 0.031\n",
      "Epoch = 482 Step =    5060 loss = 0.031\n",
      "Epoch = 483 Step =    5070 loss = 0.031\n",
      "Epoch = 484 Step =    5080 loss = 0.031\n",
      "Epoch = 485 Step =    5090 loss = 0.031\n",
      "Epoch = 486 Step =    5100 loss = 0.031\n",
      "Epoch = 487 Step =    5110 loss = 0.031\n",
      "Epoch = 488 Step =    5120 loss = 0.031\n",
      "Epoch = 489 Step =    5130 loss = 0.031\n",
      "Epoch = 490 Step =    5140 loss = 0.031\n",
      "Epoch = 491 Step =    5150 loss = 0.031\n",
      "Epoch = 492 Step =    5160 loss = 0.031\n",
      "Epoch = 493 Step =    5170 loss = 0.031\n",
      "Epoch = 494 Step =    5180 loss = 0.031\n",
      "Epoch = 495 Step =    5190 loss = 0.031\n",
      "Epoch = 496 Step =    5200 loss = 0.031\n",
      "Epoch = 497 Step =    5210 loss = 0.031\n",
      "Epoch = 498 Step =    5220 loss = 0.031\n",
      "Epoch = 499 Step =    5230 loss = 0.031\n",
      "Epoch = 500 Step =    5240 loss = 0.031\n",
      "Epoch = 500 Step =    5250 loss = 0.031\n",
      "Epoch = 501 Step =    5260 loss = 0.031\n",
      "Epoch = 502 Step =    5270 loss = 0.031\n",
      "Epoch = 503 Step =    5280 loss = 0.031\n",
      "Epoch = 504 Step =    5290 loss = 0.031\n",
      "Epoch = 505 Step =    5300 loss = 0.031\n",
      "Epoch = 506 Step =    5310 loss = 0.031\n",
      "Epoch = 507 Step =    5320 loss = 0.031\n",
      "Epoch = 508 Step =    5330 loss = 0.031\n",
      "Epoch = 509 Step =    5340 loss = 0.031\n",
      "Epoch = 510 Step =    5350 loss = 0.031\n",
      "Epoch = 511 Step =    5360 loss = 0.031\n",
      "Epoch = 512 Step =    5370 loss = 0.031\n",
      "Epoch = 513 Step =    5380 loss = 0.031\n",
      "Epoch = 514 Step =    5390 loss = 0.031\n",
      "Epoch = 515 Step =    5400 loss = 0.031\n",
      "Epoch = 516 Step =    5410 loss = 0.031\n",
      "Epoch = 517 Step =    5420 loss = 0.031\n",
      "Epoch = 518 Step =    5430 loss = 0.031\n",
      "Epoch = 519 Step =    5440 loss = 0.031\n",
      "Epoch = 520 Step =    5450 loss = 0.031\n",
      "Epoch = 520 Step =    5460 loss = 0.031\n",
      "Epoch = 521 Step =    5470 loss = 0.031\n",
      "Epoch = 522 Step =    5480 loss = 0.031\n",
      "Epoch = 523 Step =    5490 loss = 0.031\n",
      "Epoch = 524 Step =    5500 loss = 0.031\n",
      "Epoch = 525 Step =    5510 loss = 0.031\n",
      "Epoch = 526 Step =    5520 loss = 0.031\n",
      "Epoch = 527 Step =    5530 loss = 0.031\n",
      "Epoch = 528 Step =    5540 loss = 0.031\n",
      "Epoch = 529 Step =    5550 loss = 0.031\n",
      "Epoch = 530 Step =    5560 loss = 0.031\n",
      "Epoch = 531 Step =    5570 loss = 0.031\n",
      "Epoch = 532 Step =    5580 loss = 0.031\n",
      "Epoch = 533 Step =    5590 loss = 0.031\n",
      "Epoch = 534 Step =    5600 loss = 0.031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 535 Step =    5610 loss = 0.031\n",
      "Epoch = 536 Step =    5620 loss = 0.031\n",
      "Epoch = 537 Step =    5630 loss = 0.031\n",
      "Epoch = 538 Step =    5640 loss = 0.031\n",
      "Epoch = 539 Step =    5650 loss = 0.031\n",
      "Epoch = 540 Step =    5660 loss = 0.031\n",
      "Epoch = 540 Step =    5670 loss = 0.030\n",
      "Epoch = 541 Step =    5680 loss = 0.030\n",
      "Epoch = 542 Step =    5690 loss = 0.030\n",
      "Epoch = 543 Step =    5700 loss = 0.030\n",
      "Epoch = 544 Step =    5710 loss = 0.030\n",
      "Epoch = 545 Step =    5720 loss = 0.030\n",
      "Epoch = 546 Step =    5730 loss = 0.030\n",
      "Epoch = 547 Step =    5740 loss = 0.030\n",
      "Epoch = 548 Step =    5750 loss = 0.030\n",
      "Epoch = 549 Step =    5760 loss = 0.030\n",
      "Epoch = 550 Step =    5770 loss = 0.030\n",
      "Epoch = 551 Step =    5780 loss = 0.030\n",
      "INFO:tensorflow:model/global_step/sec: 16.2322\n",
      "Epoch = 552 Step =    5790 loss = 0.030\n",
      "Epoch = 553 Step =    5800 loss = 0.030\n",
      "Epoch = 554 Step =    5810 loss = 0.030\n",
      "Epoch = 555 Step =    5820 loss = 0.030\n",
      "Epoch = 556 Step =    5830 loss = 0.030\n",
      "Epoch = 557 Step =    5840 loss = 0.030\n",
      "Epoch = 558 Step =    5850 loss = 0.030\n",
      "Epoch = 559 Step =    5860 loss = 0.030\n",
      "Epoch = 560 Step =    5870 loss = 0.030\n",
      "Epoch = 560 Step =    5880 loss = 0.030\n",
      "Epoch = 561 Step =    5890 loss = 0.030\n",
      "Epoch = 562 Step =    5900 loss = 0.030\n",
      "Epoch = 563 Step =    5910 loss = 0.030\n",
      "Epoch = 564 Step =    5920 loss = 0.030\n",
      "Epoch = 565 Step =    5930 loss = 0.030\n",
      "Epoch = 566 Step =    5940 loss = 0.031\n",
      "Epoch = 567 Step =    5950 loss = 0.031\n",
      "Epoch = 568 Step =    5960 loss = 0.031\n",
      "Epoch = 569 Step =    5970 loss = 0.031\n",
      "Epoch = 570 Step =    5980 loss = 0.031\n",
      "Epoch = 571 Step =    5990 loss = 0.031\n",
      "Epoch = 572 Step =    6000 loss = 0.031\n",
      "Epoch = 573 Step =    6010 loss = 0.031\n",
      "Epoch = 574 Step =    6020 loss = 0.031\n",
      "Epoch = 575 Step =    6030 loss = 0.031\n",
      "Epoch = 576 Step =    6040 loss = 0.031\n",
      "Epoch = 577 Step =    6050 loss = 0.031\n",
      "Epoch = 578 Step =    6060 loss = 0.031\n",
      "Epoch = 579 Step =    6070 loss = 0.031\n",
      "Epoch = 580 Step =    6080 loss = 0.031\n",
      "Epoch = 580 Step =    6090 loss = 0.031\n",
      "Epoch = 581 Step =    6100 loss = 0.031\n",
      "Epoch = 582 Step =    6110 loss = 0.031\n",
      "Epoch = 583 Step =    6120 loss = 0.031\n",
      "Epoch = 584 Step =    6130 loss = 0.031\n",
      "Epoch = 585 Step =    6140 loss = 0.031\n",
      "Epoch = 586 Step =    6150 loss = 0.031\n",
      "Epoch = 587 Step =    6160 loss = 0.031\n",
      "Epoch = 588 Step =    6170 loss = 0.031\n",
      "Epoch = 589 Step =    6180 loss = 0.031\n",
      "Epoch = 590 Step =    6190 loss = 0.031\n",
      "Epoch = 591 Step =    6200 loss = 0.031\n",
      "Epoch = 592 Step =    6210 loss = 0.031\n",
      "Epoch = 593 Step =    6220 loss = 0.031\n",
      "Epoch = 594 Step =    6230 loss = 0.031\n",
      "Epoch = 595 Step =    6240 loss = 0.031\n",
      "Epoch = 596 Step =    6250 loss = 0.031\n",
      "Epoch = 597 Step =    6260 loss = 0.031\n",
      "Epoch = 598 Step =    6270 loss = 0.031\n",
      "Epoch = 599 Step =    6280 loss = 0.031\n",
      "Epoch = 600 Step =    6290 loss = 0.031\n",
      "Epoch = 600 Step =    6300 loss = 0.031\n",
      "Epoch = 601 Step =    6310 loss = 0.031\n",
      "Epoch = 602 Step =    6320 loss = 0.031\n",
      "Epoch = 603 Step =    6330 loss = 0.031\n",
      "Epoch = 604 Step =    6340 loss = 0.031\n",
      "Epoch = 605 Step =    6350 loss = 0.031\n",
      "Epoch = 606 Step =    6360 loss = 0.031\n",
      "Epoch = 607 Step =    6370 loss = 0.031\n",
      "Epoch = 608 Step =    6380 loss = 0.031\n",
      "Epoch = 609 Step =    6390 loss = 0.031\n",
      "Epoch = 610 Step =    6400 loss = 0.031\n",
      "Epoch = 611 Step =    6410 loss = 0.031\n",
      "Epoch = 612 Step =    6420 loss = 0.031\n",
      "Epoch = 613 Step =    6430 loss = 0.031\n",
      "Epoch = 614 Step =    6440 loss = 0.031\n",
      "Epoch = 615 Step =    6450 loss = 0.031\n",
      "Epoch = 616 Step =    6460 loss = 0.031\n",
      "Epoch = 617 Step =    6470 loss = 0.031\n",
      "Epoch = 618 Step =    6480 loss = 0.031\n",
      "Epoch = 619 Step =    6490 loss = 0.031\n",
      "Epoch = 620 Step =    6500 loss = 0.031\n",
      "Epoch = 620 Step =    6510 loss = 0.031\n",
      "Epoch = 621 Step =    6520 loss = 0.031\n",
      "Epoch = 622 Step =    6530 loss = 0.031\n",
      "Epoch = 623 Step =    6540 loss = 0.031\n",
      "Epoch = 624 Step =    6550 loss = 0.031\n",
      "Epoch = 625 Step =    6560 loss = 0.031\n",
      "Epoch = 626 Step =    6570 loss = 0.031\n",
      "Epoch = 627 Step =    6580 loss = 0.031\n",
      "Epoch = 628 Step =    6590 loss = 0.031\n",
      "Epoch = 629 Step =    6600 loss = 0.031\n",
      "Epoch = 630 Step =    6610 loss = 0.031\n",
      "Epoch = 631 Step =    6620 loss = 0.031\n",
      "Epoch = 632 Step =    6630 loss = 0.031\n",
      "Epoch = 633 Step =    6640 loss = 0.031\n",
      "Epoch = 634 Step =    6650 loss = 0.031\n",
      "Epoch = 635 Step =    6660 loss = 0.031\n",
      "Epoch = 636 Step =    6670 loss = 0.031\n",
      "Epoch = 637 Step =    6680 loss = 0.031\n",
      "Epoch = 638 Step =    6690 loss = 0.031\n",
      "Epoch = 639 Step =    6700 loss = 0.031\n",
      "Epoch = 640 Step =    6710 loss = 0.031\n",
      "Epoch = 640 Step =    6720 loss = 0.031\n",
      "Epoch = 641 Step =    6730 loss = 0.031\n",
      "Epoch = 642 Step =    6740 loss = 0.031\n",
      "Epoch = 643 Step =    6750 loss = 0.031\n",
      "Epoch = 644 Step =    6760 loss = 0.031\n",
      "Epoch = 645 Step =    6770 loss = 0.031\n",
      "Epoch = 646 Step =    6780 loss = 0.031\n",
      "Epoch = 647 Step =    6790 loss = 0.031\n",
      "Epoch = 648 Step =    6800 loss = 0.031\n",
      "Epoch = 649 Step =    6810 loss = 0.031\n",
      "Epoch = 650 Step =    6820 loss = 0.031\n",
      "Epoch = 651 Step =    6830 loss = 0.031\n",
      "Epoch = 652 Step =    6840 loss = 0.031\n",
      "Epoch = 653 Step =    6850 loss = 0.031\n",
      "Epoch = 654 Step =    6860 loss = 0.031\n",
      "Epoch = 655 Step =    6870 loss = 0.031\n",
      "Epoch = 656 Step =    6880 loss = 0.031\n",
      "Epoch = 657 Step =    6890 loss = 0.031\n",
      "Epoch = 658 Step =    6900 loss = 0.032\n",
      "Epoch = 659 Step =    6910 loss = 0.032\n",
      "Epoch = 660 Step =    6920 loss = 0.032\n",
      "Epoch = 660 Step =    6930 loss = 0.032\n",
      "Epoch = 661 Step =    6940 loss = 0.032\n",
      "Epoch = 662 Step =    6950 loss = 0.032\n",
      "Epoch = 663 Step =    6960 loss = 0.032\n",
      "Epoch = 664 Step =    6970 loss = 0.032\n",
      "Epoch = 665 Step =    6980 loss = 0.032\n",
      "Epoch = 666 Step =    6990 loss = 0.032\n",
      "Epoch = 667 Step =    7000 loss = 0.032\n",
      "Epoch = 668 Step =    7010 loss = 0.032\n",
      "Epoch = 669 Step =    7020 loss = 0.032\n",
      "Epoch = 670 Step =    7030 loss = 0.032\n",
      "Epoch = 671 Step =    7040 loss = 0.032\n",
      "Epoch = 672 Step =    7050 loss = 0.032\n",
      "Epoch = 673 Step =    7060 loss = 0.032\n",
      "Epoch = 674 Step =    7070 loss = 0.032\n",
      "Epoch = 675 Step =    7080 loss = 0.032\n",
      "Epoch = 676 Step =    7090 loss = 0.032\n",
      "Epoch = 677 Step =    7100 loss = 0.032\n",
      "Epoch = 678 Step =    7110 loss = 0.032\n",
      "Epoch = 679 Step =    7120 loss = 0.032\n",
      "Epoch = 680 Step =    7130 loss = 0.032\n",
      "Epoch = 680 Step =    7140 loss = 0.032\n",
      "Epoch = 681 Step =    7150 loss = 0.032\n",
      "Epoch = 682 Step =    7160 loss = 0.032\n",
      "Epoch = 683 Step =    7170 loss = 0.032\n",
      "Epoch = 684 Step =    7180 loss = 0.032\n",
      "Epoch = 685 Step =    7190 loss = 0.032\n",
      "Epoch = 686 Step =    7200 loss = 0.032\n",
      "Epoch = 687 Step =    7210 loss = 0.032\n",
      "Epoch = 688 Step =    7220 loss = 0.032\n",
      "Epoch = 689 Step =    7230 loss = 0.032\n",
      "Epoch = 690 Step =    7240 loss = 0.032\n",
      "Epoch = 691 Step =    7250 loss = 0.032\n",
      "Epoch = 692 Step =    7260 loss = 0.032\n",
      "Epoch = 693 Step =    7270 loss = 0.032\n",
      "Epoch = 694 Step =    7280 loss = 0.033\n",
      "Epoch = 695 Step =    7290 loss = 0.033\n",
      "Epoch = 696 Step =    7300 loss = 0.033\n",
      "Epoch = 697 Step =    7310 loss = 0.033\n",
      "Epoch = 698 Step =    7320 loss = 0.033\n",
      "Epoch = 699 Step =    7330 loss = 0.033\n",
      "Epoch = 700 Step =    7340 loss = 0.033\n",
      "Epoch = 700 Step =    7350 loss = 0.033\n",
      "Epoch = 701 Step =    7360 loss = 0.033\n",
      "Epoch = 702 Step =    7370 loss = 0.033\n",
      "Epoch = 703 Step =    7380 loss = 0.033\n",
      "Epoch = 704 Step =    7390 loss = 0.033\n",
      "Epoch = 705 Step =    7400 loss = 0.033\n",
      "Epoch = 706 Step =    7410 loss = 0.033\n",
      "Epoch = 707 Step =    7420 loss = 0.033\n",
      "Epoch = 708 Step =    7430 loss = 0.033\n",
      "Epoch = 709 Step =    7440 loss = 0.033\n",
      "Epoch = 710 Step =    7450 loss = 0.033\n",
      "Epoch = 711 Step =    7460 loss = 0.034\n",
      "Epoch = 712 Step =    7470 loss = 0.034\n",
      "Epoch = 713 Step =    7480 loss = 0.034\n",
      "Epoch = 714 Step =    7490 loss = 0.034\n",
      "Epoch = 715 Step =    7500 loss = 0.034\n",
      "Epoch = 716 Step =    7510 loss = 0.034\n",
      "Epoch = 717 Step =    7520 loss = 0.034\n",
      "Epoch = 718 Step =    7530 loss = 0.034\n",
      "Epoch = 719 Step =    7540 loss = 0.034\n",
      "Epoch = 720 Step =    7550 loss = 0.034\n",
      "Epoch = 720 Step =    7560 loss = 0.034\n",
      "Epoch = 721 Step =    7570 loss = 0.034\n",
      "Epoch = 722 Step =    7580 loss = 0.035\n",
      "Epoch = 723 Step =    7590 loss = 0.035\n",
      "Epoch = 724 Step =    7600 loss = 0.035\n",
      "Epoch = 725 Step =    7610 loss = 0.035\n",
      "Epoch = 726 Step =    7620 loss = 0.035\n",
      "Epoch = 727 Step =    7630 loss = 0.035\n",
      "Epoch = 728 Step =    7640 loss = 0.035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 729 Step =    7650 loss = 0.036\n",
      "Epoch = 730 Step =    7660 loss = 0.036\n",
      "Epoch = 731 Step =    7670 loss = 0.036\n",
      "Epoch = 732 Step =    7680 loss = 0.036\n",
      "Epoch = 733 Step =    7690 loss = 0.036\n",
      "Epoch = 734 Step =    7700 loss = 0.036\n",
      "Epoch = 735 Step =    7710 loss = 0.037\n",
      "Epoch = 736 Step =    7720 loss = 0.037\n",
      "Epoch = 737 Step =    7730 loss = 0.037\n",
      "INFO:tensorflow:model/global_step/sec: 16.258\n",
      "Epoch = 738 Step =    7740 loss = 0.037\n",
      "Epoch = 739 Step =    7750 loss = 0.037\n",
      "Epoch = 740 Step =    7760 loss = 0.037\n",
      "Epoch = 740 Step =    7770 loss = 0.038\n",
      "Epoch = 741 Step =    7780 loss = 0.038\n",
      "Epoch = 742 Step =    7790 loss = 0.038\n",
      "Epoch = 743 Step =    7800 loss = 0.038\n",
      "Epoch = 744 Step =    7810 loss = 0.038\n",
      "Epoch = 745 Step =    7820 loss = 0.039\n",
      "Epoch = 746 Step =    7830 loss = 0.039\n",
      "Epoch = 747 Step =    7840 loss = 0.039\n",
      "Epoch = 748 Step =    7850 loss = 0.039\n",
      "Epoch = 749 Step =    7860 loss = 0.039\n",
      "Epoch = 750 Step =    7870 loss = 0.040\n",
      "Epoch = 751 Step =    7880 loss = 0.040\n",
      "Epoch = 752 Step =    7890 loss = 0.040\n",
      "Epoch = 753 Step =    7900 loss = 0.040\n",
      "Epoch = 754 Step =    7910 loss = 0.040\n",
      "Epoch = 755 Step =    7920 loss = 0.041\n",
      "Epoch = 756 Step =    7930 loss = 0.041\n",
      "Epoch = 757 Step =    7940 loss = 0.041\n",
      "Epoch = 758 Step =    7950 loss = 0.041\n",
      "Epoch = 759 Step =    7960 loss = 0.042\n",
      "Epoch = 760 Step =    7970 loss = 0.042\n",
      "Epoch = 760 Step =    7980 loss = 0.042\n",
      "Epoch = 761 Step =    7990 loss = 0.042\n",
      "Epoch = 762 Step =    8000 loss = 0.043\n",
      "Epoch = 763 Step =    8010 loss = 0.043\n",
      "Epoch = 764 Step =    8020 loss = 0.043\n",
      "Epoch = 765 Step =    8030 loss = 0.044\n",
      "Epoch = 766 Step =    8040 loss = 0.044\n",
      "Epoch = 767 Step =    8050 loss = 0.044\n",
      "Epoch = 768 Step =    8060 loss = 0.044\n",
      "Epoch = 769 Step =    8070 loss = 0.045\n",
      "Epoch = 770 Step =    8080 loss = 0.045\n",
      "Epoch = 771 Step =    8090 loss = 0.045\n",
      "Epoch = 772 Step =    8100 loss = 0.045\n",
      "Epoch = 773 Step =    8110 loss = 0.046\n",
      "Epoch = 774 Step =    8120 loss = 0.046\n",
      "Epoch = 775 Step =    8130 loss = 0.046\n",
      "Epoch = 776 Step =    8140 loss = 0.047\n",
      "Epoch = 777 Step =    8150 loss = 0.047\n",
      "Epoch = 778 Step =    8160 loss = 0.047\n",
      "Epoch = 779 Step =    8170 loss = 0.047\n",
      "Epoch = 780 Step =    8180 loss = 0.048\n",
      "Epoch = 780 Step =    8190 loss = 0.048\n",
      "Epoch = 781 Step =    8200 loss = 0.048\n",
      "Epoch = 782 Step =    8210 loss = 0.049\n",
      "Epoch = 783 Step =    8220 loss = 0.049\n",
      "Epoch = 784 Step =    8230 loss = 0.049\n",
      "Epoch = 785 Step =    8240 loss = 0.049\n",
      "Epoch = 786 Step =    8250 loss = 0.050\n",
      "Epoch = 787 Step =    8260 loss = 0.050\n",
      "Epoch = 788 Step =    8270 loss = 0.050\n",
      "Epoch = 789 Step =    8280 loss = 0.051\n",
      "Epoch = 790 Step =    8290 loss = 0.051\n",
      "Epoch = 791 Step =    8300 loss = 0.051\n",
      "Epoch = 792 Step =    8310 loss = 0.052\n",
      "Epoch = 793 Step =    8320 loss = 0.052\n",
      "Epoch = 794 Step =    8330 loss = 0.052\n",
      "Epoch = 795 Step =    8340 loss = 0.052\n",
      "Epoch = 796 Step =    8350 loss = 0.053\n",
      "Epoch = 797 Step =    8360 loss = 0.053\n",
      "Epoch = 798 Step =    8370 loss = 0.053\n",
      "Epoch = 799 Step =    8380 loss = 0.054\n",
      "Epoch = 800 Step =    8390 loss = 0.054\n",
      "Epoch = 800 Step =    8400 loss = 0.054\n",
      "Epoch = 801 Step =    8410 loss = 0.054\n",
      "Epoch = 802 Step =    8420 loss = 0.055\n",
      "Epoch = 803 Step =    8430 loss = 0.055\n",
      "Epoch = 804 Step =    8440 loss = 0.055\n",
      "Epoch = 805 Step =    8450 loss = 0.056\n",
      "Epoch = 806 Step =    8460 loss = 0.056\n",
      "Epoch = 807 Step =    8470 loss = 0.056\n",
      "Epoch = 808 Step =    8480 loss = 0.057\n",
      "Epoch = 809 Step =    8490 loss = 0.057\n",
      "Epoch = 810 Step =    8500 loss = 0.057\n",
      "Epoch = 811 Step =    8510 loss = 0.057\n",
      "Epoch = 812 Step =    8520 loss = 0.058\n",
      "Epoch = 813 Step =    8530 loss = 0.058\n",
      "Epoch = 814 Step =    8540 loss = 0.058\n",
      "Epoch = 815 Step =    8550 loss = 0.059\n",
      "Epoch = 816 Step =    8560 loss = 0.059\n",
      "Epoch = 817 Step =    8570 loss = 0.059\n",
      "Epoch = 818 Step =    8580 loss = 0.060\n",
      "Epoch = 819 Step =    8590 loss = 0.060\n",
      "Epoch = 820 Step =    8600 loss = 0.060\n",
      "Epoch = 820 Step =    8610 loss = 0.061\n",
      "Epoch = 821 Step =    8620 loss = 0.061\n",
      "Epoch = 822 Step =    8630 loss = 0.061\n",
      "Epoch = 823 Step =    8640 loss = 0.061\n",
      "Epoch = 824 Step =    8650 loss = 0.062\n",
      "Epoch = 825 Step =    8660 loss = 0.062\n",
      "Epoch = 826 Step =    8670 loss = 0.062\n",
      "Epoch = 827 Step =    8680 loss = 0.063\n",
      "Epoch = 828 Step =    8690 loss = 0.063\n",
      "Epoch = 829 Step =    8700 loss = 0.063\n",
      "Epoch = 830 Step =    8710 loss = 0.064\n",
      "Epoch = 831 Step =    8720 loss = 0.064\n",
      "Epoch = 832 Step =    8730 loss = 0.064\n",
      "Epoch = 833 Step =    8740 loss = 0.065\n",
      "Epoch = 834 Step =    8750 loss = 0.065\n",
      "Epoch = 835 Step =    8760 loss = 0.065\n",
      "Epoch = 836 Step =    8770 loss = 0.066\n",
      "Epoch = 837 Step =    8780 loss = 0.066\n",
      "Epoch = 838 Step =    8790 loss = 0.066\n",
      "Epoch = 839 Step =    8800 loss = 0.067\n",
      "Epoch = 840 Step =    8810 loss = 0.067\n",
      "Epoch = 840 Step =    8820 loss = 0.068\n",
      "Epoch = 841 Step =    8830 loss = 0.068\n",
      "Epoch = 842 Step =    8840 loss = 0.068\n",
      "Epoch = 843 Step =    8850 loss = 0.069\n",
      "Epoch = 844 Step =    8860 loss = 0.069\n",
      "Epoch = 845 Step =    8870 loss = 0.069\n",
      "Epoch = 846 Step =    8880 loss = 0.070\n",
      "Epoch = 847 Step =    8890 loss = 0.070\n",
      "Epoch = 848 Step =    8900 loss = 0.070\n",
      "Epoch = 849 Step =    8910 loss = 0.071\n",
      "Epoch = 850 Step =    8920 loss = 0.071\n",
      "Epoch = 851 Step =    8930 loss = 0.071\n",
      "Epoch = 852 Step =    8940 loss = 0.072\n",
      "Epoch = 853 Step =    8950 loss = 0.072\n",
      "Epoch = 854 Step =    8960 loss = 0.072\n",
      "Epoch = 855 Step =    8970 loss = 0.073\n",
      "Epoch = 856 Step =    8980 loss = 0.073\n",
      "Epoch = 857 Step =    8990 loss = 0.074\n",
      "Epoch = 858 Step =    9000 loss = 0.074\n",
      "Epoch = 859 Step =    9010 loss = 0.074\n",
      "Epoch = 860 Step =    9020 loss = 0.075\n",
      "Epoch = 860 Step =    9030 loss = 0.075\n",
      "Epoch = 861 Step =    9040 loss = 0.075\n",
      "Epoch = 862 Step =    9050 loss = 0.076\n",
      "Epoch = 863 Step =    9060 loss = 0.076\n",
      "Epoch = 864 Step =    9070 loss = 0.076\n",
      "Epoch = 865 Step =    9080 loss = 0.077\n",
      "Epoch = 866 Step =    9090 loss = 0.077\n",
      "Epoch = 867 Step =    9100 loss = 0.077\n",
      "Epoch = 868 Step =    9110 loss = 0.078\n",
      "Epoch = 869 Step =    9120 loss = 0.078\n",
      "Epoch = 870 Step =    9130 loss = 0.078\n",
      "Epoch = 871 Step =    9140 loss = 0.079\n",
      "Epoch = 872 Step =    9150 loss = 0.079\n",
      "Epoch = 873 Step =    9160 loss = 0.080\n",
      "Epoch = 874 Step =    9170 loss = 0.080\n",
      "Epoch = 875 Step =    9180 loss = 0.080\n",
      "Epoch = 876 Step =    9190 loss = 0.081\n",
      "Epoch = 877 Step =    9200 loss = 0.081\n",
      "Epoch = 878 Step =    9210 loss = 0.081\n",
      "Epoch = 879 Step =    9220 loss = 0.082\n",
      "Epoch = 880 Step =    9230 loss = 0.082\n",
      "Epoch = 880 Step =    9240 loss = 0.082\n",
      "Epoch = 881 Step =    9250 loss = 0.083\n",
      "Epoch = 882 Step =    9260 loss = 0.083\n",
      "Epoch = 883 Step =    9270 loss = 0.084\n",
      "Epoch = 884 Step =    9280 loss = 0.084\n",
      "Epoch = 885 Step =    9290 loss = 0.084\n",
      "Epoch = 886 Step =    9300 loss = 0.085\n",
      "Epoch = 887 Step =    9310 loss = 0.085\n",
      "Epoch = 888 Step =    9320 loss = 0.085\n",
      "Epoch = 889 Step =    9330 loss = 0.086\n",
      "Epoch = 890 Step =    9340 loss = 0.086\n",
      "Epoch = 891 Step =    9350 loss = 0.086\n",
      "Epoch = 892 Step =    9360 loss = 0.087\n",
      "Epoch = 893 Step =    9370 loss = 0.087\n",
      "Epoch = 894 Step =    9380 loss = 0.087\n",
      "Epoch = 895 Step =    9390 loss = 0.088\n",
      "Epoch = 896 Step =    9400 loss = 0.088\n",
      "Epoch = 897 Step =    9410 loss = 0.088\n",
      "Epoch = 898 Step =    9420 loss = 0.089\n",
      "Epoch = 899 Step =    9430 loss = 0.089\n",
      "Epoch = 900 Step =    9440 loss = 0.089\n",
      "Epoch = 900 Step =    9450 loss = 0.090\n",
      "Epoch = 901 Step =    9460 loss = 0.090\n",
      "Epoch = 902 Step =    9470 loss = 0.091\n",
      "Epoch = 903 Step =    9480 loss = 0.091\n",
      "Epoch = 904 Step =    9490 loss = 0.091\n",
      "Epoch = 905 Step =    9500 loss = 0.092\n",
      "Epoch = 906 Step =    9510 loss = 0.092\n",
      "Epoch = 907 Step =    9520 loss = 0.092\n",
      "Epoch = 908 Step =    9530 loss = 0.093\n",
      "Epoch = 909 Step =    9540 loss = 0.093\n",
      "Epoch = 910 Step =    9550 loss = 0.093\n",
      "Epoch = 911 Step =    9560 loss = 0.094\n",
      "Epoch = 912 Step =    9570 loss = 0.094\n",
      "Epoch = 913 Step =    9580 loss = 0.094\n",
      "Epoch = 914 Step =    9590 loss = 0.095\n",
      "Epoch = 915 Step =    9600 loss = 0.095\n",
      "Epoch = 916 Step =    9610 loss = 0.096\n",
      "Epoch = 917 Step =    9620 loss = 0.096\n",
      "Epoch = 918 Step =    9630 loss = 0.096\n",
      "Epoch = 919 Step =    9640 loss = 0.097\n",
      "Epoch = 920 Step =    9650 loss = 0.097\n",
      "Epoch = 920 Step =    9660 loss = 0.097\n",
      "Epoch = 921 Step =    9670 loss = 0.098\n",
      "Epoch = 922 Step =    9680 loss = 0.098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 923 Step =    9690 loss = 0.098\n",
      "Epoch = 924 Step =    9700 loss = 0.099\n",
      "Epoch = 925 Step =    9710 loss = 0.099\n",
      "Epoch = 926 Step =    9720 loss = 0.099\n",
      "INFO:tensorflow:model/global_step/sec: 16.618\n",
      "INFO:tensorflow:Saving checkpoint to path ./trained_models\\model.ckpt\n",
      "Epoch = 927 Step =    9730 loss = 0.100\n",
      "Epoch = 928 Step =    9740 loss = 0.100\n",
      "Epoch = 929 Step =    9750 loss = 0.101\n",
      "Epoch = 930 Step =    9760 loss = 0.101\n",
      "Epoch = 931 Step =    9770 loss = 0.101\n",
      "Epoch = 932 Step =    9780 loss = 0.102\n",
      "Epoch = 933 Step =    9790 loss = 0.102\n",
      "Epoch = 934 Step =    9800 loss = 0.102\n",
      "Epoch = 935 Step =    9810 loss = 0.103\n",
      "Epoch = 936 Step =    9820 loss = 0.103\n",
      "Epoch = 937 Step =    9830 loss = 0.103\n",
      "Epoch = 938 Step =    9840 loss = 0.104\n",
      "Epoch = 939 Step =    9850 loss = 0.104\n",
      "Epoch = 940 Step =    9860 loss = 0.104\n",
      "Epoch = 940 Step =    9870 loss = 0.105\n",
      "Epoch = 941 Step =    9880 loss = 0.105\n",
      "Epoch = 942 Step =    9890 loss = 0.105\n",
      "Epoch = 943 Step =    9900 loss = 0.106\n",
      "Epoch = 944 Step =    9910 loss = 0.106\n",
      "Epoch = 945 Step =    9920 loss = 0.106\n",
      "Epoch = 946 Step =    9930 loss = 0.107\n",
      "Epoch = 947 Step =    9940 loss = 0.107\n",
      "Epoch = 948 Step =    9950 loss = 0.107\n",
      "Epoch = 949 Step =    9960 loss = 0.108\n",
      "Epoch = 950 Step =    9970 loss = 0.108\n",
      "Epoch = 951 Step =    9980 loss = 0.109\n",
      "Epoch = 952 Step =    9990 loss = 0.109\n",
      "Epoch = 953 Step =   10000 loss = 0.109\n",
      "Epoch = 954 Step =   10010 loss = 0.110\n",
      "Epoch = 955 Step =   10020 loss = 0.110\n",
      "Epoch = 956 Step =   10030 loss = 0.110\n",
      "Epoch = 957 Step =   10040 loss = 0.111\n",
      "Epoch = 958 Step =   10050 loss = 0.111\n",
      "Epoch = 959 Step =   10060 loss = 0.111\n",
      "Epoch = 960 Step =   10070 loss = 0.112\n",
      "Epoch = 960 Step =   10080 loss = 0.112\n",
      "Epoch = 961 Step =   10090 loss = 0.112\n",
      "Epoch = 962 Step =   10100 loss = 0.113\n",
      "Epoch = 963 Step =   10110 loss = 0.113\n",
      "Epoch = 964 Step =   10120 loss = 0.113\n",
      "Epoch = 965 Step =   10130 loss = 0.114\n",
      "Epoch = 966 Step =   10140 loss = 0.114\n",
      "Epoch = 967 Step =   10150 loss = 0.114\n",
      "Epoch = 968 Step =   10160 loss = 0.115\n",
      "Epoch = 969 Step =   10170 loss = 0.115\n",
      "Epoch = 970 Step =   10180 loss = 0.115\n",
      "Epoch = 971 Step =   10190 loss = 0.116\n",
      "Epoch = 972 Step =   10200 loss = 0.116\n",
      "Epoch = 973 Step =   10210 loss = 0.116\n",
      "Epoch = 974 Step =   10220 loss = 0.117\n",
      "Epoch = 975 Step =   10230 loss = 0.117\n",
      "Epoch = 976 Step =   10240 loss = 0.117\n",
      "Epoch = 977 Step =   10250 loss = 0.118\n",
      "Epoch = 978 Step =   10260 loss = 0.118\n",
      "Epoch = 979 Step =   10270 loss = 0.118\n",
      "Epoch = 980 Step =   10280 loss = 0.119\n",
      "Epoch = 980 Step =   10290 loss = 0.119\n",
      "Epoch = 981 Step =   10300 loss = 0.119\n",
      "Epoch = 982 Step =   10310 loss = 0.120\n",
      "Epoch = 983 Step =   10320 loss = 0.120\n",
      "Epoch = 984 Step =   10330 loss = 0.120\n",
      "Epoch = 985 Step =   10340 loss = 0.120\n",
      "Epoch = 986 Step =   10350 loss = 0.121\n",
      "Epoch = 987 Step =   10360 loss = 0.121\n",
      "Epoch = 988 Step =   10370 loss = 0.121\n",
      "Epoch = 989 Step =   10380 loss = 0.122\n",
      "Epoch = 990 Step =   10390 loss = 0.122\n",
      "Epoch = 991 Step =   10400 loss = 0.122\n",
      "Epoch = 992 Step =   10410 loss = 0.123\n",
      "Epoch = 993 Step =   10420 loss = 0.123\n",
      "Epoch = 994 Step =   10430 loss = 0.123\n",
      "Epoch = 995 Step =   10440 loss = 0.124\n",
      "Epoch = 996 Step =   10450 loss = 0.124\n",
      "Epoch = 997 Step =   10460 loss = 0.124\n",
      "Epoch = 998 Step =   10470 loss = 0.125\n",
      "Epoch = 999 Step =   10480 loss = 0.125\n",
      "Epoch = 1000 Step =   10490 loss = 0.125\n",
      "Training is done.\n",
      "INFO:tensorflow:Restoring parameters from ./trained_models\\model.ckpt-9725\n",
      "INFO:tensorflow:Froze 11 variables.\n",
      "INFO:tensorflow:Converted 11 variables to const ops.\n",
      "1683 ops in the final graph.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Named Entity Recognition \n",
    "\n",
    "        Author : Sangkeun Jung (2017)\n",
    "        - using Tensorflow\n",
    "\"\"\"\n",
    "\n",
    "import sys, os, inspect\n",
    "\n",
    "# add common to path\n",
    "from pathlib import Path\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "common_path = str(Path(currentdir).parent.parent)\n",
    "sys.path.append( common_path )\n",
    "\n",
    "from common.nlp.vocab import Vocab\n",
    "from common.nlp.data_loader import N2NTextData\n",
    "from common.nlp.converter import N2NConverter\n",
    "\n",
    "from dataset import NERDataset\n",
    "from dataset import load_data\n",
    "from common.ml.hparams import HParams\n",
    "\n",
    "import numpy as np\n",
    "import copy \n",
    "import time \n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn\n",
    "from tensorflow.contrib.layers.python.layers import linear\n",
    "from tensorflow.python.ops import variable_scope\n",
    "from tensorflow.contrib.seq2seq import sequence_loss#sequence의 loss의 평균값을 구할 수있음\n",
    "\n",
    "from common.ml.tf.deploy import freeze_graph\n",
    "\n",
    "\n",
    "\n",
    "print( \"Tensorflow Version : \", tf.__version__)\n",
    "\n",
    "class NER():\n",
    "    def __init__(self, hps, mode=\"train\"):\n",
    "        self.hps = hps\n",
    "        self.x = tf.placeholder(tf.int32,   [None, hps.num_steps], name=\"pl_tokens\")\n",
    "        self.y = tf.placeholder(tf.int32,   [None, hps.num_steps], name=\"pl_target\")\n",
    "        self.w = tf.placeholder(tf.float32, [None, hps.num_steps], name=\"pl_weight\")\n",
    "        self.keep_prob = tf.placeholder(tf.float32, [], name=\"pl_keep_prob\")\n",
    "\n",
    "        ### 4 blocks ###\n",
    "        # 1) embedding\n",
    "        # 2) dropout on input embedding\n",
    "        # 3) sentence encoding using rnn\n",
    "        # 4) bidirectional rnn's output to target classes\n",
    "        # 5) loss calcaulation\n",
    "\n",
    "        def _embedding(x):\n",
    "            # character embedding \n",
    "            shape       = [hps.vocab_size, hps.emb_size]\n",
    "            initializer = tf.initializers.variance_scaling(distribution=\"uniform\", dtype=tf.float32)\n",
    "            emb_mat     = tf.get_variable(\"emb\", shape, initializer=initializer, dtype=tf.float32)\n",
    "            input_emb   = tf.nn.embedding_lookup(emb_mat, x)   # [batch_size, sent_len, emb_dim]\n",
    "\n",
    "            # split input_emb -> num_steps\n",
    "            step_inputs = tf.unstack(input_emb, axis=1)\n",
    "            return step_inputs\n",
    "\n",
    "        def _sequence_dropout(step_inputs, keep_prob):\n",
    "            # apply dropout to each input\n",
    "            # input : a list of input tensor which shape is [None, input_dim]\n",
    "            with tf.name_scope('sequence_dropout') as scope:\n",
    "                step_outputs = []\n",
    "                for t, input in enumerate(step_inputs):\n",
    "                    step_outputs.append( tf.nn.dropout(input, keep_prob) )\n",
    "            return step_outputs\n",
    "\n",
    "        def sequence_encoding_n2n(step_inputs, seq_length, cell_size):\n",
    "            # birnn based N2N encoding and output\n",
    "            f_rnn_cell = tf.contrib.rnn.GRUCell(cell_size, reuse=False)\n",
    "            b_rnn_cell = tf.contrib.rnn.GRUCell(cell_size, reuse=False)\n",
    "            _inputs    = tf.stack(step_inputs, axis=1)\n",
    "\n",
    "            # step_inputs = a list of [batch_size, emb_dim]\n",
    "            # input = [batch_size, num_step, emb_dim]\n",
    "            # np.stack( [a,b,c,] )\n",
    "            outputs, states, = tf.nn.bidirectional_dynamic_rnn( f_rnn_cell,\n",
    "                                                                b_rnn_cell,\n",
    "                                                                _inputs,\n",
    "                                                                sequence_length=tf.cast(seq_length, tf.int64),\n",
    "                                                                time_major=False,\n",
    "                                                                dtype=tf.float32,\n",
    "                                                                scope='birnn',\n",
    "                                                            )\n",
    "            output_fw, output_bw = outputs\n",
    "            states_fw, states_bw = states \n",
    "\n",
    "            output       = tf.concat([output_fw, output_bw], 2)\n",
    "            step_outputs = tf.unstack(output, axis=1)\n",
    "\n",
    "            final_state  = tf.concat([states_fw, states_bw], 1)\n",
    "            return step_outputs # a list of [batch_size, enc_dim]\n",
    "\n",
    "        def _to_class_n2n(step_inputs, num_class):\n",
    "            T = len(step_inputs)\n",
    "            step_output_logits = []\n",
    "            for t in range(T):\n",
    "                # encoder to linear(map)\n",
    "                out = step_inputs[t]\n",
    "                if t==0: out = linear(out, num_class, scope=\"Rnn2Target\")\n",
    "                else:    out = linear(out, num_class, scope=\"Rnn2Target\", reuse=True)\n",
    "                step_output_logits.append(out)\n",
    "            return step_output_logits\n",
    "\n",
    "        def _loss(step_outputs, step_refs, weights):\n",
    "            # step_outputs : a list of [batch_size, num_class] float32 - unscaled logits\n",
    "            # step_refs    : [batch_size, num_steps] int32\n",
    "            # weights      : [batch_size, num_steps] float32\n",
    "            # calculate sequence wise loss function using cross-entropy\n",
    "            _batch_output_logits = tf.stack(step_outputs, axis=1)\n",
    "            loss = sequence_loss(\n",
    "                                    logits=_batch_output_logits,        \n",
    "                                    targets=step_refs,\n",
    "                                    weights=weights\n",
    "                                )\n",
    "            return loss\n",
    "        \n",
    "        seq_length    = tf.reduce_sum(self.w, 1) # [batch_size]\n",
    "\n",
    "        step_inputs       = _embedding(self.x)\n",
    "        step_inputs       = _sequence_dropout(step_inputs, self.keep_prob)\n",
    "        step_enc_outputs  = sequence_encoding_n2n(step_inputs, seq_length, hps.enc_dim)\n",
    "        step_outputs      = _to_class_n2n(step_enc_outputs, hps.num_target_class)\n",
    "\n",
    "        self.loss = _loss(step_outputs, self.y, self.w)\n",
    "\n",
    "        # step_preds and step_out_probs\n",
    "        step_out_probs = []\n",
    "        step_out_preds = []\n",
    "        for _output in step_outputs:\n",
    "            _out_probs  = tf.nn.softmax(_output)\n",
    "            _out_pred   = tf.argmax(_out_probs, 1)\n",
    "\n",
    "            step_out_probs.append(_out_probs)\n",
    "            step_out_preds.append(_out_pred)\n",
    "\n",
    "        # stack for interface\n",
    "        self.step_out_probs = tf.stack(step_out_probs, axis=1, name=\"step_out_probs\")\n",
    "        self.step_out_preds = tf.stack(step_out_preds, axis=1, name=\"step_out_preds\")\n",
    "\n",
    "        self.global_step = tf.get_variable(\"global_step\", [], tf.int32, initializer=tf.zeros_initializer, trainable=False)\n",
    "\n",
    "        if mode == \"train\":\n",
    "            optimizer       = tf.train.AdamOptimizer(hps.learning_rate)\n",
    "            self.train_op   = optimizer.minimize(self.loss, global_step=self.global_step)\n",
    "        else:\n",
    "            self.train_op = tf.no_op()\n",
    "\n",
    "        for v in tf.trainable_variables(): print(v.name)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_default_hparams():\n",
    "        return HParams(\n",
    "            learning_rate     = 0.01,\n",
    "            keep_prob         = 0.5,\n",
    "        )\n",
    "\n",
    "\n",
    "def train(train_id_data, num_vocabs, num_taget_class):\n",
    "    #\n",
    "    # train sentiment analysis using given train_id_data\n",
    "    #\n",
    "    max_epoch = 1000\n",
    "    model_dir = \"./trained_models\"\n",
    "    hps = NER.get_default_hparams()\n",
    "    hps.update(\n",
    "                    batch_size= 50,\n",
    "                    num_steps = 85,\n",
    "                    emb_size  = 40,\n",
    "                    enc_dim   = 50,\n",
    "                    vocab_size=num_vocabs,\n",
    "                    num_target_class=num_taget_class\n",
    "               )\n",
    "\n",
    "    with tf.variable_scope(\"model\"):\n",
    "        model = NER(hps, \"train\")\n",
    "\n",
    "    sv = tf.train.Supervisor(is_chief=True,\n",
    "                             logdir=model_dir,\n",
    "                             summary_op=None,  \n",
    "                             global_step=model.global_step)\n",
    "\n",
    "    # tf assign compatible operators for gpu and cpu \n",
    "    tf_config = tf.ConfigProto(allow_soft_placement=True)\n",
    "\n",
    "    with sv.managed_session(config=tf_config) as sess:\n",
    "        local_step       = 0\n",
    "        prev_global_step = sess.run(model.global_step)\n",
    "\n",
    "        train_data_set = NERDataset(train_id_data, hps.batch_size, hps.num_steps)\n",
    "        losses = []\n",
    "        while not sv.should_stop():\n",
    "            fetches = [model.global_step, model.loss, model.train_op]\n",
    "            a_batch_data = next( train_data_set.iterator )\n",
    "            y, x, w = a_batch_data\n",
    "            fetched = sess.run(fetches, {\n",
    "                                            model.x: x, \n",
    "                                            model.y: y, \n",
    "                                            model.w: w,\n",
    "\n",
    "                                            model.keep_prob: hps.keep_prob,\n",
    "                                        }\n",
    "                              )\n",
    "\n",
    "            local_step += 1\n",
    "\n",
    "            _global_step = fetched[0]\n",
    "            _loss        = fetched[1]\n",
    "            losses.append( _loss )\n",
    "            if local_step < 10 or local_step % 10 == 0:\n",
    "                epoch = train_data_set.get_epoch_num()\n",
    "                print(\"Epoch = {:3d} Step = {:7d} loss = {:5.3f}\".format(epoch, _global_step, np.mean(losses)) )\n",
    "                _loss = []                \n",
    "                if epoch >= max_epoch : break \n",
    "\n",
    "        print(\"Training is done.\")\n",
    "    sv.stop()\n",
    "\n",
    "    # model.out_pred, model.out_probs\n",
    "    freeze_graph(model_dir, \"model/step_out_preds,model/step_out_probs\", \"frozen_graph.tf.pb\") ## freeze graph with params to probobuf format\n",
    "    \n",
    "from tensorflow.core.framework import graph_pb2\n",
    "def predict(token_vocab, target_vocab, sent):\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # force to use cpu only (prediction)\n",
    "    model_dir = \"./trained_models\"\n",
    "\n",
    "    # prepare sentence converting\n",
    "    # to make raw sentence to id data easily\n",
    "    pred_data     = N2NTextData(sent, mode='sentence')\n",
    "    pred_id_data  = N2NConverter.convert(pred_data, target_vocab, token_vocab)\n",
    "    pred_data_set = NERDataset(pred_id_data, 1, 85)\n",
    "    #\n",
    "    a_batch_data = next(pred_data_set.predict_iterator) # a result\n",
    "    b_nes_id, b_token_ids, b_weight = a_batch_data\n",
    "\n",
    "    # Restore graph\n",
    "    # note that frozen_graph.tf.pb contains graph definition with parameter values in binary format\n",
    "    _graph_fn =  os.path.join(model_dir, 'frozen_graph.tf.pb')\n",
    "    with tf.gfile.GFile(_graph_fn, \"rb\") as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "    \n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.import_graph_def(graph_def)\n",
    "\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        # to check load graph\n",
    "        #for n in tf.get_default_graph().as_graph_def().node: print(n.name)\n",
    "\n",
    "        # make interface for input\n",
    "        pl_token     = graph.get_tensor_by_name('import/model/pl_tokens:0')\n",
    "        pl_weight    = graph.get_tensor_by_name('import/model/pl_weight:0')\n",
    "        pl_keep_prob = graph.get_tensor_by_name('import/model/pl_keep_prob:0')\n",
    "\n",
    "        # make interface for output\n",
    "        step_out_preds = graph.get_tensor_by_name('import/model/step_out_preds:0')\n",
    "        step_out_probs = graph.get_tensor_by_name('import/model/step_out_probs:0')\n",
    "        \n",
    "\n",
    "        # predict sentence \n",
    "        b_best_step_pred_indexs, b_step_pred_probs = sess.run([step_out_preds, step_out_probs], \n",
    "                                                              feed_dict={\n",
    "                                                                            pl_token  : b_token_ids,\n",
    "                                                                            pl_weight : b_weight,\n",
    "                                                                            pl_keep_prob : 1.0,\n",
    "                                                                        }\n",
    "                                                             )\n",
    "        best_step_pred_indexs = b_best_step_pred_indexs[0]\n",
    "        step_pred_probs = b_step_pred_probs[0]\n",
    "\n",
    "        step_best_targets = []\n",
    "        step_best_target_probs = []\n",
    "        for time_step, best_pred_index in enumerate(best_step_pred_indexs):\n",
    "            _target_class = target_vocab.get_symbol(best_pred_index)\n",
    "            step_best_targets.append( _target_class )\n",
    "            _prob = step_pred_probs[time_step][best_pred_index]\n",
    "            step_best_target_probs.append( _prob ) \n",
    "\n",
    "        for idx, char in enumerate(list(sent)):\n",
    "            print('{}\\t{}\\t{}'.format(char, step_best_targets[idx], step_best_target_probs[idx]) ) \n",
    "        #return list(sent)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_id_data, token_vocab, target_vocab = load_data()\n",
    "    num_vocabs       = token_vocab.get_num_tokens()\n",
    "    num_target_class = target_vocab.get_num_targets()\n",
    "\n",
    "    train_data_set = NERDataset(train_id_data, 5, 85)\n",
    "    train(train_id_data, num_vocabs, num_target_class)\n",
    "    \n",
    "    #predict(token_vocab, target_vocab, '아프가니스탄의 장래를 더욱 불투명하게 하는 것은 강경파 헤즈비 이슬라미와 우즈베크 민병대의 대립이다.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가\tB-AC\t0.8060729503631592\n",
      "족\tI-AC\t0.9727397561073303\n",
      "이\tO\t0.9209614992141724\n",
      "랑\tO\t0.9622595310211182\n",
      " \tO\t0.969157874584198\n",
      "겨\tB-DT\t0.8904657959938049\n",
      "울\tI-DT\t0.9489538669586182\n",
      "에\tO\t0.8773403167724609\n",
      " \tO\t0.9541662335395813\n",
      "3\tB-PR\t0.7125526070594788\n",
      "박\tI-PR\t0.9858400821685791\n",
      "4\tI-PR\t0.9686891436576843\n",
      "일\tI-PR\t0.9736447930335999\n",
      " \tO\t0.958454430103302\n",
      "여\tO\t0.9833940863609314\n",
      "행\tO\t0.9892008304595947\n"
     ]
    }
   ],
   "source": [
    "predict(token_vocab, target_vocab, '가족이랑 겨울에 3박4일 여행')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "여\tO\t0.8624535799026489\n",
      "자\tI-AC\t0.9172770977020264\n",
      "친\tI-AC\t0.6635997891426086\n",
      "구\tI-AC\t0.9188944697380066\n",
      "랑\tO\t0.9151795506477356\n",
      " \tO\t0.9791615009307861\n",
      "휴\tB-PU\t0.8528434038162231\n",
      "양\tI-PU\t0.8842093348503113\n",
      "지\tO\t0.5214310884475708\n",
      " \tO\t0.9751706123352051\n",
      "가\tO\t0.9164026975631714\n",
      "서\tO\t0.9763439893722534\n",
      " \tO\t0.9904068112373352\n",
      "힐\tB-PU\t0.9648532867431641\n",
      "링\tI-PU\t0.7901679873466492\n",
      "하\tO\t0.9669514298439026\n",
      "고\tO\t0.8655789494514465\n",
      " \tO\t0.9966482520103455\n",
      "싶\tO\t0.8834100365638733\n",
      "어\tO\t0.9749166369438171\n",
      "요\tO\t0.9904847145080566\n",
      "!\tO\t0.9736834168434143\n",
      " \tO\t0.9930007457733154\n",
      "9\tB-DT\t0.8449887633323669\n",
      "월\tI-DT\t0.9530041813850403\n",
      "1\tO\t0.44898349046707153\n",
      "5\tI-DT\t0.37916913628578186\n",
      "일\tI-DT\t0.47819021344184875\n",
      "부\tO\t0.5371277928352356\n",
      "터\tO\t0.9179531931877136\n",
      " \tO\t0.903346836566925\n",
      "1\tO\t0.4468589723110199\n",
      "8\tO\t0.681887686252594\n",
      "일\tI-PR\t0.9567570686340332\n",
      "까\tO\t0.8890085220336914\n",
      "지\tO\t0.9437020421028137\n"
     ]
    }
   ],
   "source": [
    "predict(token_vocab, target_vocab, '여자친구랑 휴양지 가서 힐링하고 싶어요! 9월15일부터 18일까지')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
