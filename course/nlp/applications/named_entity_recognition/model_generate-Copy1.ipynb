{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tjdwn\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version :  1.10.0\n",
      "<common.nlp.data_loader.N2NTextData object at 0x000001EEC9F19F28>\n",
      "WARNING:tensorflow:From C:\\Users\\tjdwn\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:430: calling reverse_sequence (from tensorflow.python.ops.array_ops) with seq_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "seq_dim is deprecated, use seq_axis instead\n",
      "WARNING:tensorflow:From C:\\Users\\tjdwn\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:454: calling reverse_sequence (from tensorflow.python.ops.array_ops) with batch_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "batch_dim is deprecated, use batch_axis instead\n",
      "model/emb:0\n",
      "model/birnn/fw/gru_cell/gates/kernel:0\n",
      "model/birnn/fw/gru_cell/gates/bias:0\n",
      "model/birnn/fw/gru_cell/candidate/kernel:0\n",
      "model/birnn/fw/gru_cell/candidate/bias:0\n",
      "model/birnn/bw/gru_cell/gates/kernel:0\n",
      "model/birnn/bw/gru_cell/gates/bias:0\n",
      "model/birnn/bw/gru_cell/candidate/kernel:0\n",
      "model/birnn/bw/gru_cell/candidate/bias:0\n",
      "model/Rnn2Target/weights:0\n",
      "model/Rnn2Target/biases:0\n",
      "WARNING:tensorflow:From <ipython-input-1-d174b5fa6746>:187: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.MonitoredTrainingSession\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Starting standard services.\n",
      "INFO:tensorflow:Saving checkpoint to path ./trained_models\\model.ckpt\n",
      "INFO:tensorflow:Starting queue runners.\n",
      "INFO:tensorflow:model/global_step/sec: inf\n",
      "Epoch =   1 Step =       1 loss = 2.561\n",
      "Epoch =   1 Step =       2 loss = 2.482\n",
      "Epoch =   1 Step =       3 loss = 2.316\n",
      "Epoch =   1 Step =       4 loss = 2.015\n",
      "Epoch =   1 Step =       5 loss = 1.905\n",
      "Epoch =   1 Step =       6 loss = 1.811\n",
      "Epoch =   1 Step =       7 loss = 1.731\n",
      "Epoch =   1 Step =       8 loss = 1.637\n",
      "Epoch =   2 Step =       9 loss = 1.561\n",
      "Epoch =   2 Step =      10 loss = 1.504\n",
      "Epoch =   3 Step =      20 loss = 1.218\n",
      "Epoch =   4 Step =      30 loss = 1.101\n",
      "Epoch =   5 Step =      40 loss = 1.022\n",
      "Epoch =   6 Step =      50 loss = 0.944\n",
      "Epoch =   7 Step =      60 loss = 0.868\n",
      "Epoch =   9 Step =      70 loss = 0.798\n",
      "Epoch =  10 Step =      80 loss = 0.737\n",
      "Epoch =  11 Step =      90 loss = 0.683\n",
      "Epoch =  12 Step =     100 loss = 0.637\n",
      "Epoch =  13 Step =     110 loss = 0.597\n",
      "Epoch =  14 Step =     120 loss = 0.563\n",
      "Epoch =  16 Step =     130 loss = 0.533\n",
      "Epoch =  17 Step =     140 loss = 0.507\n",
      "Epoch =  18 Step =     150 loss = 0.483\n",
      "Epoch =  19 Step =     160 loss = 0.462\n",
      "Epoch =  20 Step =     170 loss = 0.443\n",
      "Epoch =  21 Step =     180 loss = 0.426\n",
      "Epoch =  23 Step =     190 loss = 0.410\n",
      "Epoch =  24 Step =     200 loss = 0.395\n",
      "Epoch =  25 Step =     210 loss = 0.382\n",
      "Epoch =  26 Step =     220 loss = 0.370\n",
      "Epoch =  27 Step =     230 loss = 0.358\n",
      "Epoch =  28 Step =     240 loss = 0.348\n",
      "Epoch =  30 Step =     250 loss = 0.338\n",
      "Epoch =  31 Step =     260 loss = 0.328\n",
      "Epoch =  32 Step =     270 loss = 0.319\n",
      "Epoch =  33 Step =     280 loss = 0.311\n",
      "Epoch =  34 Step =     290 loss = 0.303\n",
      "Epoch =  35 Step =     300 loss = 0.296\n",
      "Epoch =  37 Step =     310 loss = 0.289\n",
      "Epoch =  38 Step =     320 loss = 0.282\n",
      "Epoch =  39 Step =     330 loss = 0.276\n",
      "Epoch =  40 Step =     340 loss = 0.270\n",
      "Epoch =  41 Step =     350 loss = 0.264\n",
      "Epoch =  42 Step =     360 loss = 0.259\n",
      "Epoch =  44 Step =     370 loss = 0.254\n",
      "Epoch =  45 Step =     380 loss = 0.249\n",
      "Epoch =  46 Step =     390 loss = 0.244\n",
      "Epoch =  47 Step =     400 loss = 0.239\n",
      "Epoch =  48 Step =     410 loss = 0.235\n",
      "Epoch =  49 Step =     420 loss = 0.231\n",
      "Epoch =  50 Step =     430 loss = 0.227\n",
      "Epoch =  52 Step =     440 loss = 0.223\n",
      "Epoch =  53 Step =     450 loss = 0.219\n",
      "Epoch =  54 Step =     460 loss = 0.215\n",
      "Epoch =  55 Step =     470 loss = 0.212\n",
      "Epoch =  56 Step =     480 loss = 0.208\n",
      "INFO:tensorflow:model/global_step/sec: 4.03331\n",
      "Epoch =  57 Step =     490 loss = 0.205\n",
      "Epoch =  59 Step =     500 loss = 0.202\n",
      "Epoch =  60 Step =     510 loss = 0.199\n",
      "Epoch =  61 Step =     520 loss = 0.196\n",
      "Epoch =  62 Step =     530 loss = 0.193\n",
      "Epoch =  63 Step =     540 loss = 0.190\n",
      "Epoch =  64 Step =     550 loss = 0.187\n",
      "Epoch =  66 Step =     560 loss = 0.184\n",
      "Epoch =  67 Step =     570 loss = 0.182\n",
      "Epoch =  68 Step =     580 loss = 0.179\n",
      "Epoch =  69 Step =     590 loss = 0.177\n",
      "Epoch =  70 Step =     600 loss = 0.175\n",
      "Epoch =  71 Step =     610 loss = 0.172\n",
      "Epoch =  73 Step =     620 loss = 0.170\n",
      "Epoch =  74 Step =     630 loss = 0.168\n",
      "Epoch =  75 Step =     640 loss = 0.166\n",
      "Epoch =  76 Step =     650 loss = 0.164\n",
      "Epoch =  77 Step =     660 loss = 0.162\n",
      "Epoch =  78 Step =     670 loss = 0.160\n",
      "Epoch =  80 Step =     680 loss = 0.158\n",
      "Epoch =  81 Step =     690 loss = 0.156\n",
      "Epoch =  82 Step =     700 loss = 0.154\n",
      "Epoch =  83 Step =     710 loss = 0.152\n",
      "Epoch =  84 Step =     720 loss = 0.150\n",
      "Epoch =  85 Step =     730 loss = 0.149\n",
      "Epoch =  87 Step =     740 loss = 0.147\n",
      "Epoch =  88 Step =     750 loss = 0.145\n",
      "Epoch =  89 Step =     760 loss = 0.144\n",
      "Epoch =  90 Step =     770 loss = 0.142\n",
      "Epoch =  91 Step =     780 loss = 0.141\n",
      "Epoch =  92 Step =     790 loss = 0.139\n",
      "Epoch =  94 Step =     800 loss = 0.138\n",
      "Epoch =  95 Step =     810 loss = 0.136\n",
      "Epoch =  96 Step =     820 loss = 0.135\n",
      "Epoch =  97 Step =     830 loss = 0.133\n",
      "Epoch =  98 Step =     840 loss = 0.132\n",
      "Epoch =  99 Step =     850 loss = 0.131\n",
      "Epoch = 100 Step =     860 loss = 0.129\n",
      "Epoch = 102 Step =     870 loss = 0.128\n",
      "Epoch = 103 Step =     880 loss = 0.127\n",
      "Epoch = 104 Step =     890 loss = 0.126\n",
      "Epoch = 105 Step =     900 loss = 0.124\n",
      "Epoch = 106 Step =     910 loss = 0.123\n",
      "Epoch = 107 Step =     920 loss = 0.122\n",
      "Epoch = 109 Step =     930 loss = 0.121\n",
      "Epoch = 110 Step =     940 loss = 0.120\n",
      "Epoch = 111 Step =     950 loss = 0.119\n",
      "Epoch = 112 Step =     960 loss = 0.118\n",
      "Epoch = 113 Step =     970 loss = 0.117\n",
      "Epoch = 114 Step =     980 loss = 0.115\n",
      "Epoch = 116 Step =     990 loss = 0.114\n",
      "INFO:tensorflow:model/global_step/sec: 4.28326\n",
      "Epoch = 117 Step =    1000 loss = 0.113\n",
      "Epoch = 118 Step =    1010 loss = 0.112\n",
      "Epoch = 119 Step =    1020 loss = 0.112\n",
      "Epoch = 120 Step =    1030 loss = 0.111\n",
      "Epoch = 121 Step =    1040 loss = 0.110\n",
      "Epoch = 123 Step =    1050 loss = 0.109\n",
      "Epoch = 124 Step =    1060 loss = 0.108\n",
      "Epoch = 125 Step =    1070 loss = 0.107\n",
      "Epoch = 126 Step =    1080 loss = 0.106\n",
      "Epoch = 127 Step =    1090 loss = 0.105\n",
      "Epoch = 128 Step =    1100 loss = 0.104\n",
      "Epoch = 130 Step =    1110 loss = 0.104\n",
      "Epoch = 131 Step =    1120 loss = 0.103\n",
      "Epoch = 132 Step =    1130 loss = 0.102\n",
      "Epoch = 133 Step =    1140 loss = 0.101\n",
      "Epoch = 134 Step =    1150 loss = 0.100\n",
      "Epoch = 135 Step =    1160 loss = 0.100\n",
      "Epoch = 137 Step =    1170 loss = 0.099\n",
      "Epoch = 138 Step =    1180 loss = 0.098\n",
      "Epoch = 139 Step =    1190 loss = 0.097\n",
      "Epoch = 140 Step =    1200 loss = 0.097\n",
      "Epoch = 141 Step =    1210 loss = 0.096\n",
      "Epoch = 142 Step =    1220 loss = 0.095\n",
      "Epoch = 144 Step =    1230 loss = 0.095\n",
      "Epoch = 145 Step =    1240 loss = 0.094\n",
      "Epoch = 146 Step =    1250 loss = 0.093\n",
      "Epoch = 147 Step =    1260 loss = 0.092\n",
      "Epoch = 148 Step =    1270 loss = 0.092\n",
      "Epoch = 149 Step =    1280 loss = 0.091\n",
      "Epoch = 150 Step =    1290 loss = 0.091\n",
      "Epoch = 152 Step =    1300 loss = 0.090\n",
      "Epoch = 153 Step =    1310 loss = 0.089\n",
      "Epoch = 154 Step =    1320 loss = 0.089\n",
      "Epoch = 155 Step =    1330 loss = 0.088\n",
      "Epoch = 156 Step =    1340 loss = 0.088\n",
      "Epoch = 157 Step =    1350 loss = 0.087\n",
      "Epoch = 159 Step =    1360 loss = 0.086\n",
      "INFO:tensorflow:model/global_step/sec: 3.03292\n",
      "Epoch = 160 Step =    1370 loss = 0.086\n",
      "Epoch = 161 Step =    1380 loss = 0.085\n",
      "Epoch = 162 Step =    1390 loss = 0.085\n",
      "Epoch = 163 Step =    1400 loss = 0.084\n",
      "Epoch = 164 Step =    1410 loss = 0.084\n",
      "Epoch = 166 Step =    1420 loss = 0.083\n",
      "Epoch = 167 Step =    1430 loss = 0.083\n",
      "Epoch = 168 Step =    1440 loss = 0.082\n",
      "Epoch = 169 Step =    1450 loss = 0.082\n",
      "Epoch = 170 Step =    1460 loss = 0.081\n",
      "Epoch = 171 Step =    1470 loss = 0.081\n",
      "Epoch = 173 Step =    1480 loss = 0.080\n",
      "Epoch = 174 Step =    1490 loss = 0.080\n",
      "Epoch = 175 Step =    1500 loss = 0.079\n",
      "Epoch = 176 Step =    1510 loss = 0.079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 177 Step =    1520 loss = 0.078\n",
      "Epoch = 178 Step =    1530 loss = 0.078\n",
      "Epoch = 180 Step =    1540 loss = 0.077\n",
      "Epoch = 181 Step =    1550 loss = 0.077\n",
      "Epoch = 182 Step =    1560 loss = 0.076\n",
      "Epoch = 183 Step =    1570 loss = 0.076\n",
      "Epoch = 184 Step =    1580 loss = 0.076\n",
      "Epoch = 185 Step =    1590 loss = 0.075\n",
      "Epoch = 187 Step =    1600 loss = 0.075\n",
      "Epoch = 188 Step =    1610 loss = 0.074\n",
      "Epoch = 189 Step =    1620 loss = 0.074\n",
      "Epoch = 190 Step =    1630 loss = 0.074\n",
      "Epoch = 191 Step =    1640 loss = 0.073\n",
      "Epoch = 192 Step =    1650 loss = 0.073\n",
      "Epoch = 194 Step =    1660 loss = 0.072\n",
      "Epoch = 195 Step =    1670 loss = 0.072\n",
      "Epoch = 196 Step =    1680 loss = 0.072\n",
      "Epoch = 197 Step =    1690 loss = 0.071\n",
      "Epoch = 198 Step =    1700 loss = 0.071\n",
      "Epoch = 199 Step =    1710 loss = 0.070\n",
      "Epoch = 200 Step =    1720 loss = 0.070\n",
      "Epoch = 202 Step =    1730 loss = 0.070\n",
      "INFO:tensorflow:model/global_step/sec: 3.06692\n",
      "Epoch = 203 Step =    1740 loss = 0.069\n",
      "Epoch = 204 Step =    1750 loss = 0.069\n",
      "Epoch = 205 Step =    1760 loss = 0.069\n",
      "Epoch = 206 Step =    1770 loss = 0.068\n",
      "Epoch = 207 Step =    1780 loss = 0.068\n",
      "Epoch = 209 Step =    1790 loss = 0.068\n",
      "Epoch = 210 Step =    1800 loss = 0.067\n",
      "Epoch = 211 Step =    1810 loss = 0.067\n",
      "Epoch = 212 Step =    1820 loss = 0.067\n",
      "Epoch = 213 Step =    1830 loss = 0.066\n",
      "Epoch = 214 Step =    1840 loss = 0.066\n",
      "Epoch = 216 Step =    1850 loss = 0.066\n",
      "Epoch = 217 Step =    1860 loss = 0.065\n",
      "Epoch = 218 Step =    1870 loss = 0.065\n",
      "Epoch = 219 Step =    1880 loss = 0.065\n",
      "Epoch = 220 Step =    1890 loss = 0.064\n",
      "Epoch = 221 Step =    1900 loss = 0.064\n",
      "Epoch = 223 Step =    1910 loss = 0.064\n",
      "Epoch = 224 Step =    1920 loss = 0.063\n",
      "Epoch = 225 Step =    1930 loss = 0.063\n",
      "Epoch = 226 Step =    1940 loss = 0.063\n",
      "Epoch = 227 Step =    1950 loss = 0.063\n",
      "Epoch = 228 Step =    1960 loss = 0.062\n",
      "Epoch = 230 Step =    1970 loss = 0.062\n",
      "Epoch = 231 Step =    1980 loss = 0.062\n",
      "Epoch = 232 Step =    1990 loss = 0.061\n",
      "Epoch = 233 Step =    2000 loss = 0.061\n",
      "Epoch = 234 Step =    2010 loss = 0.061\n",
      "Epoch = 235 Step =    2020 loss = 0.061\n",
      "Epoch = 237 Step =    2030 loss = 0.060\n",
      "Epoch = 238 Step =    2040 loss = 0.060\n",
      "Epoch = 239 Step =    2050 loss = 0.060\n",
      "Epoch = 240 Step =    2060 loss = 0.060\n",
      "Epoch = 241 Step =    2070 loss = 0.059\n",
      "Epoch = 242 Step =    2080 loss = 0.059\n",
      "INFO:tensorflow:Saving checkpoint to path ./trained_models\\model.ckpt\n",
      "INFO:tensorflow:model/global_step/sec: 2.96639\n",
      "Epoch = 244 Step =    2090 loss = 0.059\n",
      "Epoch = 245 Step =    2100 loss = 0.059\n",
      "Epoch = 246 Step =    2110 loss = 0.058\n",
      "Epoch = 247 Step =    2120 loss = 0.058\n",
      "Epoch = 248 Step =    2130 loss = 0.058\n",
      "Epoch = 249 Step =    2140 loss = 0.058\n",
      "Epoch = 250 Step =    2150 loss = 0.057\n",
      "Epoch = 252 Step =    2160 loss = 0.057\n",
      "Epoch = 253 Step =    2170 loss = 0.057\n",
      "Epoch = 254 Step =    2180 loss = 0.057\n",
      "Epoch = 255 Step =    2190 loss = 0.056\n",
      "Epoch = 256 Step =    2200 loss = 0.056\n",
      "Epoch = 257 Step =    2210 loss = 0.056\n",
      "Epoch = 259 Step =    2220 loss = 0.056\n",
      "Epoch = 260 Step =    2230 loss = 0.056\n",
      "Epoch = 261 Step =    2240 loss = 0.055\n",
      "Epoch = 262 Step =    2250 loss = 0.055\n",
      "Epoch = 263 Step =    2260 loss = 0.055\n",
      "Epoch = 264 Step =    2270 loss = 0.055\n",
      "Epoch = 266 Step =    2280 loss = 0.054\n",
      "Epoch = 267 Step =    2290 loss = 0.054\n",
      "Epoch = 268 Step =    2300 loss = 0.054\n",
      "Epoch = 269 Step =    2310 loss = 0.054\n",
      "Epoch = 270 Step =    2320 loss = 0.054\n",
      "Epoch = 271 Step =    2330 loss = 0.053\n",
      "Epoch = 273 Step =    2340 loss = 0.053\n",
      "Epoch = 274 Step =    2350 loss = 0.053\n",
      "Epoch = 275 Step =    2360 loss = 0.053\n",
      "Epoch = 276 Step =    2370 loss = 0.053\n",
      "Epoch = 277 Step =    2380 loss = 0.052\n",
      "Epoch = 278 Step =    2390 loss = 0.052\n",
      "Epoch = 280 Step =    2400 loss = 0.052\n",
      "Epoch = 281 Step =    2410 loss = 0.052\n",
      "Epoch = 282 Step =    2420 loss = 0.052\n",
      "Epoch = 283 Step =    2430 loss = 0.051\n",
      "Epoch = 284 Step =    2440 loss = 0.051\n",
      "Epoch = 285 Step =    2450 loss = 0.051\n",
      "Epoch = 287 Step =    2460 loss = 0.051\n",
      "Epoch = 288 Step =    2470 loss = 0.051\n",
      "Epoch = 289 Step =    2480 loss = 0.051\n",
      "Epoch = 290 Step =    2490 loss = 0.050\n",
      "Epoch = 291 Step =    2500 loss = 0.050\n",
      "INFO:tensorflow:model/global_step/sec: 3.4502\n",
      "Epoch = 292 Step =    2510 loss = 0.050\n",
      "Epoch = 294 Step =    2520 loss = 0.050\n",
      "Epoch = 295 Step =    2530 loss = 0.050\n",
      "Epoch = 296 Step =    2540 loss = 0.050\n",
      "Epoch = 297 Step =    2550 loss = 0.049\n",
      "Epoch = 298 Step =    2560 loss = 0.049\n",
      "Epoch = 299 Step =    2570 loss = 0.049\n",
      "Epoch = 300 Step =    2580 loss = 0.049\n",
      "Epoch = 302 Step =    2590 loss = 0.049\n",
      "Epoch = 303 Step =    2600 loss = 0.049\n",
      "Epoch = 304 Step =    2610 loss = 0.048\n",
      "Epoch = 305 Step =    2620 loss = 0.048\n",
      "Epoch = 306 Step =    2630 loss = 0.048\n",
      "Epoch = 307 Step =    2640 loss = 0.048\n",
      "Epoch = 309 Step =    2650 loss = 0.048\n",
      "Epoch = 310 Step =    2660 loss = 0.048\n",
      "Epoch = 311 Step =    2670 loss = 0.047\n",
      "Epoch = 312 Step =    2680 loss = 0.047\n",
      "Epoch = 313 Step =    2690 loss = 0.047\n",
      "Epoch = 314 Step =    2700 loss = 0.047\n",
      "Epoch = 316 Step =    2710 loss = 0.047\n",
      "Epoch = 317 Step =    2720 loss = 0.047\n",
      "Epoch = 318 Step =    2730 loss = 0.046\n",
      "Epoch = 319 Step =    2740 loss = 0.046\n",
      "Epoch = 320 Step =    2750 loss = 0.046\n",
      "Epoch = 321 Step =    2760 loss = 0.046\n",
      "Epoch = 323 Step =    2770 loss = 0.046\n",
      "Epoch = 324 Step =    2780 loss = 0.046\n",
      "Epoch = 325 Step =    2790 loss = 0.046\n",
      "Epoch = 326 Step =    2800 loss = 0.045\n",
      "Epoch = 327 Step =    2810 loss = 0.045\n",
      "Epoch = 328 Step =    2820 loss = 0.045\n",
      "Epoch = 330 Step =    2830 loss = 0.045\n",
      "Epoch = 331 Step =    2840 loss = 0.045\n",
      "Epoch = 332 Step =    2850 loss = 0.045\n",
      "Epoch = 333 Step =    2860 loss = 0.045\n",
      "Epoch = 334 Step =    2870 loss = 0.044\n",
      "Epoch = 335 Step =    2880 loss = 0.044\n",
      "Epoch = 337 Step =    2890 loss = 0.044\n",
      "Epoch = 338 Step =    2900 loss = 0.044\n",
      "Epoch = 339 Step =    2910 loss = 0.044\n",
      "Epoch = 340 Step =    2920 loss = 0.044\n",
      "Epoch = 341 Step =    2930 loss = 0.044\n",
      "Epoch = 342 Step =    2940 loss = 0.044\n",
      "Epoch = 344 Step =    2950 loss = 0.043\n",
      "Epoch = 345 Step =    2960 loss = 0.043\n",
      "Epoch = 346 Step =    2970 loss = 0.043\n",
      "Epoch = 347 Step =    2980 loss = 0.043\n",
      "INFO:tensorflow:model/global_step/sec: 4.03367\n",
      "Epoch = 348 Step =    2990 loss = 0.043\n",
      "Epoch = 349 Step =    3000 loss = 0.043\n",
      "Epoch = 350 Step =    3010 loss = 0.043\n",
      "Epoch = 352 Step =    3020 loss = 0.043\n",
      "Epoch = 353 Step =    3030 loss = 0.042\n",
      "Epoch = 354 Step =    3040 loss = 0.042\n",
      "Epoch = 355 Step =    3050 loss = 0.042\n",
      "Epoch = 356 Step =    3060 loss = 0.042\n",
      "Epoch = 357 Step =    3070 loss = 0.042\n",
      "Epoch = 359 Step =    3080 loss = 0.042\n",
      "Epoch = 360 Step =    3090 loss = 0.042\n",
      "Epoch = 361 Step =    3100 loss = 0.042\n",
      "Epoch = 362 Step =    3110 loss = 0.041\n",
      "Epoch = 363 Step =    3120 loss = 0.041\n",
      "Epoch = 364 Step =    3130 loss = 0.041\n",
      "Epoch = 366 Step =    3140 loss = 0.041\n",
      "Epoch = 367 Step =    3150 loss = 0.041\n",
      "Epoch = 368 Step =    3160 loss = 0.041\n",
      "Epoch = 369 Step =    3170 loss = 0.041\n",
      "Epoch = 370 Step =    3180 loss = 0.041\n",
      "Epoch = 371 Step =    3190 loss = 0.040\n",
      "Epoch = 373 Step =    3200 loss = 0.040\n",
      "Epoch = 374 Step =    3210 loss = 0.040\n",
      "Epoch = 375 Step =    3220 loss = 0.040\n",
      "Epoch = 376 Step =    3230 loss = 0.040\n",
      "Epoch = 377 Step =    3240 loss = 0.040\n",
      "Epoch = 378 Step =    3250 loss = 0.040\n",
      "Epoch = 380 Step =    3260 loss = 0.040\n",
      "Epoch = 381 Step =    3270 loss = 0.040\n",
      "Epoch = 382 Step =    3280 loss = 0.040\n",
      "Epoch = 383 Step =    3290 loss = 0.039\n",
      "Epoch = 384 Step =    3300 loss = 0.039\n",
      "Epoch = 385 Step =    3310 loss = 0.039\n",
      "Epoch = 387 Step =    3320 loss = 0.039\n",
      "Epoch = 388 Step =    3330 loss = 0.039\n",
      "Epoch = 389 Step =    3340 loss = 0.039\n",
      "Epoch = 390 Step =    3350 loss = 0.039\n",
      "Epoch = 391 Step =    3360 loss = 0.039\n",
      "Epoch = 392 Step =    3370 loss = 0.039\n",
      "Epoch = 394 Step =    3380 loss = 0.039\n",
      "Epoch = 395 Step =    3390 loss = 0.038\n",
      "Epoch = 396 Step =    3400 loss = 0.038\n",
      "Epoch = 397 Step =    3410 loss = 0.038\n",
      "Epoch = 398 Step =    3420 loss = 0.038\n",
      "Epoch = 399 Step =    3430 loss = 0.038\n",
      "Epoch = 400 Step =    3440 loss = 0.038\n",
      "Epoch = 402 Step =    3450 loss = 0.038\n",
      "Epoch = 403 Step =    3460 loss = 0.038\n",
      "Epoch = 404 Step =    3470 loss = 0.038\n",
      "Epoch = 405 Step =    3480 loss = 0.038\n",
      "Epoch = 406 Step =    3490 loss = 0.038\n",
      "INFO:tensorflow:model/global_step/sec: 4.23327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 407 Step =    3500 loss = 0.037\n",
      "Epoch = 409 Step =    3510 loss = 0.037\n",
      "Epoch = 410 Step =    3520 loss = 0.037\n",
      "Epoch = 411 Step =    3530 loss = 0.037\n",
      "Epoch = 412 Step =    3540 loss = 0.037\n",
      "Epoch = 413 Step =    3550 loss = 0.037\n",
      "Epoch = 414 Step =    3560 loss = 0.037\n",
      "Epoch = 416 Step =    3570 loss = 0.037\n",
      "Epoch = 417 Step =    3580 loss = 0.037\n",
      "Epoch = 418 Step =    3590 loss = 0.037\n",
      "Epoch = 419 Step =    3600 loss = 0.037\n",
      "Epoch = 420 Step =    3610 loss = 0.036\n",
      "Epoch = 421 Step =    3620 loss = 0.036\n",
      "Epoch = 423 Step =    3630 loss = 0.036\n",
      "Epoch = 424 Step =    3640 loss = 0.036\n",
      "Epoch = 425 Step =    3650 loss = 0.036\n",
      "Epoch = 426 Step =    3660 loss = 0.036\n",
      "Epoch = 427 Step =    3670 loss = 0.036\n",
      "Epoch = 428 Step =    3680 loss = 0.036\n",
      "Epoch = 430 Step =    3690 loss = 0.036\n",
      "Epoch = 431 Step =    3700 loss = 0.036\n",
      "Epoch = 432 Step =    3710 loss = 0.036\n",
      "Epoch = 433 Step =    3720 loss = 0.036\n",
      "Epoch = 434 Step =    3730 loss = 0.035\n",
      "Epoch = 435 Step =    3740 loss = 0.035\n",
      "Epoch = 437 Step =    3750 loss = 0.035\n",
      "Epoch = 438 Step =    3760 loss = 0.035\n",
      "Epoch = 439 Step =    3770 loss = 0.035\n",
      "Epoch = 440 Step =    3780 loss = 0.035\n",
      "Epoch = 441 Step =    3790 loss = 0.035\n",
      "Epoch = 442 Step =    3800 loss = 0.035\n",
      "Epoch = 444 Step =    3810 loss = 0.035\n",
      "Epoch = 445 Step =    3820 loss = 0.035\n",
      "Epoch = 446 Step =    3830 loss = 0.035\n",
      "Epoch = 447 Step =    3840 loss = 0.035\n",
      "Epoch = 448 Step =    3850 loss = 0.035\n",
      "Epoch = 449 Step =    3860 loss = 0.034\n",
      "Epoch = 450 Step =    3870 loss = 0.034\n",
      "Epoch = 452 Step =    3880 loss = 0.034\n",
      "Epoch = 453 Step =    3890 loss = 0.034\n",
      "Epoch = 454 Step =    3900 loss = 0.034\n",
      "Epoch = 455 Step =    3910 loss = 0.034\n",
      "Epoch = 456 Step =    3920 loss = 0.034\n",
      "Epoch = 457 Step =    3930 loss = 0.034\n",
      "Epoch = 459 Step =    3940 loss = 0.034\n",
      "Epoch = 460 Step =    3950 loss = 0.034\n",
      "Epoch = 461 Step =    3960 loss = 0.034\n",
      "Epoch = 462 Step =    3970 loss = 0.034\n",
      "Epoch = 463 Step =    3980 loss = 0.034\n",
      "Epoch = 464 Step =    3990 loss = 0.034\n",
      "Epoch = 466 Step =    4000 loss = 0.033\n",
      "Epoch = 467 Step =    4010 loss = 0.033\n",
      "Epoch = 468 Step =    4020 loss = 0.033\n",
      "Epoch = 469 Step =    4030 loss = 0.033\n",
      "Epoch = 470 Step =    4040 loss = 0.033\n",
      "INFO:tensorflow:model/global_step/sec: 4.57512\n",
      "Epoch = 471 Step =    4050 loss = 0.033\n",
      "Epoch = 473 Step =    4060 loss = 0.033\n",
      "Epoch = 474 Step =    4070 loss = 0.033\n",
      "Epoch = 475 Step =    4080 loss = 0.033\n",
      "Epoch = 476 Step =    4090 loss = 0.033\n",
      "Epoch = 477 Step =    4100 loss = 0.033\n",
      "Epoch = 478 Step =    4110 loss = 0.033\n",
      "Epoch = 480 Step =    4120 loss = 0.033\n",
      "Epoch = 481 Step =    4130 loss = 0.033\n",
      "Epoch = 482 Step =    4140 loss = 0.033\n",
      "Epoch = 483 Step =    4150 loss = 0.033\n",
      "Epoch = 484 Step =    4160 loss = 0.032\n",
      "Epoch = 485 Step =    4170 loss = 0.032\n",
      "Epoch = 487 Step =    4180 loss = 0.032\n",
      "Epoch = 488 Step =    4190 loss = 0.032\n",
      "Epoch = 489 Step =    4200 loss = 0.032\n",
      "Epoch = 490 Step =    4210 loss = 0.032\n",
      "Epoch = 491 Step =    4220 loss = 0.032\n",
      "Epoch = 492 Step =    4230 loss = 0.032\n",
      "Epoch = 494 Step =    4240 loss = 0.032\n",
      "Epoch = 495 Step =    4250 loss = 0.032\n",
      "Epoch = 496 Step =    4260 loss = 0.032\n",
      "Epoch = 497 Step =    4270 loss = 0.032\n",
      "Epoch = 498 Step =    4280 loss = 0.032\n",
      "Epoch = 499 Step =    4290 loss = 0.032\n",
      "Epoch = 500 Step =    4300 loss = 0.032\n",
      "Epoch = 502 Step =    4310 loss = 0.032\n",
      "Epoch = 503 Step =    4320 loss = 0.031\n",
      "Epoch = 504 Step =    4330 loss = 0.031\n",
      "Epoch = 505 Step =    4340 loss = 0.031\n",
      "Epoch = 506 Step =    4350 loss = 0.031\n",
      "Epoch = 507 Step =    4360 loss = 0.031\n",
      "Epoch = 509 Step =    4370 loss = 0.031\n",
      "Epoch = 510 Step =    4380 loss = 0.031\n",
      "Epoch = 511 Step =    4390 loss = 0.031\n",
      "Epoch = 512 Step =    4400 loss = 0.031\n",
      "Epoch = 513 Step =    4410 loss = 0.031\n",
      "Epoch = 514 Step =    4420 loss = 0.031\n",
      "Epoch = 516 Step =    4430 loss = 0.031\n",
      "Epoch = 517 Step =    4440 loss = 0.031\n",
      "Epoch = 518 Step =    4450 loss = 0.031\n",
      "Epoch = 519 Step =    4460 loss = 0.031\n",
      "Epoch = 520 Step =    4470 loss = 0.031\n",
      "Epoch = 521 Step =    4480 loss = 0.031\n",
      "Epoch = 523 Step =    4490 loss = 0.031\n",
      "Epoch = 524 Step =    4500 loss = 0.030\n",
      "Epoch = 525 Step =    4510 loss = 0.030\n",
      "Epoch = 526 Step =    4520 loss = 0.030\n",
      "Epoch = 527 Step =    4530 loss = 0.030\n",
      "Epoch = 528 Step =    4540 loss = 0.030\n",
      "Epoch = 530 Step =    4550 loss = 0.030\n",
      "Epoch = 531 Step =    4560 loss = 0.030\n",
      "Epoch = 532 Step =    4570 loss = 0.030\n",
      "Epoch = 533 Step =    4580 loss = 0.030\n",
      "Epoch = 534 Step =    4590 loss = 0.030\n",
      "INFO:tensorflow:Saving checkpoint to path ./trained_models\\model.ckpt\n",
      "Epoch = 535 Step =    4600 loss = 0.030\n",
      "Epoch = 537 Step =    4610 loss = 0.030\n",
      "Epoch = 538 Step =    4620 loss = 0.030\n",
      "Epoch = 539 Step =    4630 loss = 0.030\n",
      "Epoch = 540 Step =    4640 loss = 0.030\n",
      "Epoch = 541 Step =    4650 loss = 0.030\n",
      "Epoch = 542 Step =    4660 loss = 0.030\n",
      "Epoch = 544 Step =    4670 loss = 0.030\n",
      "Epoch = 545 Step =    4680 loss = 0.029\n",
      "Epoch = 546 Step =    4690 loss = 0.029\n",
      "Epoch = 547 Step =    4700 loss = 0.029\n",
      "Epoch = 548 Step =    4710 loss = 0.029\n",
      "Epoch = 549 Step =    4720 loss = 0.029\n",
      "Epoch = 550 Step =    4730 loss = 0.029\n",
      "Epoch = 552 Step =    4740 loss = 0.029\n",
      "Epoch = 553 Step =    4750 loss = 0.029\n",
      "Epoch = 554 Step =    4760 loss = 0.029\n",
      "Epoch = 555 Step =    4770 loss = 0.029\n",
      "Epoch = 556 Step =    4780 loss = 0.029\n",
      "Epoch = 557 Step =    4790 loss = 0.029\n",
      "Epoch = 559 Step =    4800 loss = 0.029\n",
      "Epoch = 560 Step =    4810 loss = 0.029\n",
      "Epoch = 561 Step =    4820 loss = 0.029\n",
      "Epoch = 562 Step =    4830 loss = 0.029\n",
      "Epoch = 563 Step =    4840 loss = 0.029\n",
      "Epoch = 564 Step =    4850 loss = 0.029\n",
      "Epoch = 566 Step =    4860 loss = 0.029\n",
      "Epoch = 567 Step =    4870 loss = 0.029\n",
      "Epoch = 568 Step =    4880 loss = 0.029\n",
      "Epoch = 569 Step =    4890 loss = 0.029\n",
      "Epoch = 570 Step =    4900 loss = 0.028\n",
      "Epoch = 571 Step =    4910 loss = 0.028\n",
      "Epoch = 573 Step =    4920 loss = 0.028\n",
      "Epoch = 574 Step =    4930 loss = 0.028\n",
      "Epoch = 575 Step =    4940 loss = 0.028\n",
      "Epoch = 576 Step =    4950 loss = 0.028\n",
      "Epoch = 577 Step =    4960 loss = 0.028\n",
      "Epoch = 578 Step =    4970 loss = 0.028\n",
      "Epoch = 580 Step =    4980 loss = 0.028\n",
      "Epoch = 581 Step =    4990 loss = 0.028\n",
      "Epoch = 582 Step =    5000 loss = 0.028\n",
      "Epoch = 583 Step =    5010 loss = 0.028\n",
      "Epoch = 584 Step =    5020 loss = 0.028\n",
      "Epoch = 585 Step =    5030 loss = 0.028\n",
      "Epoch = 587 Step =    5040 loss = 0.028\n",
      "Epoch = 588 Step =    5050 loss = 0.028\n",
      "Epoch = 589 Step =    5060 loss = 0.028\n",
      "Epoch = 590 Step =    5070 loss = 0.028\n",
      "Epoch = 591 Step =    5080 loss = 0.028\n",
      "Epoch = 592 Step =    5090 loss = 0.028\n",
      "Epoch = 594 Step =    5100 loss = 0.028\n",
      "Epoch = 595 Step =    5110 loss = 0.028\n",
      "Epoch = 596 Step =    5120 loss = 0.027\n",
      "Epoch = 597 Step =    5130 loss = 0.027\n",
      "Epoch = 598 Step =    5140 loss = 0.027\n",
      "Epoch = 599 Step =    5150 loss = 0.027\n",
      "Epoch = 600 Step =    5160 loss = 0.027\n",
      "Epoch = 602 Step =    5170 loss = 0.027\n",
      "Epoch = 603 Step =    5180 loss = 0.027\n",
      "Epoch = 604 Step =    5190 loss = 0.027\n",
      "Epoch = 605 Step =    5200 loss = 0.027\n",
      "Epoch = 606 Step =    5210 loss = 0.027\n",
      "Epoch = 607 Step =    5220 loss = 0.027\n",
      "Epoch = 609 Step =    5230 loss = 0.027\n",
      "Epoch = 610 Step =    5240 loss = 0.027\n",
      "Epoch = 611 Step =    5250 loss = 0.027\n",
      "Epoch = 612 Step =    5260 loss = 0.027\n",
      "Epoch = 613 Step =    5270 loss = 0.027\n",
      "Epoch = 614 Step =    5280 loss = 0.027\n",
      "Epoch = 616 Step =    5290 loss = 0.027\n",
      "Epoch = 617 Step =    5300 loss = 0.027\n",
      "Epoch = 618 Step =    5310 loss = 0.027\n",
      "Epoch = 619 Step =    5320 loss = 0.027\n",
      "Epoch = 620 Step =    5330 loss = 0.027\n",
      "Epoch = 621 Step =    5340 loss = 0.027\n",
      "Epoch = 623 Step =    5350 loss = 0.027\n",
      "Epoch = 624 Step =    5360 loss = 0.027\n",
      "Epoch = 625 Step =    5370 loss = 0.026\n",
      "Epoch = 626 Step =    5380 loss = 0.026\n",
      "Epoch = 627 Step =    5390 loss = 0.026\n",
      "Epoch = 628 Step =    5400 loss = 0.026\n",
      "Epoch = 630 Step =    5410 loss = 0.026\n",
      "Epoch = 631 Step =    5420 loss = 0.026\n",
      "Epoch = 632 Step =    5430 loss = 0.026\n",
      "Epoch = 633 Step =    5440 loss = 0.026\n",
      "Epoch = 634 Step =    5450 loss = 0.026\n",
      "Epoch = 635 Step =    5460 loss = 0.026\n",
      "Epoch = 637 Step =    5470 loss = 0.026\n",
      "Epoch = 638 Step =    5480 loss = 0.026\n",
      "Epoch = 639 Step =    5490 loss = 0.026\n",
      "Epoch = 640 Step =    5500 loss = 0.026\n",
      "Epoch = 641 Step =    5510 loss = 0.026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 642 Step =    5520 loss = 0.026\n",
      "Epoch = 644 Step =    5530 loss = 0.026\n",
      "Epoch = 645 Step =    5540 loss = 0.026\n",
      "Epoch = 646 Step =    5550 loss = 0.026\n",
      "Epoch = 647 Step =    5560 loss = 0.026\n",
      "Epoch = 648 Step =    5570 loss = 0.026\n",
      "Epoch = 649 Step =    5580 loss = 0.026\n",
      "Epoch = 650 Step =    5590 loss = 0.026\n",
      "Epoch = 652 Step =    5600 loss = 0.026\n",
      "Epoch = 653 Step =    5610 loss = 0.026\n",
      "Epoch = 654 Step =    5620 loss = 0.026\n",
      "Epoch = 655 Step =    5630 loss = 0.025\n",
      "Epoch = 656 Step =    5640 loss = 0.025\n",
      "Epoch = 657 Step =    5650 loss = 0.025\n",
      "Epoch = 659 Step =    5660 loss = 0.025\n",
      "Epoch = 660 Step =    5670 loss = 0.025\n",
      "Epoch = 661 Step =    5680 loss = 0.025\n",
      "Epoch = 662 Step =    5690 loss = 0.025\n",
      "Epoch = 663 Step =    5700 loss = 0.025\n",
      "Epoch = 664 Step =    5710 loss = 0.025\n",
      "Epoch = 666 Step =    5720 loss = 0.025\n",
      "Epoch = 667 Step =    5730 loss = 0.025\n",
      "Epoch = 668 Step =    5740 loss = 0.025\n",
      "Epoch = 669 Step =    5750 loss = 0.025\n",
      "Epoch = 670 Step =    5760 loss = 0.025\n",
      "Epoch = 671 Step =    5770 loss = 0.025\n",
      "Epoch = 673 Step =    5780 loss = 0.025\n",
      "Epoch = 674 Step =    5790 loss = 0.025\n",
      "Epoch = 675 Step =    5800 loss = 0.025\n",
      "Epoch = 676 Step =    5810 loss = 0.025\n",
      "Epoch = 677 Step =    5820 loss = 0.025\n",
      "Epoch = 678 Step =    5830 loss = 0.025\n",
      "Epoch = 680 Step =    5840 loss = 0.025\n",
      "Epoch = 681 Step =    5850 loss = 0.025\n",
      "Epoch = 682 Step =    5860 loss = 0.025\n",
      "Epoch = 683 Step =    5870 loss = 0.025\n",
      "Epoch = 684 Step =    5880 loss = 0.025\n",
      "Epoch = 685 Step =    5890 loss = 0.025\n",
      "Epoch = 687 Step =    5900 loss = 0.025\n",
      "Epoch = 688 Step =    5910 loss = 0.025\n",
      "Epoch = 689 Step =    5920 loss = 0.025\n",
      "Epoch = 690 Step =    5930 loss = 0.025\n",
      "Epoch = 691 Step =    5940 loss = 0.025\n",
      "Epoch = 692 Step =    5950 loss = 0.024\n",
      "Epoch = 694 Step =    5960 loss = 0.024\n",
      "Epoch = 695 Step =    5970 loss = 0.024\n",
      "Epoch = 696 Step =    5980 loss = 0.024\n",
      "Epoch = 697 Step =    5990 loss = 0.024\n",
      "Epoch = 698 Step =    6000 loss = 0.024\n",
      "Epoch = 699 Step =    6010 loss = 0.024\n",
      "Epoch = 700 Step =    6020 loss = 0.024\n",
      "Epoch = 702 Step =    6030 loss = 0.024\n",
      "Epoch = 703 Step =    6040 loss = 0.024\n",
      "Epoch = 704 Step =    6050 loss = 0.024\n",
      "Epoch = 705 Step =    6060 loss = 0.024\n",
      "Epoch = 706 Step =    6070 loss = 0.024\n",
      "Epoch = 707 Step =    6080 loss = 0.024\n",
      "Epoch = 709 Step =    6090 loss = 0.024\n",
      "Epoch = 710 Step =    6100 loss = 0.024\n",
      "Epoch = 711 Step =    6110 loss = 0.024\n",
      "Epoch = 712 Step =    6120 loss = 0.024\n",
      "Epoch = 713 Step =    6130 loss = 0.024\n",
      "Epoch = 714 Step =    6140 loss = 0.024\n",
      "Epoch = 716 Step =    6150 loss = 0.024\n",
      "Epoch = 717 Step =    6160 loss = 0.024\n",
      "Epoch = 718 Step =    6170 loss = 0.024\n",
      "Epoch = 719 Step =    6180 loss = 0.024\n",
      "Epoch = 720 Step =    6190 loss = 0.024\n",
      "Epoch = 721 Step =    6200 loss = 0.024\n",
      "Epoch = 723 Step =    6210 loss = 0.024\n",
      "Epoch = 724 Step =    6220 loss = 0.024\n",
      "Epoch = 725 Step =    6230 loss = 0.024\n",
      "Epoch = 726 Step =    6240 loss = 0.024\n",
      "Epoch = 727 Step =    6250 loss = 0.024\n",
      "Epoch = 728 Step =    6260 loss = 0.024\n",
      "Epoch = 730 Step =    6270 loss = 0.024\n",
      "Epoch = 731 Step =    6280 loss = 0.024\n",
      "Epoch = 732 Step =    6290 loss = 0.024\n",
      "Epoch = 733 Step =    6300 loss = 0.023\n",
      "Epoch = 734 Step =    6310 loss = 0.023\n",
      "Epoch = 735 Step =    6320 loss = 0.023\n",
      "Epoch = 737 Step =    6330 loss = 0.023\n",
      "Epoch = 738 Step =    6340 loss = 0.023\n",
      "Epoch = 739 Step =    6350 loss = 0.023\n",
      "Epoch = 740 Step =    6360 loss = 0.023\n",
      "Epoch = 741 Step =    6370 loss = 0.023\n",
      "Epoch = 742 Step =    6380 loss = 0.023\n",
      "Epoch = 744 Step =    6390 loss = 0.023\n",
      "Epoch = 745 Step =    6400 loss = 0.023\n",
      "Epoch = 746 Step =    6410 loss = 0.023\n",
      "Epoch = 747 Step =    6420 loss = 0.023\n",
      "Epoch = 748 Step =    6430 loss = 0.023\n",
      "Epoch = 749 Step =    6440 loss = 0.023\n",
      "Epoch = 750 Step =    6450 loss = 0.023\n",
      "Epoch = 752 Step =    6460 loss = 0.023\n",
      "Epoch = 753 Step =    6470 loss = 0.023\n",
      "Epoch = 754 Step =    6480 loss = 0.023\n",
      "Epoch = 755 Step =    6490 loss = 0.023\n",
      "Epoch = 756 Step =    6500 loss = 0.023\n",
      "Epoch = 757 Step =    6510 loss = 0.023\n",
      "Epoch = 759 Step =    6520 loss = 0.023\n",
      "Epoch = 760 Step =    6530 loss = 0.023\n",
      "Epoch = 761 Step =    6540 loss = 0.023\n",
      "Epoch = 762 Step =    6550 loss = 0.023\n",
      "Epoch = 763 Step =    6560 loss = 0.023\n",
      "Epoch = 764 Step =    6570 loss = 0.023\n",
      "Epoch = 766 Step =    6580 loss = 0.023\n",
      "Epoch = 767 Step =    6590 loss = 0.023\n",
      "Epoch = 768 Step =    6600 loss = 0.023\n",
      "Epoch = 769 Step =    6610 loss = 0.023\n",
      "Epoch = 770 Step =    6620 loss = 0.023\n",
      "Epoch = 771 Step =    6630 loss = 0.023\n",
      "Epoch = 773 Step =    6640 loss = 0.023\n",
      "Epoch = 774 Step =    6650 loss = 0.023\n",
      "Epoch = 775 Step =    6660 loss = 0.023\n",
      "Epoch = 776 Step =    6670 loss = 0.023\n",
      "Epoch = 777 Step =    6680 loss = 0.023\n",
      "Epoch = 778 Step =    6690 loss = 0.023\n",
      "Epoch = 780 Step =    6700 loss = 0.023\n",
      "Epoch = 781 Step =    6710 loss = 0.023\n",
      "Epoch = 782 Step =    6720 loss = 0.022\n",
      "Epoch = 783 Step =    6730 loss = 0.022\n",
      "Epoch = 784 Step =    6740 loss = 0.022\n",
      "Epoch = 785 Step =    6750 loss = 0.022\n",
      "Epoch = 787 Step =    6760 loss = 0.022\n",
      "Epoch = 788 Step =    6770 loss = 0.022\n",
      "Epoch = 789 Step =    6780 loss = 0.022\n",
      "Epoch = 790 Step =    6790 loss = 0.022\n",
      "Epoch = 791 Step =    6800 loss = 0.022\n",
      "Epoch = 792 Step =    6810 loss = 0.022\n",
      "Epoch = 794 Step =    6820 loss = 0.022\n",
      "Epoch = 795 Step =    6830 loss = 0.022\n",
      "Epoch = 796 Step =    6840 loss = 0.022\n",
      "Epoch = 797 Step =    6850 loss = 0.022\n",
      "Epoch = 798 Step =    6860 loss = 0.022\n",
      "Epoch = 799 Step =    6870 loss = 0.022\n",
      "Epoch = 800 Step =    6880 loss = 0.022\n",
      "Training is done.\n",
      "INFO:tensorflow:Restoring parameters from ./trained_models\\model.ckpt-4590\n",
      "INFO:tensorflow:Froze 11 variables.\n",
      "INFO:tensorflow:Converted 11 variables to const ops.\n",
      "1683 ops in the final graph.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Named Entity Recognition \n",
    "\n",
    "        Author : Sangkeun Jung (2017)\n",
    "        - using Tensorflow\n",
    "\"\"\"\n",
    "\n",
    "import sys, os, inspect\n",
    "\n",
    "# add common to path\n",
    "from pathlib import Path\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "common_path = str(Path(currentdir).parent.parent)\n",
    "sys.path.append( common_path )\n",
    "\n",
    "from common.nlp.vocab import Vocab\n",
    "from common.nlp.data_loader import N2NTextData\n",
    "from common.nlp.converter import N2NConverter\n",
    "\n",
    "from dataset import NERDataset\n",
    "from dataset import load_data\n",
    "from common.ml.hparams import HParams\n",
    "\n",
    "import numpy as np\n",
    "import copy \n",
    "import time \n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn\n",
    "from tensorflow.contrib.layers.python.layers import linear\n",
    "from tensorflow.python.ops import variable_scope\n",
    "from tensorflow.contrib.seq2seq import sequence_loss#sequence의 loss의 평균값을 구할 수있음\n",
    "\n",
    "from common.ml.tf.deploy import freeze_graph\n",
    "\n",
    "\n",
    "\n",
    "print( \"Tensorflow Version : \", tf.__version__)\n",
    "\n",
    "class NER():\n",
    "    def __init__(self, hps, mode=\"train\"):\n",
    "        self.hps = hps\n",
    "        self.x = tf.placeholder(tf.int32,   [None, hps.num_steps], name=\"pl_tokens\")\n",
    "        self.y = tf.placeholder(tf.int32,   [None, hps.num_steps], name=\"pl_target\")\n",
    "        self.w = tf.placeholder(tf.float32, [None, hps.num_steps], name=\"pl_weight\")\n",
    "        self.keep_prob = tf.placeholder(tf.float32, [], name=\"pl_keep_prob\")\n",
    "\n",
    "        ### 4 blocks ###\n",
    "        # 1) embedding\n",
    "        # 2) dropout on input embedding\n",
    "        # 3) sentence encoding using rnn\n",
    "        # 4) bidirectional rnn's output to target classes\n",
    "        # 5) loss calcaulation\n",
    "\n",
    "        def _embedding(x):\n",
    "            # character embedding \n",
    "            shape       = [hps.vocab_size, hps.emb_size]\n",
    "            initializer = tf.initializers.variance_scaling(distribution=\"uniform\", dtype=tf.float32)\n",
    "            emb_mat     = tf.get_variable(\"emb\", shape, initializer=initializer, dtype=tf.float32)\n",
    "            input_emb   = tf.nn.embedding_lookup(emb_mat, x)   # [batch_size, sent_len, emb_dim]\n",
    "\n",
    "            # split input_emb -> num_steps\n",
    "            step_inputs = tf.unstack(input_emb, axis=1)\n",
    "            return step_inputs\n",
    "\n",
    "        def _sequence_dropout(step_inputs, keep_prob):\n",
    "            # apply dropout to each input\n",
    "            # input : a list of input tensor which shape is [None, input_dim]\n",
    "            with tf.name_scope('sequence_dropout') as scope:\n",
    "                step_outputs = []\n",
    "                for t, input in enumerate(step_inputs):\n",
    "                    step_outputs.append( tf.nn.dropout(input, keep_prob) )\n",
    "            return step_outputs\n",
    "\n",
    "        def sequence_encoding_n2n(step_inputs, seq_length, cell_size):\n",
    "            # birnn based N2N encoding and output\n",
    "            f_rnn_cell = tf.contrib.rnn.GRUCell(cell_size, reuse=False)\n",
    "            b_rnn_cell = tf.contrib.rnn.GRUCell(cell_size, reuse=False)\n",
    "            _inputs    = tf.stack(step_inputs, axis=1)\n",
    "\n",
    "            # step_inputs = a list of [batch_size, emb_dim]\n",
    "            # input = [batch_size, num_step, emb_dim]\n",
    "            # np.stack( [a,b,c,] )\n",
    "            outputs, states, = tf.nn.bidirectional_dynamic_rnn( f_rnn_cell,\n",
    "                                                                b_rnn_cell,\n",
    "                                                                _inputs,\n",
    "                                                                sequence_length=tf.cast(seq_length, tf.int64),\n",
    "                                                                time_major=False,\n",
    "                                                                dtype=tf.float32,\n",
    "                                                                scope='birnn',\n",
    "                                                            )\n",
    "            output_fw, output_bw = outputs\n",
    "            states_fw, states_bw = states \n",
    "\n",
    "            output       = tf.concat([output_fw, output_bw], 2)\n",
    "            step_outputs = tf.unstack(output, axis=1)\n",
    "\n",
    "            final_state  = tf.concat([states_fw, states_bw], 1)\n",
    "            return step_outputs # a list of [batch_size, enc_dim]\n",
    "\n",
    "        def _to_class_n2n(step_inputs, num_class):\n",
    "            T = len(step_inputs)\n",
    "            step_output_logits = []\n",
    "            for t in range(T):\n",
    "                # encoder to linear(map)\n",
    "                out = step_inputs[t]\n",
    "                if t==0: out = linear(out, num_class, scope=\"Rnn2Target\")\n",
    "                else:    out = linear(out, num_class, scope=\"Rnn2Target\", reuse=True)\n",
    "                step_output_logits.append(out)\n",
    "            return step_output_logits\n",
    "\n",
    "        def _loss(step_outputs, step_refs, weights):\n",
    "            # step_outputs : a list of [batch_size, num_class] float32 - unscaled logits\n",
    "            # step_refs    : [batch_size, num_steps] int32\n",
    "            # weights      : [batch_size, num_steps] float32\n",
    "            # calculate sequence wise loss function using cross-entropy\n",
    "            _batch_output_logits = tf.stack(step_outputs, axis=1)\n",
    "            loss = sequence_loss(\n",
    "                                    logits=_batch_output_logits,        \n",
    "                                    targets=step_refs,\n",
    "                                    weights=weights\n",
    "                                )\n",
    "            return loss\n",
    "        \n",
    "        seq_length    = tf.reduce_sum(self.w, 1) # [batch_size]\n",
    "\n",
    "        step_inputs       = _embedding(self.x)\n",
    "        step_inputs       = _sequence_dropout(step_inputs, self.keep_prob)\n",
    "        step_enc_outputs  = sequence_encoding_n2n(step_inputs, seq_length, hps.enc_dim)\n",
    "        step_outputs      = _to_class_n2n(step_enc_outputs, hps.num_target_class)\n",
    "\n",
    "        self.loss = _loss(step_outputs, self.y, self.w)\n",
    "\n",
    "        # step_preds and step_out_probs\n",
    "        step_out_probs = []\n",
    "        step_out_preds = []\n",
    "        for _output in step_outputs:\n",
    "            _out_probs  = tf.nn.softmax(_output)\n",
    "            _out_pred   = tf.argmax(_out_probs, 1)\n",
    "\n",
    "            step_out_probs.append(_out_probs)\n",
    "            step_out_preds.append(_out_pred)\n",
    "\n",
    "        # stack for interface\n",
    "        self.step_out_probs = tf.stack(step_out_probs, axis=1, name=\"step_out_probs\")\n",
    "        self.step_out_preds = tf.stack(step_out_preds, axis=1, name=\"step_out_preds\")\n",
    "\n",
    "        self.global_step = tf.get_variable(\"global_step\", [], tf.int32, initializer=tf.zeros_initializer, trainable=False)\n",
    "\n",
    "        if mode == \"train\":\n",
    "            optimizer       = tf.train.AdamOptimizer(hps.learning_rate)\n",
    "            self.train_op   = optimizer.minimize(self.loss, global_step=self.global_step)\n",
    "        else:\n",
    "            self.train_op = tf.no_op()\n",
    "\n",
    "        for v in tf.trainable_variables(): print(v.name)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_default_hparams():\n",
    "        return HParams(\n",
    "            learning_rate     = 0.005,\n",
    "            keep_prob         = 0.5,\n",
    "        )\n",
    "\n",
    "\n",
    "def train(train_id_data, num_vocabs, num_taget_class):\n",
    "    #\n",
    "    # train sentiment analysis using given train_id_data\n",
    "    #\n",
    "    max_epoch = 800\n",
    "    model_dir = \"./trained_models\"\n",
    "    hps = NER.get_default_hparams()\n",
    "    hps.update(\n",
    "                    batch_size= 100,\n",
    "                    num_steps = 85,\n",
    "                    emb_size  = 40,\n",
    "                    enc_dim   = 100,\n",
    "                    vocab_size=num_vocabs,\n",
    "                    num_target_class=num_taget_class\n",
    "               )\n",
    "\n",
    "    with tf.variable_scope(\"model\"):\n",
    "        model = NER(hps, \"train\")\n",
    "\n",
    "    sv = tf.train.Supervisor(is_chief=True,\n",
    "                             logdir=model_dir,\n",
    "                             summary_op=None,  \n",
    "                             global_step=model.global_step)\n",
    "\n",
    "    # tf assign compatible operators for gpu and cpu \n",
    "    tf_config = tf.ConfigProto(allow_soft_placement=True)\n",
    "\n",
    "    with sv.managed_session(config=tf_config) as sess:\n",
    "        local_step       = 0\n",
    "        prev_global_step = sess.run(model.global_step)\n",
    "\n",
    "        train_data_set = NERDataset(train_id_data, hps.batch_size, hps.num_steps)\n",
    "        losses = []\n",
    "        while not sv.should_stop():\n",
    "            fetches = [model.global_step, model.loss, model.train_op]\n",
    "            a_batch_data = next( train_data_set.iterator )\n",
    "            y, x, w = a_batch_data\n",
    "            fetched = sess.run(fetches, {\n",
    "                                            model.x: x, \n",
    "                                            model.y: y, \n",
    "                                            model.w: w,\n",
    "\n",
    "                                            model.keep_prob: hps.keep_prob,\n",
    "                                        }\n",
    "                              )\n",
    "\n",
    "            local_step += 1\n",
    "\n",
    "            _global_step = fetched[0]\n",
    "            _loss        = fetched[1]\n",
    "            losses.append( _loss )\n",
    "            if local_step < 10 or local_step % 10 == 0:\n",
    "                epoch = train_data_set.get_epoch_num()\n",
    "                print(\"Epoch = {:3d} Step = {:7d} loss = {:5.3f}\".format(epoch, _global_step, np.mean(losses)) )\n",
    "                _loss = []                \n",
    "                if epoch >= max_epoch : break \n",
    "\n",
    "        print(\"Training is done.\")\n",
    "    sv.stop()\n",
    "\n",
    "    # model.out_pred, model.out_probs\n",
    "    freeze_graph(model_dir, \"model/step_out_preds,model/step_out_probs\", \"frozen_graph.tf.pb\") ## freeze graph with params to probobuf format\n",
    "    \n",
    "from tensorflow.core.framework import graph_pb2\n",
    "def predict(token_vocab, target_vocab, sent):\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # force to use cpu only (prediction)\n",
    "    model_dir = \"./trained_models\"\n",
    "\n",
    "    # prepare sentence converting\n",
    "    # to make raw sentence to id data easily\n",
    "    pred_data     = N2NTextData(sent, mode='sentence')\n",
    "    pred_id_data  = N2NConverter.convert(pred_data, target_vocab, token_vocab)\n",
    "    pred_data_set = NERDataset(pred_id_data, 1, 85)\n",
    "    #\n",
    "    a_batch_data = next(pred_data_set.predict_iterator) # a result\n",
    "    b_nes_id, b_token_ids, b_weight = a_batch_data\n",
    "\n",
    "    # Restore graph\n",
    "    # note that frozen_graph.tf.pb contains graph definition with parameter values in binary format\n",
    "    _graph_fn =  os.path.join(model_dir, 'frozen_graph.tf.pb')\n",
    "    with tf.gfile.GFile(_graph_fn, \"rb\") as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "    \n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.import_graph_def(graph_def)\n",
    "\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        # to check load graph\n",
    "        #for n in tf.get_default_graph().as_graph_def().node: print(n.name)\n",
    "\n",
    "        # make interface for input\n",
    "        pl_token     = graph.get_tensor_by_name('import/model/pl_tokens:0')\n",
    "        pl_weight    = graph.get_tensor_by_name('import/model/pl_weight:0')\n",
    "        pl_keep_prob = graph.get_tensor_by_name('import/model/pl_keep_prob:0')\n",
    "\n",
    "        # make interface for output\n",
    "        step_out_preds = graph.get_tensor_by_name('import/model/step_out_preds:0')\n",
    "        step_out_probs = graph.get_tensor_by_name('import/model/step_out_probs:0')\n",
    "        \n",
    "\n",
    "        # predict sentence \n",
    "        b_best_step_pred_indexs, b_step_pred_probs = sess.run([step_out_preds, step_out_probs], \n",
    "                                                              feed_dict={\n",
    "                                                                            pl_token  : b_token_ids,\n",
    "                                                                            pl_weight : b_weight,\n",
    "                                                                            pl_keep_prob : 1.0,\n",
    "                                                                        }\n",
    "                                                             )\n",
    "        best_step_pred_indexs = b_best_step_pred_indexs[0]\n",
    "        step_pred_probs = b_step_pred_probs[0]\n",
    "\n",
    "        step_best_targets = []\n",
    "        step_best_target_probs = []\n",
    "        for time_step, best_pred_index in enumerate(best_step_pred_indexs):\n",
    "            _target_class = target_vocab.get_symbol(best_pred_index)\n",
    "            step_best_targets.append( _target_class )\n",
    "            _prob = step_pred_probs[time_step][best_pred_index]\n",
    "            step_best_target_probs.append( _prob ) \n",
    "        for idx, char in enumerate(list(sent)):\n",
    "            print('{}\\t:{}\\t'.format(char, step_best_targets[idx], step_best_target_probs[idx]) ) \n",
    "        #return list(sent)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_id_data, token_vocab, target_vocab = load_data()\n",
    "    num_vocabs       = token_vocab.get_num_tokens()\n",
    "    num_target_class = target_vocab.get_num_targets()\n",
    "\n",
    "    train_data_set = NERDataset(train_id_data, 5, 85)\n",
    "    train(train_id_data, num_vocabs, num_target_class)\n",
    "    \n",
    "    #predict(token_vocab, target_vocab, '아프가니스탄의 장래를 더욱 불투명하게 하는 것은 강경파 헤즈비 이슬라미와 우즈베크 민병대의 대립이다.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 65\n",
      "처\t:O\t\n",
      "음\t:O\t\n",
      "으\t:O\t\n",
      "로\t:O\t\n",
      " \t:O\t\n",
      "해\t:O\t\n",
      "외\t:O\t\n",
      "여\t:O\t\n",
      "행\t:O\t\n",
      "을\t:O\t\n",
      " \t:O\t\n",
      "가\t:O\t\n",
      "볼\t:O\t\n",
      "까\t:O\t\n",
      "합\t:O\t\n",
      "니\t:O\t\n",
      "다\t:O\t\n",
      " \t:O\t\n",
      "다\t:O\t\n",
      "녀\t:O\t\n",
      "오\t:I-AC\t\n",
      "신\t:O\t\n",
      "분\t:O\t\n",
      "이\t:O\t\n",
      "나\t:O\t\n",
      " \t:O\t\n",
      "계\t:O\t\n",
      "획\t:O\t\n",
      "하\t:O\t\n",
      "시\t:O\t\n",
      "는\t:O\t\n",
      " \t:O\t\n",
      "분\t:O\t\n",
      "들\t:O\t\n",
      "의\t:O\t\n",
      " \t:O\t\n",
      "계\t:O\t\n",
      "획\t:O\t\n",
      "을\t:O\t\n",
      " \t:O\t\n",
      "듣\t:O\t\n",
      "고\t:O\t\n",
      "싶\t:O\t\n",
      "네\t:O\t\n",
      "요\t:O\t\n",
      " \t:O\t\n",
      "감\t:O\t\n",
      "사\t:O\t\n",
      "합\t:O\t\n",
      "니\t:O\t\n",
      "다\t:O\t\n",
      " \t:O\t\n",
      "3\t:B-DT\t\n",
      "월\t:I-DT\t\n",
      "초\t:I-DT\t\n",
      "나\t:O\t\n",
      "중\t:O\t\n",
      "순\t:O\t\n",
      "에\t:O\t\n",
      " \t:O\t\n",
      "갈\t:O\t\n",
      "예\t:O\t\n",
      "정\t:O\t\n",
      "입\t:O\t\n",
      "니\t:O\t\n",
      "다\t:O\t\n",
      "\n",
      "\t:O\t\n",
      "올\t:O\t\n",
      " \t:O\t\n",
      "1\t:B-DT\t\n",
      "0\t:I-DT\t\n",
      "월\t:I-DT\t\n",
      "~\t:I-DT\t\n",
      "1\t:I-DT\t\n",
      "2\t:I-DT\t\n",
      "월\t:I-DT\t\n",
      " \t:I-DT\t\n",
      "사\t:I-DT\t\n",
      "이\t:O\t\n",
      "에\t:O\t\n",
      " \t:O\t\n",
      "부\t:B-AC\t\n",
      "모\t:I-AC\t\n",
      "님\t:I-AC\t\n",
      " \t:O\t\n",
      "첫\t:O\t\n",
      " \t:O\t\n",
      "해\t:O\t\n",
      "외\t:O\t\n",
      "여\t:O\t\n",
      "행\t:O\t\n",
      " \t:O\t\n",
      "보\t:O\t\n",
      "내\t:O\t\n",
      "드\t:O\t\n",
      "리\t:O\t\n",
      "려\t:O\t\n",
      "고\t:O\t\n",
      " \t:O\t\n",
      "합\t:O\t\n",
      "니\t:O\t\n",
      "다\t:O\t\n",
      ".\t:O\t\n",
      "예\t:O\t\n",
      "산\t:O\t\n",
      "은\t:O\t\n",
      " \t:O\t\n",
      "두\t:O\t\n",
      "분\t:O\t\n",
      "해\t:O\t\n",
      "서\t:O\t\n",
      " \t:O\t\n",
      "3\t:O\t\n",
      "0\t:O\t\n",
      "0\t:O\t\n",
      "내\t:O\t\n",
      "외\t:O\t\n",
      "로\t:O\t\n",
      " \t:O\t\n",
      "생\t:O\t\n",
      "각\t:O\t\n",
      "하\t:O\t\n",
      "고\t:O\t\n",
      " \t:O\t\n",
      "있\t:O\t\n",
      "고\t:O\t\n",
      "요\t:O\t\n",
      ".\t:O\t\n",
      "\n",
      "\t:O\t\n",
      "9\t:B-DT\t\n",
      "월\t:I-DT\t\n",
      "정\t:I-DT\t\n",
      "도\t:I-DT\t\n",
      "에\t:O\t\n",
      " \t:O\t\n",
      "친\t:B-AC\t\n",
      "구\t:I-AC\t\n",
      "들\t:O\t\n",
      "끼\t:O\t\n",
      "리\t:O\t\n",
      " \t:O\t\n",
      "여\t:O\t\n",
      "행\t:O\t\n",
      "을\t:O\t\n",
      "가\t:O\t\n",
      "려\t:O\t\n",
      "고\t:O\t\n",
      " \t:O\t\n",
      "하\t:O\t\n",
      "는\t:O\t\n",
      "데\t:O\t\n",
      "요\t:O\t\n",
      " \t:O\t\n",
      "해\t:O\t\n",
      "외\t:O\t\n",
      "여\t:O\t\n",
      "행\t:O\t\n",
      "은\t:O\t\n",
      " \t:O\t\n",
      "처\t:O\t\n",
      "음\t:O\t\n",
      "이\t:O\t\n",
      "라\t:O\t\n",
      " \t:O\t\n",
      "겁\t:O\t\n",
      "도\t:O\t\n",
      " \t:O\t\n",
      "나\t:O\t\n",
      "네\t:O\t\n",
      "요\t:O\t\n",
      ".\t:O\t\n",
      " \t:O\t\n",
      "경\t:O\t\n",
      "비\t:O\t\n",
      "가\t:O\t\n",
      " \t:O\t\n",
      "저\t:O\t\n",
      "렴\t:O\t\n",
      "하\t:O\t\n",
      "면\t:O\t\n",
      "서\t:O\t\n",
      "도\t:O\t\n",
      " \t:O\t\n",
      "좋\t:O\t\n",
      "은\t:O\t\n",
      "곳\t:O\t\n",
      " \t:O\t\n",
      "추\t:O\t\n",
      "천\t:O\t\n",
      "바\t:O\t\n",
      "랍\t:O\t\n",
      "니\t:O\t\n",
      "다\t:O\t\n",
      ".\t:O\t\n",
      "^\t:O\t\n",
      "^\t:O\t\n",
      "\n",
      "\t:O\t\n",
      "2\t:O\t\n",
      "0\t:O\t\n",
      "대\t:O\t\n",
      " \t:O\t\n",
      "여\t:O\t\n",
      "자\t:O\t\n",
      " \t:O\t\n",
      "셋\t:I-AC\t\n",
      "이\t:O\t\n",
      "에\t:O\t\n",
      "요\t:O\t\n",
      ",\t:O\t\n",
      ",\t:O\t\n",
      " \t:O\t\n",
      "ㅎ\t:O\t\n",
      "ㅎ\t:O\t\n",
      "1\t:O\t\n",
      "0\t:O\t\n",
      "년\t:O\t\n",
      "이\t:O\t\n",
      " \t:O\t\n",
      "넘\t:O\t\n",
      "은\t:O\t\n",
      " \t:O\t\n",
      "친\t:B-AC\t\n",
      "구\t:I-AC\t\n",
      "들\t:O\t\n",
      "인\t:O\t\n",
      "데\t:O\t\n",
      " \t:O\t\n",
      "드\t:O\t\n",
      "디\t:O\t\n",
      "어\t:O\t\n",
      " \t:O\t\n",
      "시\t:O\t\n",
      "간\t:O\t\n",
      "이\t:O\t\n",
      " \t:O\t\n",
      "맞\t:O\t\n",
      "아\t:O\t\n",
      "서\t:O\t\n",
      " \t:O\t\n",
      "첫\t:O\t\n",
      " \t:O\t\n",
      "해\t:O\t\n",
      "외\t:O\t\n",
      "여\t:O\t\n",
      "행\t:O\t\n",
      "을\t:O\t\n",
      " \t:O\t\n",
      "가\t:O\t\n",
      "기\t:O\t\n",
      "로\t:O\t\n",
      "했\t:O\t\n",
      "답\t:O\t\n",
      "니\t:O\t\n",
      "다\t:O\t\n",
      "\n",
      "\t:O\t\n",
      "1\t:B-DT\t\n",
      "1\t:I-DT\t\n",
      "월\t:I-DT\t\n",
      "2\t:O\t\n",
      "0\t:O\t\n",
      "~\t:O\t\n",
      "3\t:O\t\n",
      "0\t:O\t\n",
      "일\t:O\t\n",
      " \t:O\t\n",
      "사\t:O\t\n",
      "이\t:O\t\n",
      "에\t:O\t\n",
      " \t:O\t\n",
      "3\t:B-PR\t\n",
      "박\t:I-PR\t\n",
      "5\t:I-PR\t\n",
      "일\t:I-PR\t\n",
      " \t:O\t\n",
      "정\t:O\t\n",
      "도\t:O\t\n",
      " \t:O\t\n",
      "갈\t:O\t\n",
      "꺼\t:O\t\n",
      "거\t:O\t\n",
      "든\t:O\t\n",
      "요\t:O\t\n",
      "경\t:O\t\n",
      "비\t:O\t\n",
      "는\t:O\t\n",
      " \t:O\t\n",
      "비\t:O\t\n",
      "행\t:O\t\n",
      "기\t:O\t\n",
      "값\t:O\t\n",
      "까\t:O\t\n",
      "지\t:O\t\n",
      " \t:O\t\n",
      "1\t:O\t\n",
      "인\t:O\t\n",
      "당\t:O\t\n",
      " \t:O\t\n",
      "7\t:O\t\n",
      ".\t:O\t\n",
      "8\t:O\t\n",
      "0\t:O\t\n",
      " \t:O\t\n",
      "생\t:O\t\n",
      "각\t:O\t\n",
      "해\t:O\t\n",
      "요\t:O\t\n",
      "\n",
      "\t:O\t\n",
      "근\t:O\t\n",
      "데\t:O\t\n",
      " \t:O\t\n",
      "셋\t:O\t\n",
      " \t:O\t\n",
      "다\t:O\t\n",
      " \t:O\t\n",
      "여\t:O\t\n",
      "행\t:O\t\n",
      "지\t:O\t\n",
      "식\t:O\t\n",
      " \t:O\t\n",
      "문\t:O\t\n",
      "외\t:O\t\n",
      "한\t:O\t\n",
      "이\t:O\t\n",
      "라\t:O\t\n",
      " \t:O\t\n",
      "몇\t:O\t\n",
      "일\t:O\t\n",
      "째\t:O\t\n",
      " \t:O\t\n",
      "정\t:O\t\n",
      "하\t:O\t\n",
      "지\t:O\t\n",
      "도\t:O\t\n",
      " \t:O\t\n",
      "못\t:O\t\n",
      "하\t:O\t\n",
      "고\t:O\t\n",
      "있\t:O\t\n",
      "네\t:O\t\n",
      "요\t:O\t\n",
      "맨\t:O\t\n",
      " \t:O\t\n",
      "처\t:O\t\n",
      "음\t:O\t\n",
      " \t:O\t\n",
      "무\t:O\t\n",
      "턱\t:O\t\n",
      "대\t:O\t\n",
      "고\t:O\t\n",
      " \t:O\t\n",
      "가\t:O\t\n",
      "고\t:O\t\n",
      "싶\t:O\t\n",
      "었\t:O\t\n",
      "던\t:O\t\n",
      "곳\t:O\t\n",
      "은\t:O\t\n",
      " \t:O\t\n",
      "보\t:B-LC\t\n",
      "라\t:I-LC\t\n",
      "카\t:I-LC\t\n",
      "이\t:O\t\n",
      "였\t:O\t\n",
      "는\t:O\t\n",
      "데\t:O\t\n",
      " \t:O\t\n",
      "점\t:B-PR\t\n",
      "점\t:O\t\n",
      " \t:O\t\n",
      "여\t:O\t\n",
      "행\t:O\t\n",
      "지\t:O\t\n",
      "가\t:O\t\n",
      " \t:O\t\n",
      "추\t:O\t\n",
      "가\t:O\t\n",
      "되\t:O\t\n",
      "다\t:O\t\n",
      "보\t:O\t\n",
      "니\t:O\t\n",
      "\n",
      "\t:O\t\n",
      "세\t:B-LC\t\n",
      "부\t:I-LC\t\n",
      ",\t:O\t\n",
      "방\t:B-LC\t\n",
      "콕\t:I-LC\t\n",
      ",\t:O\t\n",
      "대\t:B-LC\t\n",
      "만\t:I-LC\t\n",
      ",\t:O\t\n",
      "다\t:O\t\n",
      "낭\t:I-LC\t\n",
      "까\t:O\t\n",
      "지\t:O\t\n",
      " \t:O\t\n",
      "더\t:O\t\n",
      "해\t:O\t\n",
      "졌\t:O\t\n",
      "네\t:O\t\n",
      "요\t:O\t\n",
      " \t:O\t\n",
      "대\t:B-LC\t\n",
      "만\t:I-LC\t\n",
      " \t:O\t\n",
      "먹\t:B-PU\t\n",
      "방\t:I-LC\t\n",
      " \t:O\t\n",
      "여\t:O\t\n",
      "행\t:O\t\n",
      "도\t:O\t\n",
      " \t:O\t\n",
      "괜\t:O\t\n",
      "찮\t:O\t\n",
      "아\t:O\t\n",
      "보\t:O\t\n",
      "이\t:O\t\n",
      "고\t:O\t\n",
      ",\t:O\t\n",
      "\n",
      "\t:O\t\n",
      "요\t:O\t\n",
      "즘\t:O\t\n",
      "은\t:O\t\n",
      " \t:O\t\n",
      "다\t:B-PU\t\n",
      "낭\t:I-LC\t\n",
      "도\t:O\t\n",
      " \t:O\t\n",
      "많\t:O\t\n",
      "이\t:O\t\n",
      " \t:O\t\n",
      "가\t:O\t\n",
      "는\t:O\t\n",
      "것\t:O\t\n",
      "같\t:O\t\n",
      "구\t:O\t\n",
      "요\t:O\t\n",
      "검\t:O\t\n",
      "색\t:O\t\n",
      "해\t:O\t\n",
      "보\t:O\t\n",
      "면\t:O\t\n",
      " \t:O\t\n",
      "다\t:O\t\n",
      " \t:O\t\n",
      "너\t:O\t\n",
      "무\t:O\t\n",
      " \t:O\t\n",
      "좋\t:O\t\n",
      "아\t:O\t\n",
      "보\t:O\t\n",
      "이\t:O\t\n",
      "고\t:O\t\n",
      " \t:O\t\n",
      "다\t:O\t\n",
      " \t:O\t\n",
      "가\t:O\t\n",
      "고\t:O\t\n",
      "싶\t:O\t\n",
      "더\t:O\t\n",
      "라\t:O\t\n",
      "구\t:O\t\n",
      "요\t:O\t\n",
      "하\t:O\t\n",
      "지\t:O\t\n",
      "만\t:O\t\n",
      " \t:O\t\n",
      "한\t:O\t\n",
      "군\t:O\t\n",
      "데\t:O\t\n",
      "만\t:O\t\n",
      " \t:O\t\n",
      "택\t:O\t\n",
      "해\t:O\t\n",
      "야\t:O\t\n",
      "하\t:O\t\n",
      "는\t:O\t\n",
      " \t:O\t\n",
      "현\t:O\t\n",
      "실\t:O\t\n",
      "\n",
      "\t:O\t\n",
      "저\t:O\t\n",
      "희\t:O\t\n",
      "는\t:O\t\n",
      " \t:O\t\n",
      "적\t:O\t\n",
      "당\t:O\t\n",
      "한\t:O\t\n",
      " \t:O\t\n",
      "관\t:B-PU\t\n",
      "광\t:I-PU\t\n",
      "+\t:O\t\n",
      "휴\t:B-PU\t\n",
      "양\t:I-PU\t\n",
      " \t:O\t\n",
      "(\t:O\t\n",
      "마\t:B-PU\t\n",
      "사\t:I-PU\t\n",
      "지\t:I-PU\t\n",
      ")\t:O\t\n",
      "를\t:O\t\n",
      " \t:O\t\n",
      "원\t:O\t\n",
      "해\t:O\t\n",
      "요\t:O\t\n",
      "직\t:O\t\n",
      "장\t:O\t\n",
      "생\t:O\t\n",
      "활\t:O\t\n",
      "에\t:O\t\n",
      " \t:O\t\n",
      "찌\t:O\t\n",
      "들\t:O\t\n",
      "린\t:O\t\n",
      " \t:O\t\n",
      "몸\t:O\t\n",
      ",\t:O\t\n",
      " \t:O\t\n",
      "육\t:O\t\n",
      "아\t:O\t\n",
      "에\t:O\t\n",
      " \t:O\t\n",
      "찌\t:O\t\n",
      "들\t:O\t\n",
      "린\t:O\t\n",
      " \t:O\t\n",
      "정\t:O\t\n",
      "신\t:O\t\n",
      "을\t:O\t\n",
      " \t:O\t\n",
      "잠\t:O\t\n",
      "시\t:O\t\n",
      "나\t:O\t\n",
      "마\t:O\t\n",
      " \t:O\t\n",
      "힐\t:B-PU\t\n",
      "링\t:I-PU\t\n",
      "하\t:O\t\n",
      "고\t:O\t\n",
      "싶\t:O\t\n",
      "어\t:O\t\n",
      "\n",
      "\t:O\t\n",
      "떠\t:O\t\n",
      "나\t:O\t\n",
      "는\t:O\t\n",
      "거\t:O\t\n",
      "거\t:O\t\n",
      "든\t:O\t\n",
      "요\t:O\t\n",
      "물\t:O\t\n",
      "론\t:O\t\n",
      " \t:O\t\n",
      "아\t:O\t\n",
      "기\t:O\t\n",
      "는\t:O\t\n",
      " \t:O\t\n",
      "맡\t:O\t\n",
      "기\t:O\t\n",
      "고\t:O\t\n",
      " \t:O\t\n",
      "어\t:B-AC\t\n",
      "른\t:I-AC\t\n",
      "셋\t:I-AC\t\n",
      "만\t:O\t\n",
      " \t:O\t\n",
      "간\t:O\t\n",
      "답\t:O\t\n",
      "니\t:O\t\n",
      "당\t:O\t\n",
      "상\t:O\t\n",
      "세\t:O\t\n",
      "하\t:O\t\n",
      "게\t:O\t\n",
      " \t:O\t\n",
      "잘\t:O\t\n",
      " \t:O\t\n",
      "아\t:O\t\n",
      "시\t:O\t\n",
      "는\t:O\t\n",
      "분\t:O\t\n",
      "들\t:O\t\n",
      ".\t:O\t\n",
      ".\t:O\t\n",
      ".\t:O\t\n",
      " \t:O\t\n",
      "저\t:O\t\n",
      " \t:O\t\n",
      "나\t:O\t\n",
      "라\t:O\t\n",
      "들\t:O\t\n",
      " \t:O\t\n",
      "설\t:O\t\n",
      "명\t:O\t\n",
      "좀\t:O\t\n",
      "해\t:O\t\n",
      "주\t:O\t\n",
      "세\t:O\t\n",
      "요\t:O\t\n",
      "\n",
      "\t:O\t\n",
      "연\t:I-PU\t\n",
      "로\t:O\t\n",
      "하\t:O\t\n",
      "신\t:O\t\n",
      " \t:O\t\n",
      "어\t:B-AC\t\n",
      "머\t:I-AC\t\n",
      "니\t:I-AC\t\n",
      "와\t:O\t\n",
      " \t:O\t\n",
      "8\t:O\t\n",
      "살\t:O\t\n",
      " \t:O\t\n",
      "6\t:O\t\n",
      "살\t:O\t\n",
      " \t:O\t\n",
      "아\t:B-AC\t\n",
      "이\t:I-AC\t\n",
      "들\t:O\t\n",
      " \t:O\t\n",
      "가\t:B-AC\t\n",
      "족\t:I-AC\t\n",
      " \t:O\t\n",
      "해\t:O\t\n",
      "외\t:O\t\n",
      " \t:O\t\n",
      "여\t:O\t\n",
      "행\t:O\t\n",
      " \t:O\t\n",
      "계\t:O\t\n",
      "획\t:O\t\n",
      "중\t:O\t\n",
      " \t:O\t\n",
      "입\t:O\t\n",
      "니\t:O\t\n",
      "다\t:O\t\n",
      "\n",
      "\t:O\t\n",
      "3\t:B-PR\t\n",
      "박\t:I-PR\t\n",
      " \t:I-PR\t\n",
      "4\t:I-PR\t\n",
      "일\t:I-PR\t\n",
      " \t:O\t\n",
      "예\t:O\t\n",
      "정\t:O\t\n",
      " \t:O\t\n",
      "인\t:O\t\n",
      "데\t:O\t\n",
      " \t:O\t\n",
      "추\t:O\t\n",
      "천\t:O\t\n",
      " \t:O\t\n",
      "부\t:O\t\n",
      "탁\t:O\t\n",
      "드\t:O\t\n",
      "립\t:O\t\n",
      "니\t:O\t\n",
      "다\t:O\t\n",
      "어\t:B-AC\t\n",
      "머\t:I-AC\t\n",
      "니\t:I-AC\t\n",
      "와\t:O\t\n",
      " \t:O\t\n",
      "아\t:B-AC\t\n",
      "이\t:I-AC\t\n",
      "들\t:O\t\n",
      " \t:O\t\n",
      "때\t:O\t\n",
      "문\t:O\t\n",
      "에\t:O\t\n",
      " \t:O\t\n",
      "어\t:O\t\n",
      "디\t:O\t\n",
      "로\t:O\t\n",
      " \t:O\t\n",
      "갈\t:O\t\n",
      "까\t:O\t\n",
      " \t:O\t\n",
      "고\t:O\t\n",
      "민\t:O\t\n",
      " \t:O\t\n",
      "중\t:O\t\n",
      " \t:O\t\n",
      "입\t:O\t\n",
      "니\t:O\t\n",
      "다\t:O\t\n",
      "\n",
      "\t:O\t\n",
      "제\t:O\t\n",
      "가\t:O\t\n",
      " \t:O\t\n",
      "물\t:B-PU\t\n",
      "놀\t:I-PU\t\n",
      "이\t:I-PU\t\n",
      "하\t:O\t\n",
      "고\t:O\t\n",
      " \t:O\t\n",
      "에\t:O\t\n",
      "어\t:O\t\n",
      "텔\t:O\t\n",
      "로\t:O\t\n",
      " \t:O\t\n",
      "놀\t:O\t\n",
      "러\t:O\t\n",
      "가\t:O\t\n",
      "는\t:O\t\n",
      "걸\t:O\t\n",
      " \t:O\t\n",
      "좋\t:O\t\n",
      "아\t:O\t\n",
      "합\t:O\t\n",
      "니\t:O\t\n",
      "다\t:O\t\n",
      ".\t:O\t\n",
      " \t:O\t\n",
      "랑\t:O\t\n",
      " \t:O\t\n",
      "사\t:B-LC\t\n",
      "이\t:I-LC\t\n",
      "판\t:I-LC\t\n",
      "은\t:O\t\n",
      " \t:O\t\n",
      "갔\t:O\t\n",
      "다\t:O\t\n",
      "왔\t:O\t\n",
      "는\t:O\t\n",
      "데\t:O\t\n",
      " \t:O\t\n",
      "해\t:O\t\n",
      "외\t:O\t\n",
      "휴\t:O\t\n",
      "양\t:O\t\n",
      "지\t:O\t\n",
      "로\t:O\t\n",
      " \t:O\t\n",
      "3\t:B-PR\t\n",
      "박\t:I-PR\t\n",
      "4\t:I-PR\t\n",
      "일\t:I-PR\t\n",
      " \t:O\t\n",
      ",\t:O\t\n",
      " \t:O\t\n",
      "4\t:B-PR\t\n",
      "박\t:I-PR\t\n",
      "5\t:I-PR\t\n",
      "일\t:I-PR\t\n",
      "정\t:O\t\n",
      "도\t:O\t\n",
      " \t:O\t\n",
      "떠\t:O\t\n",
      "나\t:O\t\n",
      "려\t:O\t\n",
      "하\t:O\t\n",
      "는\t:O\t\n",
      "데\t:O\t\n",
      " \t:O\t\n",
      "\n",
      "\t:O\t\n",
      "휴\t:B-PU\t\n",
      "양\t:I-PU\t\n",
      "지\t:O\t\n",
      "로\t:O\t\n",
      " \t:O\t\n",
      "호\t:O\t\n",
      "텔\t:O\t\n",
      "수\t:B-PU\t\n",
      "영\t:I-PU\t\n",
      "장\t:I-PU\t\n",
      "에\t:O\t\n",
      "서\t:O\t\n",
      " \t:O\t\n",
      "놀\t:B-PU\t\n",
      "고\t:I-PU\t\n",
      " \t:O\t\n",
      "바\t:B-PU\t\n",
      "다\t:I-PU\t\n",
      "에\t:O\t\n",
      "서\t:O\t\n",
      " \t:O\t\n",
      "놀\t:O\t\n",
      "고\t:O\t\n",
      " \t:O\t\n",
      "그\t:O\t\n",
      "러\t:O\t\n",
      "면\t:O\t\n",
      "서\t:O\t\n",
      " \t:O\t\n",
      "보\t:O\t\n",
      "낼\t:O\t\n",
      " \t:O\t\n",
      "더\t:B-WT\t\n",
      "운\t:I-WT\t\n",
      " \t:O\t\n",
      "나\t:O\t\n",
      "라\t:O\t\n",
      " \t:O\t\n",
      "좀\t:O\t\n",
      " \t:O\t\n",
      "추\t:O\t\n",
      "천\t:O\t\n",
      "해\t:O\t\n",
      "주\t:O\t\n",
      "세\t:O\t\n",
      "요\t:O\t\n",
      "!\t:O\t\n",
      "\n",
      "\t:O\t\n",
      "안\t:O\t\n",
      "녕\t:O\t\n",
      "하\t:O\t\n",
      "세\t:O\t\n",
      "요\t:O\t\n",
      " \t:O\t\n",
      "~\t:O\t\n",
      " \t:O\t\n",
      "이\t:O\t\n",
      "번\t:O\t\n",
      "년\t:O\t\n",
      "도\t:O\t\n",
      "가\t:O\t\n",
      " \t:O\t\n",
      "끝\t:O\t\n",
      "나\t:O\t\n",
      "려\t:O\t\n",
      "면\t:O\t\n",
      " \t:O\t\n",
      "얼\t:O\t\n",
      "마\t:O\t\n",
      " \t:O\t\n",
      "남\t:O\t\n",
      "지\t:O\t\n",
      "않\t:O\t\n",
      "아\t:O\t\n",
      "서\t:O\t\n",
      " \t:O\t\n",
      "저\t:O\t\n",
      "포\t:O\t\n",
      "함\t:O\t\n",
      "해\t:O\t\n",
      "서\t:O\t\n",
      " \t:O\t\n",
      "친\t:B-AC\t\n",
      "구\t:I-AC\t\n",
      "와\t:O\t\n",
      " \t:O\t\n",
      "두\t:O\t\n",
      "명\t:O\t\n",
      "이\t:O\t\n",
      "서\t:O\t\n",
      " \t:O\t\n",
      "돈\t:O\t\n",
      "모\t:O\t\n",
      "아\t:O\t\n",
      " \t:O\t\n",
      "해\t:O\t\n",
      "외\t:O\t\n",
      "여\t:O\t\n",
      "행\t:O\t\n",
      "을\t:O\t\n",
      "가\t:O\t\n",
      "려\t:O\t\n",
      "고\t:O\t\n",
      " \t:O\t\n",
      "합\t:O\t\n",
      "니\t:O\t\n",
      "다\t:O\t\n",
      ".\t:O\t\n",
      " \t:O\t\n",
      "\n",
      "\t:O\t\n",
      "넉\t:O\t\n",
      "넉\t:O\t\n",
      "한\t:O\t\n",
      " \t:O\t\n",
      "형\t:B-AC\t\n",
      "편\t:I-AC\t\n",
      "이\t:O\t\n",
      " \t:O\t\n",
      "아\t:O\t\n",
      "닌\t:O\t\n",
      " \t:O\t\n",
      "학\t:O\t\n",
      "생\t:O\t\n",
      "이\t:O\t\n",
      "라\t:O\t\n",
      "서\t:O\t\n",
      " \t:O\t\n",
      "저\t:O\t\n",
      "렴\t:O\t\n",
      "하\t:O\t\n",
      "면\t:O\t\n",
      "서\t:O\t\n",
      " \t:O\t\n",
      "구\t:B-PU\t\n",
      "경\t:I-PU\t\n",
      "하\t:O\t\n",
      "기\t:O\t\n",
      "좋\t:O\t\n",
      "고\t:O\t\n",
      " \t:O\t\n",
      "물\t:B-PU\t\n",
      "가\t:I-PU\t\n",
      "도\t:O\t\n",
      " \t:O\t\n",
      "괜\t:O\t\n",
      "찮\t:O\t\n",
      "은\t:O\t\n",
      "곳\t:O\t\n",
      "을\t:O\t\n",
      " \t:O\t\n",
      "알\t:O\t\n",
      "아\t:O\t\n",
      "보\t:O\t\n",
      "고\t:O\t\n",
      " \t:O\t\n",
      "있\t:O\t\n",
      "거\t:O\t\n",
      "든\t:O\t\n",
      "요\t:O\t\n",
      ".\t:O\t\n",
      " \t:O\t\n",
      "\n",
      "\t:O\t\n",
      "행\t:O\t\n",
      "비\t:O\t\n",
      "에\t:O\t\n",
      " \t:O\t\n",
      "맞\t:O\t\n",
      "게\t:O\t\n",
      " \t:O\t\n",
      "돈\t:O\t\n",
      "도\t:O\t\n",
      " \t:O\t\n",
      "모\t:O\t\n",
      "으\t:O\t\n",
      "고\t:O\t\n",
      " \t:O\t\n",
      "여\t:O\t\n",
      "행\t:O\t\n",
      "비\t:O\t\n",
      "도\t:O\t\n",
      " \t:O\t\n",
      "모\t:O\t\n",
      "아\t:O\t\n",
      "야\t:O\t\n",
      "해\t:O\t\n",
      "서\t:O\t\n",
      " \t:O\t\n",
      "겨\t:B-DT\t\n",
      "울\t:I-DT\t\n",
      "쯤\t:O\t\n",
      "에\t:O\t\n",
      "가\t:O\t\n",
      "려\t:O\t\n",
      "고\t:O\t\n",
      " \t:O\t\n",
      "하\t:O\t\n",
      "는\t:O\t\n",
      "데\t:O\t\n",
      "요\t:O\t\n",
      " \t:O\t\n",
      "어\t:O\t\n",
      "디\t:O\t\n",
      "가\t:O\t\n",
      " \t:O\t\n",
      "좋\t:O\t\n",
      "을\t:O\t\n",
      "까\t:O\t\n",
      "요\t:O\t\n",
      "?\t:O\t\n",
      " \t:O\t\n",
      "\n",
      "\t:O\t\n",
      "배\t:O\t\n",
      "낭\t:O\t\n",
      "여\t:O\t\n",
      "행\t:O\t\n",
      "보\t:O\t\n",
      "다\t:O\t\n",
      "는\t:O\t\n",
      " \t:O\t\n",
      "패\t:O\t\n",
      "키\t:O\t\n",
      "지\t:O\t\n",
      " \t:O\t\n",
      "여\t:O\t\n",
      "행\t:O\t\n",
      ",\t:O\t\n",
      " \t:O\t\n",
      "가\t:O\t\n",
      "이\t:O\t\n",
      "드\t:O\t\n",
      "가\t:O\t\n",
      " \t:O\t\n",
      "있\t:O\t\n",
      "는\t:O\t\n",
      " \t:O\t\n",
      "여\t:O\t\n",
      "행\t:O\t\n",
      "을\t:O\t\n",
      " \t:O\t\n",
      "더\t:O\t\n",
      " \t:O\t\n",
      "선\t:O\t\n",
      "호\t:O\t\n",
      "해\t:O\t\n",
      "요\t:O\t\n",
      "내\t:O\t\n",
      "공\t:B-PU\t\n",
      "냠\t:O\t\n",
      "냠\t:O\t\n",
      " \t:O\t\n",
      "광\t:I-PU\t\n",
      "고\t:O\t\n",
      "글\t:O\t\n",
      "은\t:O\t\n",
      " \t:O\t\n",
      "신\t:O\t\n",
      "고\t:O\t\n",
      "할\t:O\t\n",
      "께\t:O\t\n",
      "요\t:O\t\n",
      "^\t:O\t\n",
      "_\t:O\t\n",
      "^\t:O\t\n",
      "\n",
      "\t:O\t\n",
      "초\t:B-AC\t\n",
      "등\t:I-AC\t\n",
      "2\t:O\t\n",
      ",\t:O\t\n",
      "4\t:O\t\n",
      "학\t:O\t\n",
      "년\t:O\t\n",
      " \t:O\t\n",
      "아\t:B-AC\t\n",
      "이\t:I-AC\t\n",
      "들\t:O\t\n",
      "과\t:O\t\n",
      " \t:O\t\n",
      "아\t:O\t\n",
      "빠\t:O\t\n",
      " \t:O\t\n",
      "셋\t:O\t\n",
      "이\t:O\t\n",
      "서\t:O\t\n",
      " \t:O\t\n",
      "가\t:B-AC\t\n",
      "족\t:I-AC\t\n",
      " \t:O\t\n",
      "해\t:O\t\n",
      "외\t:O\t\n",
      "여\t:O\t\n",
      "행\t:O\t\n",
      "을\t:O\t\n",
      " \t:O\t\n",
      "하\t:O\t\n",
      "려\t:O\t\n",
      "고\t:O\t\n",
      " \t:O\t\n",
      "합\t:O\t\n",
      "니\t:O\t\n",
      "다\t:O\t\n",
      ".\t:O\t\n",
      "\n",
      "\t:O\t\n",
      "여\t:B-DT\t\n",
      "름\t:I-DT\t\n",
      "에\t:O\t\n",
      " \t:O\t\n",
      "가\t:B-AC\t\n",
      "족\t:I-AC\t\n",
      "끼\t:O\t\n",
      "리\t:O\t\n",
      " \t:O\t\n",
      "처\t:O\t\n",
      "음\t:O\t\n",
      "으\t:O\t\n",
      "로\t:O\t\n",
      " \t:O\t\n",
      "해\t:O\t\n",
      "외\t:O\t\n",
      "여\t:O\t\n",
      "행\t:O\t\n",
      "을\t:O\t\n",
      " \t:O\t\n",
      "가\t:O\t\n",
      "볼\t:O\t\n",
      "까\t:O\t\n",
      " \t:O\t\n",
      "하\t:O\t\n",
      "는\t:O\t\n",
      "데\t:O\t\n",
      "요\t:O\t\n",
      ".\t:O\t\n",
      " \t:O\t\n",
      "\n",
      "\t:O\t\n",
      "지\t:O\t\n",
      "금\t:O\t\n",
      "까\t:O\t\n",
      "지\t:O\t\n",
      " \t:O\t\n",
      "가\t:B-AC\t\n",
      "족\t:I-AC\t\n",
      "들\t:O\t\n",
      "이\t:O\t\n",
      " \t:O\t\n",
      "각\t:O\t\n",
      "자\t:O\t\n",
      " \t:O\t\n",
      "해\t:O\t\n",
      "외\t:O\t\n",
      "에\t:O\t\n",
      "는\t:O\t\n",
      " \t:O\t\n",
      "몇\t:O\t\n",
      " \t:O\t\n",
      "번\t:O\t\n",
      " \t:O\t\n",
      "갔\t:O\t\n",
      "다\t:O\t\n",
      " \t:O\t\n",
      "왔\t:O\t\n",
      "지\t:O\t\n",
      "만\t:O\t\n",
      " \t:O\t\n",
      "온\t:O\t\n",
      " \t:O\t\n",
      "가\t:B-AC\t\n",
      "족\t:O\t\n",
      "이\t:O\t\n",
      " \t:O\t\n",
      "해\t:O\t\n",
      "외\t:O\t\n",
      "로\t:O\t\n",
      " \t:O\t\n",
      "여\t:O\t\n",
      "행\t:O\t\n",
      "가\t:O\t\n",
      "는\t:O\t\n",
      "게\t:O\t\n",
      " \t:O\t\n",
      "처\t:O\t\n",
      "음\t:O\t\n",
      "이\t:O\t\n",
      "라\t:O\t\n",
      "서\t:O\t\n",
      " \t:O\t\n",
      "고\t:O\t\n",
      "수\t:O\t\n",
      "님\t:O\t\n",
      "들\t:O\t\n",
      " \t:O\t\n",
      "도\t:B-PR\t\n",
      "움\t:O\t\n",
      "을\t:O\t\n",
      " \t:O\t\n",
      "좀\t:O\t\n",
      " \t:O\t\n",
      "얻\t:O\t\n",
      "어\t:O\t\n",
      "보\t:O\t\n",
      "려\t:O\t\n",
      "고\t:O\t\n",
      " \t:O\t\n",
      "합\t:O\t\n",
      "니\t:O\t\n",
      "다\t:O\t\n",
      ".\t:O\t\n",
      " \t:O\t\n",
      "\n",
      "\t:O\t\n",
      "몇\t:O\t\n",
      " \t:O\t\n",
      "가\t:O\t\n",
      "지\t:O\t\n",
      " \t:O\t\n",
      "주\t:O\t\n",
      "요\t:O\t\n",
      "하\t:O\t\n",
      "게\t:O\t\n",
      " \t:O\t\n",
      "들\t:O\t\n",
      "어\t:O\t\n",
      "갔\t:O\t\n",
      "으\t:O\t\n",
      "면\t:O\t\n",
      " \t:O\t\n",
      "하\t:O\t\n",
      "는\t:O\t\n",
      " \t:O\t\n",
      "것\t:O\t\n",
      "들\t:O\t\n",
      " \t:O\t\n",
      "적\t:O\t\n",
      "어\t:O\t\n",
      "드\t:O\t\n",
      "리\t:O\t\n",
      "니\t:O\t\n",
      " \t:O\t\n",
      "참\t:O\t\n",
      "고\t:O\t\n",
      "하\t:O\t\n",
      "셔\t:O\t\n",
      "서\t:O\t\n",
      " \t:O\t\n",
      "답\t:O\t\n",
      "변\t:O\t\n",
      " \t:O\t\n",
      "부\t:O\t\n",
      "탁\t:O\t\n",
      "드\t:O\t\n",
      "립\t:O\t\n",
      "니\t:O\t\n",
      "다\t:O\t\n",
      "!\t:O\t\n",
      "\n",
      "\t:O\t\n",
      "1\t:O\t\n",
      ".\t:O\t\n",
      " \t:O\t\n",
      "항\t:B-PU\t\n",
      "공\t:I-PU\t\n",
      "편\t:O\t\n",
      "은\t:O\t\n",
      " \t:O\t\n",
      "되\t:O\t\n",
      "도\t:O\t\n",
      "록\t:O\t\n",
      " \t:O\t\n",
      "1\t:O\t\n",
      "5\t:O\t\n",
      "0\t:O\t\n",
      "에\t:O\t\n",
      "서\t:O\t\n",
      " \t:O\t\n",
      "2\t:O\t\n",
      "0\t:O\t\n",
      "0\t:O\t\n",
      " \t:O\t\n",
      "사\t:O\t\n",
      "이\t:O\t\n",
      "에\t:O\t\n",
      "서\t:O\t\n",
      " \t:O\t\n",
      "구\t:O\t\n",
      "했\t:O\t\n",
      "으\t:O\t\n",
      "면\t:O\t\n",
      " \t:O\t\n",
      "합\t:O\t\n",
      "니\t:O\t\n",
      "다\t:O\t\n",
      ".\t:O\t\n",
      " \t:O\t\n",
      "(\t:O\t\n",
      "그\t:O\t\n",
      "래\t:O\t\n",
      "서\t:O\t\n",
      " \t:O\t\n",
      "가\t:O\t\n",
      "까\t:O\t\n",
      "운\t:O\t\n",
      " \t:O\t\n",
      "일\t:B-LC\t\n",
      "본\t:I-LC\t\n",
      "이\t:O\t\n",
      "나\t:O\t\n",
      " \t:O\t\n",
      "동\t:B-LC\t\n",
      "남\t:I-LC\t\n",
      "아\t:I-LC\t\n",
      "로\t:O\t\n",
      " \t:O\t\n",
      "찾\t:O\t\n",
      "아\t:O\t\n",
      "보\t:O\t\n",
      "고\t:O\t\n",
      "는\t:O\t\n",
      " \t:O\t\n",
      "있\t:O\t\n",
      "었\t:O\t\n",
      "습\t:O\t\n",
      "니\t:O\t\n",
      "다\t:O\t\n",
      ".\t:O\t\n",
      ")\t:O\t\n",
      "\n",
      "\t:O\t\n",
      "2\t:O\t\n",
      ".\t:O\t\n",
      " \t:O\t\n",
      "부\t:O\t\n",
      "모\t:O\t\n",
      "님\t:O\t\n",
      "은\t:O\t\n",
      " \t:O\t\n",
      "하\t:O\t\n",
      "루\t:O\t\n",
      " \t:O\t\n",
      "정\t:O\t\n",
      "도\t:O\t\n",
      "는\t:O\t\n",
      " \t:O\t\n",
      "골\t:B-PU\t\n",
      "프\t:I-PU\t\n",
      " \t:O\t\n",
      "라\t:O\t\n",
      "운\t:O\t\n",
      "딩\t:O\t\n",
      "을\t:O\t\n",
      " \t:O\t\n",
      "할\t:O\t\n",
      " \t:O\t\n",
      "수\t:O\t\n",
      " \t:O\t\n",
      "있\t:O\t\n",
      "었\t:O\t\n",
      "으\t:O\t\n",
      "면\t:O\t\n",
      " \t:O\t\n",
      "좋\t:O\t\n",
      "겠\t:O\t\n",
      "다\t:O\t\n",
      "고\t:O\t\n",
      " \t:O\t\n",
      "하\t:O\t\n",
      "십\t:O\t\n",
      "니\t:O\t\n",
      "다\t:O\t\n",
      ".\t:O\t\n",
      "\n",
      "\t:O\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\t:O\t\n",
      ".\t:O\t\n",
      " \t:O\t\n",
      "맛\t:O\t\n",
      "있\t:O\t\n",
      "는\t:O\t\n",
      " \t:O\t\n",
      "음\t:I-PU\t\n",
      "식\t:I-PU\t\n",
      "을\t:O\t\n",
      " \t:O\t\n",
      "많\t:O\t\n",
      "이\t:O\t\n",
      " \t:O\t\n",
      "먹\t:O\t\n",
      "을\t:O\t\n",
      " \t:O\t\n",
      "수\t:O\t\n",
      " \t:O\t\n",
      "있\t:O\t\n",
      "는\t:O\t\n",
      " \t:O\t\n",
      "곳\t:O\t\n",
      "이\t:O\t\n",
      "면\t:O\t\n",
      " \t:O\t\n",
      "좋\t:O\t\n",
      "겠\t:O\t\n",
      "습\t:O\t\n",
      "니\t:O\t\n",
      "다\t:O\t\n",
      ".\t:O\t\n",
      "많\t:O\t\n",
      "은\t:O\t\n",
      " \t:O\t\n",
      "답\t:O\t\n",
      "변\t:O\t\n",
      " \t:O\t\n",
      "부\t:O\t\n",
      "탁\t:O\t\n",
      "드\t:O\t\n",
      "립\t:O\t\n",
      "니\t:O\t\n",
      "다\t:O\t\n",
      "~\t:O\t\n",
      "!\t:O\t\n",
      "!\t:O\t\n",
      "!\t:O\t\n",
      "내\t:O\t\n",
      "공\t:O\t\n",
      " \t:O\t\n",
      "많\t:O\t\n",
      "이\t:O\t\n",
      " \t:O\t\n",
      "많\t:O\t\n",
      "이\t:O\t\n",
      " \t:O\t\n",
      "드\t:O\t\n",
      "릴\t:O\t\n",
      "께\t:O\t\n",
      "요\t:O\t\n",
      "~\t:O\t\n",
      "!\t:O\t\n",
      "!\t:O\t\n",
      "고\t:O\t\n",
      "수\t:O\t\n",
      "님\t:O\t\n",
      "들\t:O\t\n",
      " \t:O\t\n",
      "부\t:O\t\n",
      "탁\t:O\t\n",
      "드\t:O\t\n",
      "려\t:O\t\n",
      "요\t:O\t\n",
      "!\t:O\t\n",
      "!\t:O\t\n",
      "\n",
      "\t:O\t\n",
      "부\t:B-AC\t\n",
      "모\t:I-AC\t\n",
      "님\t:I-AC\t\n",
      "이\t:O\t\n",
      "랑\t:O\t\n",
      " \t:O\t\n",
      "2\t:O\t\n",
      "0\t:O\t\n",
      "대\t:O\t\n",
      " \t:O\t\n",
      "자\t:O\t\n",
      "매\t:O\t\n",
      " \t:O\t\n",
      "둘\t:O\t\n",
      " \t:O\t\n",
      "이\t:O\t\n",
      "렇\t:O\t\n",
      "게\t:O\t\n",
      " \t:O\t\n",
      "넷\t:O\t\n",
      "이\t:O\t\n",
      "서\t:O\t\n",
      " \t:O\t\n",
      "해\t:O\t\n",
      "외\t:O\t\n",
      "여\t:O\t\n",
      "행\t:O\t\n",
      "을\t:O\t\n",
      " \t:O\t\n",
      "가\t:O\t\n",
      "려\t:O\t\n",
      "고\t:O\t\n",
      "합\t:O\t\n",
      "니\t:O\t\n",
      "다\t:O\t\n",
      ".\t:O\t\n",
      "9\t:B-DT\t\n",
      "월\t:I-DT\t\n",
      "에\t:O\t\n",
      " \t:O\t\n",
      "갈\t:O\t\n",
      "껀\t:O\t\n",
      "데\t:O\t\n",
      " \t:O\t\n",
      "세\t:B-LC\t\n",
      "부\t:I-LC\t\n",
      " \t:O\t\n",
      "코\t:B-LC\t\n",
      "타\t:I-LC\t\n",
      " \t:O\t\n",
      "다\t:B-LC\t\n",
      "낭\t:I-LC\t\n",
      " \t:O\t\n",
      "중\t:O\t\n",
      "에\t:O\t\n",
      " \t:O\t\n",
      "골\t:O\t\n",
      "라\t:O\t\n",
      "주\t:O\t\n",
      "세\t:O\t\n",
      "요\t:O\t\n",
      "!\t:O\t\n",
      " \t:O\t\n",
      "\n",
      "\t:O\t\n",
      "부\t:B-AC\t\n",
      "모\t:I-AC\t\n",
      "님\t:I-AC\t\n",
      "은\t:O\t\n",
      " \t:O\t\n",
      "많\t:O\t\n",
      "이\t:O\t\n",
      " \t:O\t\n",
      "걸\t:O\t\n",
      "어\t:O\t\n",
      "다\t:O\t\n",
      "니\t:O\t\n",
      "시\t:O\t\n",
      "는\t:O\t\n",
      "걸\t:O\t\n",
      " \t:O\t\n",
      "별\t:O\t\n",
      "로\t:O\t\n",
      " \t:O\t\n",
      "안\t:O\t\n",
      "좋\t:O\t\n",
      "아\t:O\t\n",
      "하\t:O\t\n",
      "시\t:O\t\n",
      "구\t:O\t\n",
      "요\t:O\t\n",
      ",\t:O\t\n",
      " \t:O\t\n",
      "힐\t:B-PU\t\n",
      "링\t:I-PU\t\n",
      "을\t:O\t\n",
      " \t:O\t\n",
      "원\t:O\t\n",
      "하\t:O\t\n",
      "십\t:O\t\n",
      "니\t:O\t\n",
      "다\t:O\t\n",
      "!\t:O\t\n",
      "기\t:O\t\n",
      "간\t:O\t\n",
      "은\t:O\t\n",
      " \t:O\t\n",
      "3\t:B-PR\t\n",
      "박\t:I-PR\t\n",
      "4\t:I-PR\t\n",
      "일\t:I-PR\t\n",
      "로\t:O\t\n",
      " \t:O\t\n",
      "생\t:O\t\n",
      "각\t:O\t\n",
      "하\t:O\t\n",
      "고\t:O\t\n",
      " \t:O\t\n",
      "있\t:O\t\n",
      "어\t:O\t\n",
      "요\t:O\t\n",
      "~\t:O\t\n",
      "\n",
      "\t:O\t\n",
      "일\t:O\t\n",
      "본\t:O\t\n",
      "으\t:O\t\n",
      "로\t:O\t\n",
      " \t:O\t\n",
      "1\t:B-PR\t\n",
      "년\t:I-PR\t\n",
      " \t:O\t\n",
      "유\t:O\t\n",
      "학\t:O\t\n",
      "을\t:O\t\n",
      " \t:O\t\n",
      "가\t:O\t\n",
      "게\t:O\t\n",
      " \t:O\t\n",
      "되\t:O\t\n",
      "었\t:O\t\n",
      "는\t:O\t\n",
      "데\t:O\t\n",
      " \t:O\t\n",
      "여\t:O\t\n",
      "행\t:O\t\n",
      "자\t:O\t\n",
      " \t:O\t\n",
      "보\t:O\t\n",
      "험\t:O\t\n",
      " \t:O\t\n",
      "추\t:O\t\n",
      "천\t:O\t\n",
      "해\t:O\t\n",
      "주\t:O\t\n",
      "세\t:O\t\n",
      "요\t:O\t\n",
      "~\t:O\t\n",
      "~\t:O\t\n",
      "\n",
      "\t:O\t\n",
      "6\t:B-DT\t\n",
      "월\t:I-DT\t\n",
      " \t:I-DT\t\n",
      "1\t:I-DT\t\n",
      "7\t:I-DT\t\n",
      "일\t:I-DT\t\n",
      " \t:O\t\n",
      "출\t:O\t\n",
      "발\t:O\t\n",
      ",\t:O\t\n",
      " \t:O\t\n",
      "2\t:O\t\n",
      "0\t:O\t\n",
      "일\t:O\t\n",
      "(\t:O\t\n",
      "2\t:O\t\n",
      "1\t:O\t\n",
      "일\t:O\t\n",
      ")\t:O\t\n",
      " \t:O\t\n",
      "귀\t:O\t\n",
      "국\t:O\t\n",
      "하\t:O\t\n",
      "는\t:O\t\n",
      " \t:O\t\n",
      "일\t:O\t\n",
      "정\t:O\t\n",
      "의\t:O\t\n",
      " \t:O\t\n",
      "가\t:B-AC\t\n",
      "족\t:I-AC\t\n",
      " \t:O\t\n",
      "해\t:O\t\n",
      "외\t:O\t\n",
      " \t:O\t\n",
      "여\t:O\t\n",
      "행\t:O\t\n",
      "을\t:O\t\n",
      " \t:O\t\n",
      "준\t:O\t\n",
      "비\t:O\t\n",
      "중\t:O\t\n",
      "입\t:O\t\n",
      "니\t:O\t\n",
      "다\t:O\t\n",
      ".\t:O\t\n",
      " \t:O\t\n",
      "\n",
      "\t:O\t\n",
      "장\t:O\t\n",
      "인\t:O\t\n",
      " \t:O\t\n",
      "칠\t:O\t\n",
      "순\t:O\t\n",
      "을\t:O\t\n",
      " \t:O\t\n",
      "맞\t:O\t\n",
      "이\t:O\t\n",
      "하\t:O\t\n",
      "여\t:O\t\n",
      ",\t:O\t\n",
      " \t:O\t\n",
      "성\t:B-AC\t\n",
      "인\t:I-AC\t\n",
      " \t:I-AC\t\n",
      "9\t:I-AC\t\n",
      "명\t:I-AC\t\n",
      ",\t:O\t\n",
      " \t:O\t\n",
      "4\t:B-DT\t\n",
      "3\t:I-DT\t\n",
      "개\t:O\t\n",
      "월\t:O\t\n",
      " \t:O\t\n",
      "유\t:B-AC\t\n",
      "아\t:I-AC\t\n",
      " \t:O\t\n",
      "1\t:O\t\n",
      "명\t:O\t\n",
      "입\t:O\t\n",
      "니\t:O\t\n",
      "다\t:O\t\n",
      ".\t:O\t\n",
      " \t:O\t\n",
      "\n",
      "\t:O\t\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-5d84fd244dee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m             \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_vocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_vocab\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-d174b5fa6746>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(token_vocab, target_vocab, sent)\u001b[0m\n\u001b[0;32m    237\u001b[0m     \u001b[0mpred_data_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNERDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_id_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m85\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m     \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 239\u001b[1;33m     \u001b[0ma_batch_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_data_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_iterator\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# a result\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    240\u001b[0m     \u001b[0mb_nes_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_token_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma_batch_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    with open('./data/result4.txt',encoding='utf8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            predict(token_vocab, target_vocab,line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
