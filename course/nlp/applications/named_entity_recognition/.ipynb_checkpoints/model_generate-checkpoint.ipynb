{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version :  1.10.0\n",
      "<common.nlp.data_loader.N2NTextData object at 0x000001E14E336748>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Named Entity Recognition \n",
    "\n",
    "        Author : Sangkeun Jung (2017)\n",
    "        - using Tensorflow\n",
    "\"\"\"\n",
    "\n",
    "import sys, os, inspect\n",
    "\n",
    "# add common to path\n",
    "from pathlib import Path\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "common_path = str(Path(currentdir).parent.parent)\n",
    "sys.path.append( common_path )\n",
    "\n",
    "from common.nlp.vocab import Vocab\n",
    "from common.nlp.data_loader import N2NTextData\n",
    "from common.nlp.converter import N2NConverter\n",
    "\n",
    "from dataset import NERDataset\n",
    "from dataset import load_data\n",
    "from common.ml.hparams import HParams\n",
    "\n",
    "import numpy as np\n",
    "import copy \n",
    "import time \n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn\n",
    "from tensorflow.contrib.layers.python.layers import linear\n",
    "from tensorflow.python.ops import variable_scope\n",
    "from tensorflow.contrib.seq2seq import sequence_loss#sequence의 loss의 평균값을 구할 수있음\n",
    "\n",
    "from common.ml.tf.deploy import freeze_graph\n",
    "\n",
    "\n",
    "\n",
    "print( \"Tensorflow Version : \", tf.__version__)\n",
    "\n",
    "class NER():\n",
    "    def __init__(self, hps, mode=\"train\"):\n",
    "        self.hps = hps\n",
    "        self.x = tf.placeholder(tf.int32,   [None, hps.num_steps], name=\"pl_tokens\")\n",
    "        self.y = tf.placeholder(tf.int32,   [None, hps.num_steps], name=\"pl_target\")\n",
    "        self.w = tf.placeholder(tf.float32, [None, hps.num_steps], name=\"pl_weight\")\n",
    "        self.keep_prob = tf.placeholder(tf.float32, [], name=\"pl_keep_prob\")\n",
    "\n",
    "        ### 4 blocks ###\n",
    "        # 1) embedding\n",
    "        # 2) dropout on input embedding\n",
    "        # 3) sentence encoding using rnn\n",
    "        # 4) bidirectional rnn's output to target classes\n",
    "        # 5) loss calcaulation\n",
    "\n",
    "        def _embedding(x):\n",
    "            # character embedding \n",
    "            shape       = [hps.vocab_size, hps.emb_size]\n",
    "            initializer = tf.initializers.variance_scaling(distribution=\"uniform\", dtype=tf.float32)\n",
    "            emb_mat     = tf.get_variable(\"emb\", shape, initializer=initializer, dtype=tf.float32)\n",
    "            input_emb   = tf.nn.embedding_lookup(emb_mat, x)   # [batch_size, sent_len, emb_dim]\n",
    "\n",
    "            # split input_emb -> num_steps\n",
    "            step_inputs = tf.unstack(input_emb, axis=1)\n",
    "            return step_inputs\n",
    "\n",
    "        def _sequence_dropout(step_inputs, keep_prob):\n",
    "            # apply dropout to each input\n",
    "            # input : a list of input tensor which shape is [None, input_dim]\n",
    "            with tf.name_scope('sequence_dropout') as scope:\n",
    "                step_outputs = []\n",
    "                for t, input in enumerate(step_inputs):\n",
    "                    step_outputs.append( tf.nn.dropout(input, keep_prob) )\n",
    "            return step_outputs\n",
    "\n",
    "        def sequence_encoding_n2n(step_inputs, seq_length, cell_size):\n",
    "            # birnn based N2N encoding and output\n",
    "            f_rnn_cell = tf.contrib.rnn.GRUCell(cell_size, reuse=False)\n",
    "            b_rnn_cell = tf.contrib.rnn.GRUCell(cell_size, reuse=False)\n",
    "            _inputs    = tf.stack(step_inputs, axis=1)\n",
    "\n",
    "            # step_inputs = a list of [batch_size, emb_dim]\n",
    "            # input = [batch_size, num_step, emb_dim]\n",
    "            # np.stack( [a,b,c,] )\n",
    "            outputs, states, = tf.nn.bidirectional_dynamic_rnn( f_rnn_cell,\n",
    "                                                                b_rnn_cell,\n",
    "                                                                _inputs,\n",
    "                                                                sequence_length=tf.cast(seq_length, tf.int64),\n",
    "                                                                time_major=False,\n",
    "                                                                dtype=tf.float32,\n",
    "                                                                scope='birnn',\n",
    "                                                            )\n",
    "            output_fw, output_bw = outputs\n",
    "            states_fw, states_bw = states \n",
    "\n",
    "            output       = tf.concat([output_fw, output_bw], 2)\n",
    "            step_outputs = tf.unstack(output, axis=1)\n",
    "\n",
    "            final_state  = tf.concat([states_fw, states_bw], 1)\n",
    "            return step_outputs # a list of [batch_size, enc_dim]\n",
    "\n",
    "        def _to_class_n2n(step_inputs, num_class):\n",
    "            T = len(step_inputs)\n",
    "            step_output_logits = []\n",
    "            for t in range(T):\n",
    "                # encoder to linear(map)\n",
    "                out = step_inputs[t]\n",
    "                if t==0: out = linear(out, num_class, scope=\"Rnn2Target\")\n",
    "                else:    out = linear(out, num_class, scope=\"Rnn2Target\", reuse=True)\n",
    "                step_output_logits.append(out)\n",
    "            return step_output_logits\n",
    "\n",
    "        def _loss(step_outputs, step_refs, weights):\n",
    "            # step_outputs : a list of [batch_size, num_class] float32 - unscaled logits\n",
    "            # step_refs    : [batch_size, num_steps] int32\n",
    "            # weights      : [batch_size, num_steps] float32\n",
    "            # calculate sequence wise loss function using cross-entropy\n",
    "            _batch_output_logits = tf.stack(step_outputs, axis=1)\n",
    "            loss = sequence_loss(\n",
    "                                    logits=_batch_output_logits,        \n",
    "                                    targets=step_refs,\n",
    "                                    weights=weights\n",
    "                                )\n",
    "            return loss\n",
    "        \n",
    "        seq_length    = tf.reduce_sum(self.w, 1) # [batch_size]\n",
    "\n",
    "        step_inputs       = _embedding(self.x)\n",
    "        step_inputs       = _sequence_dropout(step_inputs, self.keep_prob)\n",
    "        step_enc_outputs  = sequence_encoding_n2n(step_inputs, seq_length, hps.enc_dim)\n",
    "        step_outputs      = _to_class_n2n(step_enc_outputs, hps.num_target_class)\n",
    "\n",
    "        self.loss = _loss(step_outputs, self.y, self.w)\n",
    "\n",
    "        # step_preds and step_out_probs\n",
    "        step_out_probs = []\n",
    "        step_out_preds = []\n",
    "        for _output in step_outputs:\n",
    "            _out_probs  = tf.nn.softmax(_output)\n",
    "            _out_pred   = tf.argmax(_out_probs, 1)\n",
    "\n",
    "            step_out_probs.append(_out_probs)\n",
    "            step_out_preds.append(_out_pred)\n",
    "\n",
    "        # stack for interface\n",
    "        self.step_out_probs = tf.stack(step_out_probs, axis=1, name=\"step_out_probs\")\n",
    "        self.step_out_preds = tf.stack(step_out_preds, axis=1, name=\"step_out_preds\")\n",
    "\n",
    "        self.global_step = tf.get_variable(\"global_step\", [], tf.int32, initializer=tf.zeros_initializer, trainable=False)\n",
    "\n",
    "        if mode == \"train\":\n",
    "            optimizer       = tf.train.AdamOptimizer(hps.learning_rate)\n",
    "            self.train_op   = optimizer.minimize(self.loss, global_step=self.global_step)\n",
    "        else:\n",
    "            self.train_op = tf.no_op()\n",
    "\n",
    "        for v in tf.trainable_variables(): print(v.name)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_default_hparams():\n",
    "        return HParams(\n",
    "            learning_rate     = 0.005,\n",
    "            keep_prob         = 0.5,\n",
    "        )\n",
    "\n",
    "\n",
    "def train(train_id_data, num_vocabs, num_taget_class):\n",
    "    #\n",
    "    # train sentiment analysis using given train_id_data\n",
    "    #\n",
    "    max_epoch = 800\n",
    "    model_dir = \"./trained_models\"\n",
    "    hps = NER.get_default_hparams()\n",
    "    hps.update(\n",
    "                    batch_size= 100,\n",
    "                    num_steps = 85,\n",
    "                    emb_size  = 40,\n",
    "                    enc_dim   = 100,\n",
    "                    vocab_size=num_vocabs,\n",
    "                    num_target_class=num_taget_class\n",
    "               )\n",
    "\n",
    "    with tf.variable_scope(\"model\"):\n",
    "        model = NER(hps, \"train\")\n",
    "\n",
    "    sv = tf.train.Supervisor(is_chief=True,\n",
    "                             logdir=model_dir,\n",
    "                             summary_op=None,  \n",
    "                             global_step=model.global_step)\n",
    "\n",
    "    # tf assign compatible operators for gpu and cpu \n",
    "    tf_config = tf.ConfigProto(allow_soft_placement=True)\n",
    "\n",
    "    with sv.managed_session(config=tf_config) as sess:\n",
    "        local_step       = 0\n",
    "        prev_global_step = sess.run(model.global_step)\n",
    "\n",
    "        train_data_set = NERDataset(train_id_data, hps.batch_size, hps.num_steps)\n",
    "        losses = []\n",
    "        while not sv.should_stop():\n",
    "            fetches = [model.global_step, model.loss, model.train_op]\n",
    "            a_batch_data = next( train_data_set.iterator )\n",
    "            y, x, w = a_batch_data\n",
    "            fetched = sess.run(fetches, {\n",
    "                                            model.x: x, \n",
    "                                            model.y: y, \n",
    "                                            model.w: w,\n",
    "\n",
    "                                            model.keep_prob: hps.keep_prob,\n",
    "                                        }\n",
    "                              )\n",
    "\n",
    "            local_step += 1\n",
    "\n",
    "            _global_step = fetched[0]\n",
    "            _loss        = fetched[1]\n",
    "            losses.append( _loss )\n",
    "            if local_step < 10 or local_step % 10 == 0:\n",
    "                epoch = train_data_set.get_epoch_num()\n",
    "                print(\"Epoch = {:3d} Step = {:7d} loss = {:5.3f}\".format(epoch, _global_step, np.mean(losses)) )\n",
    "                _loss = []                \n",
    "                if epoch >= max_epoch : break \n",
    "\n",
    "        print(\"Training is done.\")\n",
    "    sv.stop()\n",
    "\n",
    "    # model.out_pred, model.out_probs\n",
    "    freeze_graph(model_dir, \"model/step_out_preds,model/step_out_probs\", \"frozen_graph.tf.pb\") ## freeze graph with params to probobuf format\n",
    "    \n",
    "from tensorflow.core.framework import graph_pb2\n",
    "def predict(token_vocab, target_vocab, sent):\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # force to use cpu only (prediction)\n",
    "    model_dir = \"./trained_models\"\n",
    "\n",
    "    # prepare sentence converting\n",
    "    # to make raw sentence to id data easily\n",
    "    pred_data     = N2NTextData(sent, mode='sentence')\n",
    "    pred_id_data  = N2NConverter.convert(pred_data, target_vocab, token_vocab)\n",
    "    pred_data_set = NERDataset(pred_id_data, 1, 85)\n",
    "    #\n",
    "    try:\n",
    "        a_batch_data = next(pred_data_set.predict_iterator) # a result\n",
    "        b_nes_id, b_token_ids, b_weight = a_batch_data\n",
    "    except StopIteration:\n",
    "        print(pred_data_set)\n",
    "\n",
    "    # Restore graph\n",
    "    # note that frozen_graph.tf.pb contains graph definition with parameter values in binary format\n",
    "    _graph_fn =  os.path.join(model_dir, 'frozen_graph.tf.pb')\n",
    "    with tf.gfile.GFile(_graph_fn, \"rb\") as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "    \n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.import_graph_def(graph_def)\n",
    "\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        # to check load graph\n",
    "        #for n in tf.get_default_graph().as_graph_def().node: print(n.name)\n",
    "\n",
    "        # make interface for input\n",
    "        pl_token     = graph.get_tensor_by_name('import/model/pl_tokens:0')\n",
    "        pl_weight    = graph.get_tensor_by_name('import/model/pl_weight:0')\n",
    "        pl_keep_prob = graph.get_tensor_by_name('import/model/pl_keep_prob:0')\n",
    "\n",
    "        # make interface for output\n",
    "        step_out_preds = graph.get_tensor_by_name('import/model/step_out_preds:0')\n",
    "        step_out_probs = graph.get_tensor_by_name('import/model/step_out_probs:0')\n",
    "        \n",
    "\n",
    "        # predict sentence \n",
    "        b_best_step_pred_indexs, b_step_pred_probs = sess.run([step_out_preds, step_out_probs], \n",
    "                                                              feed_dict={\n",
    "                                                                            pl_token  : b_token_ids,\n",
    "                                                                            pl_weight : b_weight,\n",
    "                                                                            pl_keep_prob : 1.0,\n",
    "                                                                        }\n",
    "                                                             )\n",
    "        best_step_pred_indexs = b_best_step_pred_indexs[0]\n",
    "        step_pred_probs = b_step_pred_probs[0]\n",
    "\n",
    "        step_best_targets = []\n",
    "        step_best_target_probs = []\n",
    "        for time_step, best_pred_index in enumerate(best_step_pred_indexs):\n",
    "            _target_class = target_vocab.get_symbol(best_pred_index)\n",
    "            step_best_targets.append( _target_class )\n",
    "            _prob = step_pred_probs[time_step][best_pred_index]\n",
    "            step_best_target_probs.append( _prob ) \n",
    "        return sent, step_best_targets\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_id_data, token_vocab, target_vocab = load_data()\n",
    "    num_vocabs       = token_vocab.get_num_tokens()\n",
    "    num_target_class = target_vocab.get_num_targets()\n",
    "\n",
    "    #train_data_set = NERDataset(train_id_data, 5, 85)\n",
    "    #train(train_id_data, num_vocabs, num_target_class)\n",
    "    \n",
    "    #predict(token_vocab, target_vocab, '아프가니스탄의 장래를 더욱 불투명하게 하는 것은 강경파 헤즈비 이슬라미와 우즈베크 민병대의 대립이다.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처음으로 해외여행을 가볼까합니다 다녀오신분이나 계획하시는 분들의 계획을 듣고싶네요 감사합니다 3월초나중순에 갈예정입니다\n",
      "\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-AC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DT', 'I-DT', 'I-DT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "올 10월~12월 사이에 부모님 첫 해외여행 보내드리려고 합니다.예산은 두분해서 300내외로 생각하고 있고요.\n",
      "\n",
      "['O', 'O', 'B-DT', 'I-DT', 'I-DT', 'I-DT', 'I-DT', 'I-DT', 'I-DT', 'I-DT', 'I-DT', 'O', 'O', 'O', 'B-AC', 'I-AC', 'I-AC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "9월정도에 친구들끼리 여행을가려고 하는데요 해외여행은 처음이라 겁도 나네요. 경비가 저렴하면서도 좋은곳 추천바랍니다.^^\n",
      "\n",
      "['B-DT', 'I-DT', 'I-DT', 'I-DT', 'O', 'O', 'B-AC', 'I-AC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "20대 여자 셋이에요,, ㅎㅎ10년이 넘은 친구들인데 드디어 시간이 맞아서 첫 해외여행을 가기로했답니다\n",
      "\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-AC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-AC', 'I-AC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "11월20~30일 사이에 3박5일 정도 갈꺼거든요경비는 비행기값까지 1인당 7.80 생각해요\n",
      "\n",
      "['B-DT', 'I-DT', 'I-DT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PR', 'I-PR', 'I-PR', 'I-PR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "근데 셋 다 여행지식 문외한이라 몇일째 정하지도 못하고있네요맨 처음 무턱대고 가고싶었던곳은 보라카이였는데 점점 여행지가 추가되다보니\n",
      "\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LC', 'I-LC', 'I-LC', 'O', 'O', 'O', 'O', 'O', 'B-PR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "세부,방콕,대만,다낭까지 더해졌네요 대만 먹방 여행도 괜찮아보이고,\n",
      "\n",
      "['B-LC', 'I-LC', 'O', 'B-LC', 'I-LC', 'O', 'B-LC', 'I-LC', 'O', 'O', 'I-LC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LC', 'I-LC', 'O', 'B-PU', 'I-LC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "요즘은 다낭도 많이 가는것같구요검색해보면 다 너무 좋아보이고 다 가고싶더라구요하지만 한군데만 택해야하는 현실\n",
      "\n",
      "['O', 'O', 'O', 'O', 'B-PU', 'I-LC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "저희는 적당한 관광+휴양 (마사지)를 원해요직장생활에 찌들린 몸, 육아에 찌들린 정신을 잠시나마 힐링하고싶어\n",
      "\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PU', 'I-PU', 'O', 'B-PU', 'I-PU', 'O', 'O', 'B-PU', 'I-PU', 'I-PU', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PU', 'I-PU', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "떠나는거거든요물론 아기는 맡기고 어른셋만 간답니당상세하게 잘 아시는분들... 저 나라들 설명좀해주세요\n",
      "\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-AC', 'I-AC', 'I-AC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method ScopedTFGraph.__del__ of <tensorflow.python.framework.c_api_util.ScopedTFGraph object at 0x000001E14C50D978>>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\tjdwn\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\c_api_util.py\", line 51, in __del__\n",
      "    if c_api is not None and c_api.TF_DeleteGraph is not None:\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "연로하신 어머니와 8살 6살 아이들 가족 해외 여행 계획중 입니다\n",
      "\n",
      "['I-PU', 'O', 'O', 'O', 'O', 'B-AC', 'I-AC', 'I-AC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-AC', 'I-AC', 'O', 'O', 'B-AC', 'I-AC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "3박 4일 예정 인데 추천 부탁드립니다어머니와 아이들 때문에 어디로 갈까 고민 중 입니다\n",
      "\n",
      "['B-PR', 'I-PR', 'I-PR', 'I-PR', 'I-PR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-AC', 'I-AC', 'I-AC', 'O', 'O', 'B-AC', 'I-AC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "제가 물놀이하고 에어텔로 놀러가는걸 좋아합니다. 랑 사이판은 갔다왔는데 해외휴양지로 3박4일 , 4박5일정도 떠나려하는데 \n",
      "\n",
      "['O', 'O', 'O', 'B-PU', 'I-PU', 'I-PU', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LC', 'I-LC', 'I-LC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PR', 'I-PR', 'I-PR', 'I-PR', 'O', 'O', 'O', 'B-PR', 'I-PR', 'I-PR', 'I-PR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "휴양지로 호텔수영장에서 놀고 바다에서 놀고 그러면서 보낼 더운 나라 좀 추천해주세요!\n",
      "\n",
      "['B-PU', 'I-PU', 'O', 'O', 'O', 'O', 'O', 'B-PU', 'I-PU', 'I-PU', 'O', 'O', 'O', 'B-PU', 'I-PU', 'O', 'B-PU', 'I-PU', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-WT', 'I-WT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "안녕하세요 ~ 이번년도가 끝나려면 얼마 남지않아서 저포함해서 친구와 두명이서 돈모아 해외여행을가려고 합니다. \n",
      "\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-AC', 'I-AC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "넉넉한 형편이 아닌 학생이라서 저렴하면서 구경하기좋고 물가도 괜찮은곳을 알아보고 있거든요. \n",
      "\n",
      "['O', 'O', 'O', 'O', 'B-AC', 'I-AC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PU', 'I-PU', 'O', 'O', 'O', 'O', 'O', 'B-PU', 'I-PU', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "행비에 맞게 돈도 모으고 여행비도 모아야해서 겨울쯤에가려고 하는데요 어디가 좋을까요? \n",
      "\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DT', 'I-DT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "배낭여행보다는 패키지 여행, 가이드가 있는 여행을 더 선호해요내공냠냠 광고글은 신고할께요^_^\n",
      "\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PU', 'O', 'O', 'O', 'I-PU', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "초등2,4학년 아이들과 아빠 셋이서 가족 해외여행을 하려고 합니다.\n",
      "\n",
      "['B-AC', 'I-AC', 'O', 'O', 'O', 'O', 'O', 'O', 'B-AC', 'I-AC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-AC', 'I-AC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "여름에 가족끼리 처음으로 해외여행을 가볼까 하는데요. \n",
      "\n",
      "['B-DT', 'I-DT', 'O', 'O', 'B-AC', 'I-AC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "지금까지 가족들이 각자 해외에는 몇 번 갔다 왔지만 온 가족이 해외로 여행가는게 처음이라서 고수님들 도움을 좀 얻어보려고 합니다. \n",
      "\n",
      "['O', 'O', 'O', 'O', 'O', 'B-AC', 'I-AC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-AC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "몇 가지 주요하게 들어갔으면 하는 것들 적어드리니 참고하셔서 답변 부탁드립니다!\n",
      "\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "1. 항공편은 되도록 150에서 200 사이에서 구했으면 합니다. (그래서 가까운 일본이나 동남아로 찾아보고는 있었습니다.)\n",
      "\n",
      "['O', 'O', 'O', 'B-PU', 'I-PU', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LC', 'I-LC', 'O', 'O', 'O', 'B-LC', 'I-LC', 'I-LC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "2. 부모님은 하루 정도는 골프 라운딩을 할 수 있었으면 좋겠다고 하십니다.\n",
      "\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PU', 'I-PU', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "3. 맛있는 음식을 많이 먹을 수 있는 곳이면 좋겠습니다.많은 답변 부탁드립니다~!!!내공 많이 많이 드릴께요~!!고수님들 부탁드려요!!\n",
      "\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-PU', 'I-PU', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "부모님이랑 20대 자매 둘 이렇게 넷이서 해외여행을 가려고합니다.9월에 갈껀데 세부 코타 다낭 중에 골라주세요! \n",
      "\n",
      "['B-AC', 'I-AC', 'I-AC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DT', 'I-DT', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LC', 'I-LC', 'O', 'B-LC', 'I-LC', 'O', 'B-LC', 'I-LC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "부모님은 많이 걸어다니시는걸 별로 안좋아하시구요, 힐링을 원하십니다!기간은 3박4일로 생각하고 있어요~\n",
      "\n",
      "['B-AC', 'I-AC', 'I-AC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PU', 'I-PU', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PR', 'I-PR', 'I-PR', 'I-PR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "일본으로 1년 유학을 가게 되었는데 여행자 보험 추천해주세요~~\n",
      "\n",
      "['O', 'O', 'O', 'O', 'O', 'B-PR', 'I-PR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "6월 17일 출발, 20일(21일) 귀국하는 일정의 가족 해외 여행을 준비중입니다. \n",
      "\n",
      "['B-DT', 'I-DT', 'I-DT', 'I-DT', 'I-DT', 'I-DT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-AC', 'I-AC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "장인 칠순을 맞이하여, 성인 9명, 43개월 유아 1명입니다. \n",
      "\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-AC', 'I-AC', 'I-AC', 'I-AC', 'I-AC', 'O', 'O', 'B-DT', 'I-DT', 'O', 'O', 'O', 'B-AC', 'I-AC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-b8678a37cf64>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m             \u001b[0msent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_vocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_vocab\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-c7cd42ac80f1>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(token_vocab, target_vocab, sent)\u001b[0m\n\u001b[0;32m    237\u001b[0m     \u001b[0mpred_data_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNERDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_id_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m85\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m     \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 239\u001b[1;33m     \u001b[0ma_batch_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_data_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_iterator\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# a result\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    240\u001b[0m     \u001b[0mb_nes_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_token_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma_batch_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    with open('./data/result4.txt',encoding='utf8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            sent, targets = predict(token_vocab, target_vocab,line)\n",
    "            print(sent)\n",
    "            print(list(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
