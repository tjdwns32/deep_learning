{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version :  1.10.0\n"
     ]
    }
   ],
   "source": [
    "import sys, os, inspect\n",
    "\n",
    "# add common to path\n",
    "from pathlib import Path\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "common_path = str(Path(currentdir).parent.parent)\n",
    "sys.path.append( common_path )\n",
    "\n",
    "from common.nlp.vocab import Vocab\n",
    "from common.nlp.data_loader import N21TextData\n",
    "from common.nlp.converter import N21Converter\n",
    "\n",
    "from dataset import SentimentDataset\n",
    "from dataset import load_data\n",
    "from common.ml.hparams import HParams\n",
    "\n",
    "import numpy as np\n",
    "import copy \n",
    "import time \n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn\n",
    "from tensorflow.contrib.layers.python.layers import linear\n",
    "\n",
    "from common.ml.tf.deploy import freeze_graph\n",
    "\n",
    "print( \"Tensorflow Version : \", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysis():\n",
    "    def __init__(self, hps, mode=\"train\"):\n",
    "        self.hps = hps\n",
    "        self.x = tf.placeholder(tf.int32, [None, hps.num_steps], name=\"pl_tokens\")#None: batch_size를 모르기 때문에 알아서 하도록 함\n",
    "        self.y = tf.placeholder(tf.int32, [None], name=\"pl_target\")#placeholder: 다른 텐서를 할당하기 위해사용, 이름도 써주는게 좋음\n",
    "        self.w = tf.placeholder(tf.float32, [None, hps.num_steps], name=\"pl_weight\")\n",
    "        self.keep_prob = tf.placeholder(tf.float32, [], name=\"pl_keep_prob\")\n",
    "\n",
    "        ### 4 blocks ###\n",
    "        # 1) embedding\n",
    "        # 2) dropout on input embedding\n",
    "        # 3) sentence encoding using rnn\n",
    "        # 4) encoding to output classes\n",
    "        # 5) loss calcaulation\n",
    "\n",
    "        def _embedding(x):\n",
    "            # character embedding \n",
    "            #print(\"-\"*100)\n",
    "            #print(\"Implement function '{}'\".format(_embedding.__name__))\n",
    "            #print(\"Keywords\")\n",
    "            #print(\"\\t - tf.initializers.variance_scaling\")\n",
    "            #print(\"\\t - tf.get_variable\")\n",
    "            #print(\"\\t - tf.nn.embedding_lookup\")\n",
    "            #print(\"\\t - tf.unstack\")\n",
    "\n",
    "            #print('Input : Tensor(\"model/pl_tokens:0\", shape=(?, 128), dtype=int32)')\n",
    "\n",
    "            \n",
    "            #print(\"Return: a list of <tf.Tensor shape=(?, 50) dtype=float32>\")\n",
    "            #print(\"        len(a_list) = 128\")\n",
    "            shape = [hps.vocab_size,hps.emb_size]\n",
    "            initializer = tf.initializers.variance_scaling(distribution=\"uniform\",dtype=tf.float32)#가중치 벡터의 shape에 맞게 조정\n",
    "            emb_mat = tf.get_variable('emb',shape,initializer=initializer,dtype=tf.float32)#입력한 파라미터에 맞는 데이터를 생성하거나 가져옴\n",
    "            input_emb = tf.nn.embedding_lookup(emb_mat,x)#embedding tensor리스트에서 id를 검색함\n",
    "            \n",
    "            step_inputs = tf.unstack(input_emb,axis=1) # split input_emb -> num_steps, tf.unstack: 랭크를 1줄임\n",
    "            return step_inputs\n",
    "\n",
    "        def _sequence_dropout(step_inputs, keep_prob):\n",
    "            # apply dropout to each input\n",
    "            # input : a list of input tensor which shape is [None, input_dim]\n",
    "            print(\"-\"*100)\n",
    "            with tf.name_scope('sequence_dropout') as scope:\n",
    "                #print(\"Implement step_outputs\")\n",
    "                #print(\"Keywords\")\n",
    "                #print(\"\\t - tf.nn.dropout\")\n",
    "                step_outputs=[]\n",
    "                for t,input in enumerate(step_inputs):\n",
    "                    step_outputs.append(tf.nn.dropout(input,keep_prob))\n",
    "\n",
    "\n",
    "\n",
    "            #print(\"Return: a list of <tf.Tensor shape=(?, 50) dtype=float32>\")\n",
    "            return step_outputs\n",
    "\n",
    "        def sequence_encoding_n21_rnn(step_inputs, cell_size, scope_name):\n",
    "            # rnn based N21 encoding (GRU)\n",
    "            #print(\"-\"*100)\n",
    "            #print(\"Implement function '{}'\".format(sequence_encoding_n21_rnn.__name__))\n",
    "            #print('Input : a list of <tf.Tensor shape=(?, 50), dtype=float32>')\n",
    "            #print(\"Keywords\")\n",
    "            #print(\"\\t - tf.contrib.rnn.GRUCell\")\n",
    "            #print(\"\\t - tf.contrib.rnn.static_rnn\")\n",
    "\n",
    "            #print(\"Return: a list of <tf.Tensor, shape=(?, 100)>\")\n",
    "            step_inputs=list(reversed(step_inputs))#뒤 쪽에 있는 패딩심볼의 악영향을 줄이기 위해 뒤에서 부터 인코딩\n",
    "            f_rnn_cell=tf.contrib.rnn.GRUCell(cell_size,reuse=None)\n",
    "            _inputs=tf.stack(step_inputs,axis=1)\n",
    "            step_outputs, final_state=tf.contrib.rnn.static_rnn(f_rnn_cell,\n",
    "                                                               step_inputs,\n",
    "                                                               dtype=tf.float32,\n",
    "                                                               scope=scope_name)#각 step별 output과 마지막 히든레이어를 결과로\n",
    "            out = step_outputs[-1]            \n",
    "            return out\n",
    "\n",
    "        def _to_class(input, num_class):\n",
    "            #print(\"-\"*100)\n",
    "            #print(\"Implement function '{}'\".format(_to_class.__name__))\n",
    "            #print('Input : tf.Tensor, shape=(?, 100), dtype=float32')\n",
    "            #print(\"Keywords\")\n",
    "            #print(\"\\t - tensorflow.contrib.layers.python.layers.linear\")\n",
    "\n",
    "            #print(\"Return: tf.Tensor, shape=(?, 4), dtype=float32\")\n",
    "            out = linear(input,num_class,scope=\"Rnn2Sentiment\") # should be implemented\n",
    "            return out\n",
    "\n",
    "        def _loss(out, ref):\n",
    "            # out : [batch_size, num_class] float - unscaled logits\n",
    "            # ref : [batch_size] integer\n",
    "            # calculate loss function using cross-entropy\n",
    "            #print(\"-\"*100)\n",
    "            #print('Input out: tf.Tensor, shape=(?, 4)')\n",
    "            #print('Input ref: tf.Tensor(\"model/pl_target:0\", shape=(?,), dtype=int32)')\n",
    "\n",
    "            #print(\"Keywords\")\n",
    "            #print(\"\\t - tf.nn.sparse_softmax_cross_entropy_with_logits\")\n",
    "            #print(\"\\t - tf.reduce_mean\")\n",
    "            \n",
    "            #print(\"Return: tf.Tensor, shape=(), dtype=float32\")\n",
    "            batch_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=out,labels=ref,name=\"sentiment_loss\")\n",
    "            loss =  tf.reduce_mean(batch_loss)# [batch_size,emb_dim=30]\n",
    "            return loss\n",
    "        \n",
    "        seq_length    = tf.reduce_sum(self.w, 1) # [batch_size]\n",
    "\n",
    "        step_inputs   = _embedding(self.x) # [batch_size,emb_dim=50]\n",
    "        step_inputs   = _sequence_dropout(step_inputs, self.keep_prob)\n",
    "        sent_encoding = sequence_encoding_n21_rnn(step_inputs, hps.enc_dim, scope_name=\"encoder\")#sentence encoding\n",
    "        out           = _to_class(sent_encoding, hps.num_target_class)\n",
    "        loss          = _loss(out, self.y) \n",
    "\n",
    "        if loss is None: \n",
    "            print(\"All functions should be implemented!\")\n",
    "            import sys; sys.exit()\n",
    "\n",
    "\n",
    "        out_probs     = tf.nn.softmax(out, name=\"out_probs\")\n",
    "        out_pred      = tf.argmax(out_probs, 1, name=\"out_pred\")\n",
    "\n",
    "        self.loss      = loss\n",
    "        self.out_probs = out_probs#[batch_size, num_class]\n",
    "        self.out_pred  = out_pred#[batch_size]\n",
    "\n",
    "        self.global_step = tf.get_variable(\"global_step\", [], tf.int32, initializer=tf.zeros_initializer, trainable=False)\n",
    "\n",
    "        if mode == \"train\":#backpropagation\n",
    "            optimizer       = tf.train.AdamOptimizer(hps.learning_rate)#자연어에서는 주로 adam사용\n",
    "            self.train_op   = optimizer.minimize(self.loss, global_step=self.global_step)\n",
    "        else:\n",
    "            self.train_op = tf.no_op()\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def get_default_hparams():\n",
    "        return HParams(\n",
    "            learning_rate     = 0.001,\n",
    "            keep_prob         = 0.5,#문제의 손상율\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_id_data, num_vocabs, num_taget_class):\n",
    "    #\n",
    "    # train sentiment analysis using given train_id_data\n",
    "    #\n",
    "    max_epoch = 3000\n",
    "    model_dir = \"./trained_models\"\n",
    "    hps = SentimentAnalysis.get_default_hparams()\n",
    "    hps.update(\n",
    "                    batch_size= 100,\n",
    "                    num_steps = 128,#몇 개의 char가 들어오나? 즉 input\n",
    "                    emb_size  = 50,#조금 작은 값임\n",
    "                    enc_dim   = 100,#마지막 시퀀스 인코더의 dimension\n",
    "                    vocab_size=num_vocabs,\n",
    "                    num_target_class=num_taget_class\n",
    "               )\n",
    "\n",
    "    with tf.variable_scope(\"model\"):#해당 스코프안에서 정의되는 변수는 해당범위에만 한정되어짐\n",
    "        model = SentimentAnalysis(hps, \"train\")\n",
    "\n",
    "    sv = tf.train.Supervisor(is_chief=True,#프로그래밍 패턴중 하나, sv안에 세션을 담으면 알아서 다 해준다고 함\n",
    "                             logdir=model_dir,\n",
    "                             summary_op=None,  \n",
    "                             global_step=model.global_step)\n",
    "\n",
    "    # tf assign compatible operators for gpu and cpu \n",
    "    tf_config = tf.ConfigProto(allow_soft_placement=True)#cpu, gpu가 문제없이 돌아갈 수 있도록 해주는 역할\n",
    "\n",
    "    with sv.managed_session(config=tf_config) as sess:\n",
    "        local_step       = 0\n",
    "        prev_global_step = sess.run(model.global_step)\n",
    "\n",
    "        train_data_set = SentimentDataset(train_id_data, hps.batch_size, hps.num_steps)\n",
    "        losses = []\n",
    "        while not sv.should_stop():#미니배치로 실행\n",
    "            fetches = [model.global_step, model.loss, model.train_op]\n",
    "            a_batch_data = next( train_data_set.iterator )\n",
    "            y, x, w = a_batch_data\n",
    "            fetched = sess.run(fetches, {\n",
    "                                            model.x: x, \n",
    "                                            model.y: y, \n",
    "                                            model.w: w,\n",
    "\n",
    "                                            model.keep_prob: hps.keep_prob,\n",
    "                                        }\n",
    "                              )\n",
    "\n",
    "            local_step += 1\n",
    "\n",
    "            _global_step = fetched[0]\n",
    "            _loss        = fetched[1]\n",
    "            losses.append( _loss )\n",
    "            if local_step < 10 or local_step % 10 == 0:\n",
    "                epoch = train_data_set.get_epoch_num()\n",
    "                print(\"Epoch = {:3d} Step = {:7d} loss = {:5.3f}\".format(epoch, _global_step, np.mean(losses)) )\n",
    "                _loss = []                \n",
    "                if epoch >= max_epoch : break \n",
    "\n",
    "        print(\"Training is done.\")\n",
    "    sv.stop()\n",
    "\n",
    "    # model.out_pred, model.out_probs\n",
    "    freeze_graph(model_dir, \"model/out_pred,model/out_probs\", \"frozen_graph.tf.pb\") ## freeze graph with params to probobuf format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.core.framework import graph_pb2\n",
    "def predict(token_vocab, target_vocab, sent):\n",
    "    #os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # force to use cpu only (prediction)\n",
    "    os.environ['CUDA_DEVICE_ORDER']=\"PCI_BUS_ID\"\n",
    "    os.environ['CUDA_VISIBLE_DEVICES']='0'# force to use gpu only (prediction)\n",
    "    model_dir = \"./trained_models\"\n",
    "\n",
    "    # prepare sentence converting\n",
    "    # to make raw sentence to id data easily\n",
    "    in_sent = '{}\\t{}'.format('___DUMMY_CLASS___', sent)\n",
    "    pred_data     = N21TextData(in_sent, mode='sentence')\n",
    "    pred_id_data  = N21Converter.convert(pred_data, target_vocab, token_vocab)\n",
    "    pred_data_set = SentimentDataset(pred_id_data, 1, 128)\n",
    "\n",
    "    #\n",
    "    a_batch_data = next(pred_data_set.predict_iterator) # a result\n",
    "    b_sentiment_id, b_token_ids, b_weight = a_batch_data\n",
    "\n",
    "    # Restore graph\n",
    "    # note that frozen_graph.tf.pb contains graph definition with parameter values in binary format\n",
    "    _graph_fn =  os.path.join(model_dir, 'frozen_graph.tf.pb')\n",
    "    with tf.gfile.GFile(_graph_fn, \"rb\") as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "    \n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.import_graph_def(graph_def)\n",
    "\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        # to check load graph\n",
    "        #for n in tf.get_default_graph().as_graph_def().node: print(n.name)\n",
    "\n",
    "        # make interface for input\n",
    "        pl_token     = graph.get_tensor_by_name('import/model/pl_tokens:0')\n",
    "        pl_keep_prob = graph.get_tensor_by_name('import/model/pl_keep_prob:0')\n",
    "\n",
    "        # make interface for output\n",
    "        out_pred  = graph.get_tensor_by_name('import/model/out_pred:0')\n",
    "        out_probs = graph.get_tensor_by_name('import/model/out_probs:0')\n",
    "        \n",
    "\n",
    "        # predict sentence \n",
    "        b_best_pred_index, b_pred_probs = sess.run([out_pred, out_probs], feed_dict={\n",
    "                                                                                        pl_token : b_token_ids,\n",
    "                                                                                        pl_keep_prob : 1.0,\n",
    "                                                                                    }\n",
    "                                          )\n",
    "\n",
    "        best_pred_index = b_best_pred_index[0]\n",
    "        pred_probs = b_pred_probs[0]\n",
    "        #결과 출력 루틴\n",
    "        best_target_class = target_vocab.get_symbol(best_pred_index)\n",
    "        print( pred_probs[best_pred_index] )\n",
    "        best_prob  = int( pred_probs[best_pred_index] )\n",
    "        print(best_target_class, best_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99928445\n",
      "POS 0\n"
     ]
    }
   ],
   "source": [
    "train_id_data, token_vocab, target_vocab = load_data()#데이터 로드\n",
    "num_vocabs       = token_vocab.get_num_tokens()#vocab로드\n",
    "num_target_class = target_vocab.get_num_targets()#타겟(카테고리) 로드\n",
    "\n",
    "#train(train_id_data, num_vocabs, num_target_class)#학습\n",
    "predict(token_vocab, target_vocab, \"별로에요\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98873746\n",
      "POS 0\n",
      "1.0\n",
      "NEG 1\n",
      "0.9558535\n",
      "POS 0\n"
     ]
    }
   ],
   "source": [
    "predict(token_vocab, target_vocab, \"음식이 너무 맛있어요\")\n",
    "predict(token_vocab, target_vocab, \"날씨가 더워서 아무것도 할 수 없었어\")\n",
    "predict(token_vocab, target_vocab, \"경치가 예뻤으면 좋겠다!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
